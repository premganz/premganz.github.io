The Dilemma of Control 
And the case of the nerds
â€ƒ

Working with the do-calculus

A) A complexity model of computing 
From there we attempt to build upon a model of computing, where we can formalize the system with respect to its normative system.
1) Turing Model of computing.
Our idea would be to see the way in which computers are organized with respect to the system that they are created in, or embedded in. The system utilizing the computer, mostly uses it to simulate a machine or a market or a deliberative individual. That is to say if a system of business or any goal directed, normative organization involves the computer amidst it in order that it satisfies the missing link in the organization of individuals. An individual with a deliberative sense, a market or an organization needs to decide, between nontrivial choices at any point of time. These decisions are expected to be made by consensus (or if an individual, conscientous) if it is consistent and bias free. This is achieved by the development of a system which reduces the environment into a model, where problems could be posed to the system and decisions be obtained, in a simulated mode, in the interests of the stability of the system as a whole, at lower cost than negotiation over social niceties or be controlled by a central, sovereign organization. 
The computer is thus a simulator of the environment. It would hence adjudicate questions posed to it and provide its decisions. No machine can solve questions not posed to it. Any machine can answer questions that are posed most properly. It decides on questions based on previous agreements, which form its axioms. It hence decides on logical formulae based on tautological premises. Each decision however involves evaluating truths based on conditions. These conditions arise from having to evaluate particular environmental signals conveyed as numbers or signals, interpreted by individuals so that it might be referred against a standard lexicological tree (such as code tables) to take the program forward on a given path, till it halts with an accept or reject. 
Humans interpret events from the environment in according to the user interfaces. The program halts till the input is provided.  Where sensors are used to collect data it might loop. There are combinatorially many paths making the system difficult of proof but easy to verify. It could hence be said of the system that it might not even be deterministic, since reproducing the pathway in complex algorithms in production may not be possible. 
Hence, the system is reductive, but non provable. It is consistent to a good extent and hence transactions could happen in a human world so as to serve as a stabilizing consistency checker of human actions. Where human actions are not consistent, or a proposition for an 'action' is made, the sysetem either allows or denies it. 
Hence, it is important to understand that the system plays the role of a permitter/denier of actions, rather than solving intellectual questions.

2) Post Turing Machines.
a) Computers that simulate markets
It is possible to make a network of computers united over a common lexicon and discrete code tables could be programmed with regard to local truths which are expected to be determined adversarially over a network. This is a simple extension of the Turing model. The machine becomes capable of emergent effects such as 'manufacturing consent'. In this case the machine simulates a planar graph, wherein it simulates a rational authority in the system or multiple systems connecting over a network.
There might be indefinite waits on service calls. On the other hand, if the graph is to be made non planar, in the sense, that a certain truth at an earlier point in time, 'arcs' across and influences a later state and likewise where paralell pathways have connections in between them, then the signal processed of the environment, simulates the user events as well. That is to say, if a closed loop simulation is planned, it would require mimicking a free willed agents actions as well. These actions could be simulated by two elements, one is a random generator and secondly a normalizer. 
That is to say, it is more like a MCMC algorithm, where random moves are generated (say a bid is made) and based on a statistical distribution model (by plotting the accepted bids, the system might construct it in memory). Further reejections and acceptance might be made based on such normal distribution. This makes the agent a reflective agent. It tries to reap a surplus by hiding its internal state and pushing towards greater optimization within its context. A cooperative strategy might emerge from this game, leading to a situation where a set of systems might cooperate without the need for common code tables (discrete parameters which identify thresholds of continuous values). However a lexicon is required nevertheless.
Thus intelligent agents might be an extension of a traditional computer with stochastic features that could normalize on the outcomes. But it is also required that such agents could not be fully automated, because of the need for a prior. An uninformative prior or a blind search might not produce an effective system. Hence each service might have its programmers, who program certain outcomes expected wherefrom evidence is observed and a posterior is constructed, by MCMC like methods in one go. Hence, the supplication of intents are important for a computer that simulates markets.

b) Computers that simulate a deliberative agent.
Where a non planar graph model is used, it might involve an extended high dimensional reasoning. For instance, it might construct that the outcome of a certain shipment is dependent not only on the immediate causes, but also from distant causes. For instance, political turmoil in a region might dictate outcomes. If one heuristically knows that the system relies on certain knowledge which is drawn from a limited sample, then he is likely to generate events (such as risk factor) that account for this. Hence, an elaborate network might generate pathways that involve noise filteration. Specific decisions might be cliques which are bijective (such as an eligibility for a loan). The pathways that are traversed to get to either of these mutually exclusive cliques are finite and could be derived from noise filtering over specific pathways, using MCMC like local searches. 

B) Normative Intent of Extended Computing
The normative intent of these extended computing is to be able to simulate extended outcomes and make quotes or bids. That is to say, a system that is intelligent, in that it could answer incomplete questions (by supplying a normal context) might be able to make heuristic judgement, which might lift the burden of human actors. Thus these might be recommenders. Recommenders run a simulation, generating stochastically events from possible points of influence of a system and predict the possible pathway to reach a normative unambiguous point. Based on the pathway that is expected to be followed, suitable preparatory actions are recommended with respect to the initial state, either to maximize desired outcomes or minimize adverse outcomes. 
In the long run, there is only a thin line between decisions and recommendations. If complex decisions are simulated on a computer, the system on the whole is expected to self organize quickly over bots that are high frequency, low cost searches for equilibrium. These equilibria are deterministic. In general human agents are biased, they suspect they can do better than average, that is why people cannot accept a normalization by any agency and would rather like to self determine. 
Hence, a sufficiently intelligent system might simulate its handler as well. It might hence sustain a system autonomously, say we would wait for buses without being able to do anything about it. It then becomes a phenomenon, that outwits observation. There are no ways to nudge or govern the system. This results in long run regression of the system or a historical divergence where the system might incur transaction costs to search for efficient solutions adversarially. Both these situations are uncomfortable.

C) Extending Regular Models to extended models
1) A model high dimensional computing architecture
 We explore the idea of being able to design solutions for the regular business house.
- Firstly, we compose immutable entities. In such a case, every entity would have to have  an initial state from where it starts evolving. There are no updates or deletions, but the specific parts of the entity, say address of a customer keeps changing as if  adding nodes to a graph.
- This allows for not only entities, but also transcient items such as orders to go through processes of creation to closure and forever remain in history. 
- These immutable entities correspond to specific intuitive entities, such as documents that go into a file and remain there for ever (unless archived or specifically destroyed). 
- Every action would involve adding something into a registry or reading from the registry. 
- Thus there might be entities evolving and being linked by reference to other entities. Thus, a document of an order might contain reference to a shipper and a delivery address via indices. 
- It may point to the head of the referred evolutionary path, or might refer to a chronologically dictated connection. Relational databases are efficient in that an automatic pointing to the head is built in, by referential constraints. In older document based databases, this might not be the case. The link is a hardlink, or it may even be directly copied insitu instead of holding references. 
- In fast evolving systems, this may present problems, because there is needed of a hidden index with the timestamp which versions the the branch (such as with git). 
- All branches must however refer to an objective narrative, which lists the codes in use, the identities of actors (whose attributes might change) and a lexicon of abbreviations. Assets might be transcient, but real accounts are to be endured. 
- Where the evolving system branches, say a new branch is added to a vendor, then automated 'pulls' of RDBMS can have problems. 
-If we examine the advantages of this approach, we might see that the ouput graph might be used for a variety of analytical actions, where history is important.
- The graph is planar where we work with realtime transaction processing. The branching is decided by immediate 'events' which signify human processed high dimensional signals. Where such events are not to be the core of the state machine design, such as when automated simulation is preferred, the graph would have to become non planar. 
- There might be higher dimensions to decision making, employed while interpreting environmental events by human agents, as a heuristic excercise.

2) The incremental development of systems around evolutionary pathways, avoid the problem of combinatorial explosion and is capable of deciding questions that are high dimensional. 


Part2


1. Computing ought to be viewed about a boundary. This boundary seperates problems that are natural and that which are synthetic. Synthetic problems arise from human processes solving natural problems. Natural problems are the non linearities in nature. Say a process P in nature could be described by a consistent model M, to a certain degree of completeness. The decisions made using M, would nudge P to dynamically evolve in a certain direction, which is almost deterministic, though from the point of view of P, it is a dynamic evolution. The model M would then have a complementary part M' which would interrupt P and reset it. This interruption would result in surpluses. A clear example could be made out of farming, by performing certain rule bound tasks, waiting, harvesting and repeating. 
Thus, this works as a heat pump, which pumps surplus energy from an order creating system which is not intelligent enough to identify its manipulation, such as with crops or farm animals. There might be a long term fall in marginal utilities or a central regression, however.
Synthetic problems arise from the surplus energy generated by the heat pump. This would need dissipative pumps which would have to make wasteful movements to dissipate the heat. That it is to say, there gets developed complex pathways to negotiate simple dilemmas with respect to reconcillations of liberty and equality. That is to say, how does a person seek to do what is best for him and at the same time expect to compromize with possible conflict with other persons wishes. 
The numerous constructs of organization and politics involve rituals which dissipate energy. These rituals might be a democractic election every five years. The different interest groups are abstracted with an incomplete model and studied in their dynamics, till at decisive point the direction becomes apparant, such as a leftist or rightist swing and appropriate reconstitution of the government takes place. This is an oscillatory movement much like in agriculture. 

2. The role of computing in this setting is the development of an incomplete model which would guide the dynamics to specific targets ready for a second discrete intervention. This happens both in the case of synthetic as well as natural problems. Computing simulates natural problems in order to produce this effect and it could be seen as a continuation of experimental techniques. Likewise in synthetic problems, one might see that there had always been techniques that defined rituals that allowed for ritualistic wasteful movements. Computing by defining less consistent but more complete systems than before allowed for these rituals to happen over higher frequencies, because the dynamics could be computed in narrower time ranges and could be quickly harvested. 
The computing system is a purely self referential system that is placed amidst the society. The presence of this system allows for any arbitrary expression by any agent to be validated for consistency, that is to say, different ways of putting the point that one is entitled to a greater liberty than the neighbor is simplified and validated by the presence of the computing system. This allows for a lesser need to construct dialectic political parties and submission to an internal control system that reductively models, dynamically tracks and decisively resets the system. 


3. The idea itself however seems to be to minimize the use of these dissipative pumps and build orderliness. In more formal terms, it is the role of science to present moral dilemmas. That is to say, science should try to explain by cold reasoning many of the happenings of the universe, so that  a scarce few is left for being referred for moral reasoning. Hence, in this case, we see that it is the duty of science to appreciate the process of computing as reflecting from a general pursuit of evolution, as seeing the agenda of living systems and intelligence as distinct from free willed agents, who would then be presented with a moral dilemma at the end of the scientific apprisal. To get back on the scientific appraisal, every species seeks to attain the highest energy steady state possible. The mosquito develops as much intelligence as would allow it to avoid the most wily swatters, by appropriate reaction. The mosquito had at all times wished to seal the need for dissipation of free energy or the need to explore new forms by stabilizing at a high energy level. But the complex graph of mosquitoes caused several paralell branches at equal levels of sophistication and even some low level creatures being able to draw arcs to defect and continue the evolutionary path, allowing them to stabilize at not the highest possible energy level, but below it.

4. The Ultimate human question might be to find a way where no experimentation might be required. That is to say, all questions had been answered and no further reaping of free energy is required. If the ultimate aim of a species to solve problems in nature, then its pinnacle would be to solve it theoretically and experiment no further. But if reality is not static, rather it carries wave like oscillations by an actor that woudl eventually oscillate through everything. But we think reality is static at this point. Humans might hence seek to attain high energy stability conditions. 

5. A higher integration of this sort happened in multicellularity. But multicellularity is accompanied by a macabre reset, where the grand castle of billions of occupants is brought down by a timed device, only to redraw it completely by reproduction. These billion occupants do have their individual consciousness, but it had resulted in a higher collective consciousness in the CNS. Similar half complete experiments can be seen in eusocial creatures, where a hive mind is existant. But even here, the entire hive collapses at some point and one queen leaves (much like a single gamete selected from the billions of somatic cells going on to restart multicellularity). The redrawl effectively nullifies history and allows the system to add pure stochasticity to its form. The resulting experiment on the environment is hence pure random search, which leads to a redrawn creature in the new environment. The tragedy of life is the need for a reset. All consolidation of intelligence, would eventually lead to the ability for a single clean reset, with an encapsulation in a highly compressed format the whole of the grand enterprise. In simple creatures where such compressed transportation is trivially possible by spores or where cells themselves could be replicated in the vicinity as with algae, there is no grand construction and collapses. However, the collapses themsleves align to the possiblity of a natural collapse due to crowding, invasions by microbes or by simple accidental uncontained local failures.
In another incomplete consolidation, as with niches, say we see that a group of creatures such as bovines, predators and their group form the savannah niche. They explicitly try to increase the range of savannahs by bringing down trees at the edge of the forest, by elephants doing so. They seem to be working as a team with a collective consciousness. There is a recession of consciousness of the individual creatures as they play to their instincts. 
A similar projection might be made of the use of computing on synthetic problems. As natural problems get solved by simulation methods, the productivity gains increase and the use of these by incorporating them into solutions leads to the evolution of the system in strange ways. Say a bus service becomes completely self managing by dealing with fuel suppliers, driving itself, connecting with vendors and suppliers and the entire transportaiton in a metro had been automated. People no longer care about the software patches and control systems. Say over ten years, the systems marginal utlity declines and humans would want to intervene. They would likely find that the progression had been much more, the bus company had actually become a law and order partner, became a major supplier of scrap for defense purposes, making the problem of shutting down the system unpredictable. Hence, the system is effectively free of human control. The subsequent developments would lead to a hive mind like integration. 
But as more and more the integration comes closer to fruition, there would become a need for clean resets. Thus, there would likely emerge a single super computer which would eventually shut down the processes and collapse the system, and then start spawning the process again. Just like individual cells inside multicellular organisms left to starve after the collapse, the individuals would become subjected. 

6. Just as science sees this as a repeating course of the 'phenomenon' of life itself, it also observes that there does exist alternate paths to this seeming determinism. There might arise a certain poing when the agents might start becoming attached to wasteful rituals, as in culture, nations and celebrations and shun computational integration. This had infact been an exposition on the 'phenomenaology of integrated living beings with emergent consciousness' . We also might stand corrected with respect to our previous expostion on 'why no singularity'. But the coutercurrent could be observed (perhaps assisted by fermis paradox) that people infact try to maintain stasis by dissipating energy and without attempting a clean reset and integration pathway. They might in fact attempt to replicate a phenomenon within life itself, attempting terraforming and universal spread without necessarily working vertically. They might leverage on their curiosity and love for liberty and experimentation and value it more than the need for vertical consolidation in singularity. They might prefer variety and deferring away to different directions, hoping for a deferred unity on homecoming, rather than staying put and building elaborate bunkers. But that is the moral dilemma we promised.

Part 3 - Leisure and Defragmentation
It is witnessed that activism arises from some form of outrage. The outrage arises from a perception of the world to have been flawed and below in the standard of constitution of human societies. Hence, most outrages are directed at units of human organizations, which suffer from complexes which are strange and repulsive for the activist. He should hence be moved to avenge this unit of human organization. It is a reflection of how the unit, be it a family as with the Arab joint families, communities or entire countries that motivates him. It is the destruction of the tendency for humans to group together as a natural force and attempt to build a higher level consciousness and substitute it with a grouping over a political consciousness, with the elements of planning, control and above all leisure. 
That is to say, the constant pressure to work as a group while retaining conscious control over the dynamics of the group is the priority of the activist. For this end, he pursues two important actions. Firstly the development of a knowledge of highly consistent forms and secondly on the provisioning of the boundaries of completeness of the said knowledge so that freedom exists in the group to do non impactful activities. Thus, there is required of leisure to be provided for and insulated from made gainful. This repeated reinforcement of the need for consistent platforms to underlie pursuit of reward based actions and its seperation from the society at large is an important way to prevent evolution from inventing a human cohesiveness that is like the tower of Babel experiment. (cite the soft power complex).
The preservation of human agency or liberty to pursue independent spiritual romantic quests while being embedded in a group, capable of being made sense of in political terms presents an ideal which requires the selection of dedicated contributors to this end. Hence activism is a recruitment of people who can do things that does not add to their survivalist needs proportionately. There is a need for a leisure class, unmotivated by hedonism, but rather moved by stoic concerns to create a society with less of unbounded dynamics and more of well appointed dynamic space capable of political interpretations. 
That is to say, a constant flux and dynamic of economic innovation, cross border capital flow, monetary synthetic manipulations, market relationships coupling itself to gender relationships, household generational relationships, religion and institutions of tradition and customs, public spaces and power relationships cause exceptionally complex dynamics, such as uprooting from communities, emigration of family members, traditional roles being interpreted as economic oppression and their challenge through economic market egalitarianism. This we might summarize as fragmentation. 
This fragmentation is caused by a need to pursue greater orderliness than that is local and organized recursively (d-seperated). The more dynamic and shifting the relationships, the greater their need for being tracked by symbolic and quantative pointers. This could then be reintegrated to produce model organizations. This might be in the interest of lower transaction cost to decisions that concern larger wholes, at the cost of local autonomy. In the long run, this produces an integration into a newer consciousness for which there is no pointers from human actors.
The fragmentation is a subject of distress, which however is not quantifiable due to its difficulty of being expressed. People do not become less productive becuase of fragmentation, they suffer nevertheless in silence. The move of the activist is hence to stop the fragmentation by limiting and concretizing notions of modernity so as to allow for a leisurely and voluntary take on the problem, dispersing the forces of ruthless intractable integration, powered by evolution and replacing it with a pluralistic force, which refuses integration and dissipates the excess heat generated by engineering prowess either to solve problems delinked from utility or in the practice of arts.

Hence, activism always concerns itself with acting as  a group of leisure pursuers who identify a specific community or an abstract community to induce changes. These changes might be policy regulation/control or as provisioning of common resources to produce buildings, spaces and free instructions to all. Hence, these actions might be prescriptive, welfare or educative. The motive is redemptive and the medium is formal publication with openness to crticism.
This might be seen as morality in action. Morality concerns with not just principles for stability, but also those for preservation of innate human consciousness. Amorality as in postmodernity might be an antidote to this. Moral sentiments purport people to seperate actions from circumstances
pyschological impact:
In psychological terms, we might percieve that fragmentation, disorients the conginitive faculties into construction of network models of the world which are unstable, leading to volatility resulting from small changes. This instability results in synthetic models where the edges are discredited (where paranaoid delusions might mark cognition) or where nodes are discounted (where a pervasive fatalistic model might influence mood). The resulting adjustment might be permanent and influence command signals to alter it, thereby blocking off further correction by external faculties, or it might be continually oscillating and volatile perception leading to affective volatility and cognitive distortions.









A Redraft of the deed of trust
==========
The leafycampus foundation for studies in information technology and development is constituted hereby as an institution to pursue the ends detailed out hereunder:
To constitute a dedicated institution consisting of people, facilities, principles and a hinterland of people, working systematically without being pegged towards the pragmatic notions of material enrichment. This institution shall primarily work intellectually to acquire knowledge in ever increasing reflection of its pure form and base it to create pragmatic adaptations, so that consistency is advocated as a higher ideal than completeness. 
The completeness of social groups is to be attempted of materialization by complete knowledge based productive centers which are embraced by specific social groups which define their completness in cultural and artistic terms. This could be achieved only by a context bound definition of development.
Hence development of specific social groups is to be attempted by intellectual pursuit of pure knowledge, standardization of what constitutes the bounds of consistency for an applied system and leaving the incomplete areas to be managed culturally and by spiritual contemplation.
The knowledge might be the base for pragmatic adapatation resulting in productive free energy in the social system.
The standardization of consistency of such adaptive systems might arise from a tolerance and entertainment of pluralism over an intellectual arena which has due processes of critical examinations and peaceful processes and reflection of universal notions of morality.
The instution shall play the role of emancipator of specific groups, concrete or abstract by inspiration, publication, experimental and model projects.
A redemptive and context bound notion of development is proposed in a critical frame to the postmodernist construct of feedback driven, unbounded and highly subjective notions of progression of human activities, leading to emergent unifying entities, that cause fragmentation distress to individuals.
The authority of the institution is aimed to be moral and it is concieved as an idealistic one delinked to practical, material considerations.Its energies might however be directed to immediate humanitarian work even if it does not involve intellectual dimensions.
This is critical of unified notions of development over incremental problem solving power, the ability to produce greater choice to the individual while the costs themselves are collected by means that are neither transparent nor tractable. 
This engineering of unbounded, action cross-messaging platforms with minimal policy orchestration, it is felt might lead to emergent, intractable complexes. These complexes bind humans to closed infinite loops of production and consumption, with intelligent random moves migrating into the network. 
This is a metanarrative of the individuality and pluralism of narratives of development and the singularity of objective reductive knowledge, itself contained recursively in the metanarrative of plurality of ideas, openness to crticism and peaceful methods.
-
There are risks with this approach as well - conflict had arisen in the past not only out of resource sharing but also from egotism of identity (we can ontologically seperate this from evolutionary concerns). 
Cultures are identity reinforcing, independent of narratives of development. In Postmodernity mass media, consumerism and urbanization disperses culture into monotones, with microcultures emerging as transcient and forceful from time to time. These microcultures are vigourous and celeberatory or of anonymous heroism, rather than sustained in tackling problems. There is also a merging of leisure and work - as with standup comedians, representing a more general ambivalence characteristic of postmodernity. Emergent microcultures, as city belongingness satiate identity needs while being benign and open. 
We argue for restoration of leisure, work differentiation, concrete permanent identities negotiated and conflict mitigated by meta identities that overarch and sustain balance. It demands an optimism in human conflict resolution abilities, of Chivalry and Grace. It demands commitment to institutions and acknowledgement of imminent conflict, capable of resolution by deliberation, specific appointed venues like sports meet, fair towns and overarching ideas like regional agglomerations.
We present hardening of recursive structures as natural and emergent and hence their explicit construction as being desirable, as just one point of our critical representation to the general trend towards postmodernist homogenity.





Elaboration

This knowledge would be consistent and objective and be the foundation for pragmatic engineering adapatations. The development of pure ideas is the realm of idealistic institutions. Once these productive ideas are put into use, there arises in the society free energy which furthers the imperatives of growth and incremental stability of all living beings. This energy is to be appropriated for reflection on how the energy is to be applied, which include ways in which such energy is dissipated by bounding off rational enterprise to certain arbitrary bounds of consistency, so that there are not made far reaching completeness propositions by arbitrary agents. 
This enterprise of administering the standards of consistency and its boundaries is important in that it embraces distinct knowledge enterprise and bounds it from the pragmatic enterprise out of rigourous epistemological examination as human relationships, art and culture. 
The development of such standards, their administration and delivery of the philosophy which upholds an arbiter of the society as an agent of defragmentation of a geographic, cultural people, by providing opportunity to generate surplus from knowledge as well as delimitation of the knowledge enterprise from assuming high degrees of completeness at the cost of consistency, which is often by fluidic categorization and open feedback loops. 
This arbiter does not derive authority on the society by political means, but by moral means, where individual dilemmas are better resolved by referring to the leadership of the institution. 
The institution proposes scientific atonment and standardization of knowledge by its actions, inspirations and publications thereby allowing people to boldly defy complex undercurrents in the highly  globalized, market oriented, postmodernist, technologically advanced society. 



This is expected to produce in the long run, self contained unique entities which consist of clearly delineated and well formed institutions that support the pursuit of problem solving in nature and documenting them as models, which could be adapted reductively in practical adaptations towards generating gains, while being aware of their reductiveness. It encourages the other side of such society to engage in cultural utilization of such surplus, by pursuing non utilitarian problem solving, cultural celebration and to build ways of romantic reflection in which problems could at its bounds be capable of appreciation. Thus, the moral leadership by an institution which nurtures the development and standardization of knowledge as an agent of development in the form outlined above is warranted in contemproary times. 
It is required to work without bias nor the proportionate enrichment of itself or its members in the pursuit of such actions. It would encourage the application of surplus in time and resources to leisure and spaces of leisure which would not simply be hedonic but to a greater extent, a dedication of time to arts, sublime knowledge acquisition and provisioning of arbitration of a decidedly pluralistic pursuit of ideals under one roof. Thus, pluralism, leisure and reflection attained through standardization and delimitation of knowledge, the inspiration and publication of conscious influential principles of human morality form the objectives of this institution. It may obtain necessary facilities to dissemniate its ideas, carryout campaigns and actions that influence societial direction etc.
The development of idealistic institutions in the society is of high importance as long as they adhere to moral standards and standards of pluralism and peaceful processes. They are important substitute to market oriented and computing oriented processes.


Supplementing RuleBased Expert system with Connectionist modules in MDM.
========
Consistency in causal modelling:
Rule based systems allow for parametric models, which can be scaled and transformed, such as models of engines where cc, drive wheels could be parameters and one can predict the torque on the rear wheel for a range of paramtric combinations.
Where a Turing machine is incorporated into the mechanism, there are two areas where things get complex, the while loops allow for potentially unending programmatic execution, where user events (such as tyre slippage to engage 4wd) are awaited. Sometimes the infinite loops make the program a service and is enabled as a feature. The presence of a mechanism that has no final macroscopic state transformation (though microscopic internal loops might exist) might not be deterministic. Hence consistency is compromized. The second point is the caliberation of events. Some of the events might be completely random (where user makes entries to a screen) or might rely on sensor digital signal transformation thresholds. The cutoff of continuous data from the environment might lead to inconsistencies in the system. 
Thus, inferring qualitative truths from quantative data passes through arbitrary filters (rationalized by versioning), binds the system to the context.
Dynamic extension of the system (rather than static self contained modelling) becomes possible due to indefinite waits for events. Coevolution and reflexivity disturbs consistency, indefinite movement defies snapshotting for reproducible state.

Apart from this one might also consider:
The reasoning process of FSM might be non markovian, in that it might not be entirely stack based and may write to external memory of its history.
Syllogistic reasoning over definite sets, expecting certain events while excluding others. Causality assumes distinct pathways to attain certain ends. Thus, torque might be increased by gearing or increasing fuel or fuel mixing. Each of these paths have independent subpathways which have no relevance for deciding an end state (high vs low torque). These strategies are additive. There might be independent causations for the same effect in which orthogonal strategies might be used. Say the decision of whether a customer becomes loan eligible might be due to domiciliary or employability pathways both of which might be necessary and sufficient in their own branches. Privy to each branches there might be subeligibility filters, such as domiciliary verification outcome event being handled in one branch while not in another. An ontology of closed exclusive sets, produce combinations that make inferring the algorithm inductively difficult.
A consistent system might be modelled causally. Causality assumes a realism and singularity of knowledge which 'is'. From an observation of the objective, one could construct subjective, applied normative perceptions in line with Cartesian Dualism. Phenomena are dynamic changes to a system, constrained by its parameters but not motivated by them. Thus photosynthesis ought not be there, but if be there, the equation is always balanced in molarity. Only by hypothesis and experimentation are phenomena exposed. The theory defines their bounds. There are too few phenomena that exist than the combinations of theoretical parameters might permit. These are explained by initial conditions. Thus, we assume a history (or a continuity from initial conditions) wherein sets have not come to existence all at once, but phenomena connect certain subsets in ways that is arbitrary. 
Phenomena do exist and perceptible to the senses. They might be interpreted in symbols and shapes such as with the periodical table. Non linear behaviour, discontinuities (as with rows of periodic table) cause phenomena. The question is whether phenomena be considered as special cases of theoretical interactions or they should be studied phenomenologically, where the observer is active and in which case it could be contested if phenomena do exist. It might as well have been constructed constrained by sensorial peculiarities of humans, temporal spans of observation of frequencies or they may be deemed to exist in as much as they are normatively interesting. The truth might be ambivalent between the two points. Some phenomena had been studied such as photosynthesis because of their utility, while numerous phenomena might lie latent.
Likewise humans are interested in avoiding accidents. Hence, they construct circumstances in which accidents materialize. Thus, it works not on a causal model, but on designating a network where certain observations/actions percurse or increase the risk of certain trajectories. Coexistence of multiple risk factors (while being dealt as additive in quantative risk scoring) might be seen as the state space of the dynamically evolving system. The instantaneous state space distribution posits evolution along certain semi seperated pathways. They might not be strictly additive (except over a large parameter model), but more of non smooth shapes which might be assigned qualitatively distinct symbols. Thus risk information knowledge could be so represented in the form of stable state space shapes (dynamic evolutionary stability). This might be a technique to extract causality.
But this technique often ignores system boundaries, where the contributions might arise from the context as well and hence the model might be phenomenological.


Techniques:
A technique might be to use skewness detectors at the outcome of each qualitative filter. If the resulting outcome of a path in which a filter F is a member is skewed then F might have to be recaliberated. A prior recaliberation by normalization (based on a random sample) is good for a given context. In case of a realtime adjustment by feedback, the skewness of the outcome distribution, say the outcome distribution is skewed in that the positive is highly likely and it is correlated with F values being highly skewed towards a positive risk indication. Realtime deskewing would require prediction of end state at each level, which would require a backward smoothing.
Further reading on d-seperation and do calculus, inferring causality by Pearl.
Planning as Temporal reasoning James F Allen

An analysis of Computing in the Knowledge theory of Development

=============
The idea is to find if knowledge is the knowledge of the real nature of the world. If the singular knowledge system becomes more and more elaborate, being able to form the constrained rule system or theory in which all phenomena get enacted. This would allow people to leverage these phenomena towards creation of ordered systems, which can then be able to neutralize the disharmony and create incremental orderliness and predictability in its place. It is the purpose to drain the universe of its disorder or unpredictability and be able to explain everything. The point is that while we are at it, all intermediate steps should be consistent logically. Thus, multiple explanations of the universe is possible, with as much spareseness as might be warranted, as long as the system itself is consistent.



Nature of knowledge:
Progress might be described as the continuous revelation of the real knowledge. Knowledge is real in that it is singular and exists apart from human imagination or endeavour to appreciate and understand it. This positivist and realist theory of knowledge is what we would consider as the basis of our argument. Thus, scientific systems specify the rules that form the constraints of a system. These rules are coherent with mathematics, such as the electriomagnetic fields being in coherence with vector fields. The rules arising from theories, which map to mathematical theories with logical consistency define only the constraints of the systems, but do not highlight phenomena. Phenomena could be discovered by observation (unbiased) or controlled experimentation. The phenomena would have to satisfy the constraints of theory, which allows for their verification. In humanities we might map phenomena to human actions constrained by legitimacy and legality. The axiom of choice makes it necessary that actions are observable and verifiable but not predictable. Knowledge or theory is hence derived and extended from objective and real things. In the case of humanities as well, soft sciences like economics rely on sound theories in the nature of systems theory, theory of agency and soverignity, the social contract arisign therefrom and so on. Medical sciences lie somewhere in the middle. Hence, despite having noisy observability and reflexive corruption,soft sciences might still have some sound theory. In fact the presence of legal theories is what allows for objective determination of cases.
Human pursuit of perfection or human progress involves an ever increasing access to the real fabric of the cosmos, unperturbed by noise. This allows to locate phenomena in different parts of the real world and humans can carryout projects in the nature of engineering such as bringing together wood and river to produce a primitive dam. This engineering prowess is constrained by rational rules but yet are phenomenological. 
It is possible for people to work with self referential systems, such as mythological tribes. These self referential systems have different notions of reality in a subjective sense. Human progress, particularly in the project of modernity involves a steady adaptation of knowledge that is objective and real, rather than plural and mythological. It had helped in cooperation at a great degree and great engineering inventions in its course.

Nature of Problem solving: 
Human progress could be defined as a steadily increasing ability to solve problems. Human problem solving involves three phases -> solution, accomodation and consolation. Solution is only part of the approach. Once a solution is provided based on the nature of the 'real' world, that is rationalized and mathematically consistent way, the problem becomes owned by all and hence it is dealt with by accomodation in the social level, by social support and encoded into more complex legislations to protect the underprivilaged. The purely random nature of the problem once established by a consistent arbiter allows for consolation by philosophical and spiritual reflection. It is also a process of seeking completion through romantic contemplation. Hence, a utilitarian frame of progress could be seen to embody a continuous extension of completeness of rational solutions while retaining consistency so as to allow for accomodation and consolation.
From a development perspective, if the solution part involves the use of authority without consistency in a rational framework, then the alternate course is one of vengeance. Knowledge is the ability to evaluate a material development from a consistent rendering of reality in symbols and graphs. It allows for liberation to accomodate and console over uncertainty in nature or to move with vengeance, where the phenomenon of authority is clearly anamolous from the framework of reality.
Computational Knowlege: From the point of view of knowledge, computation relies on pluralistic, self referential, partial recursive constructs and hence might appear as regressive. Computation involves looking for events, which are phenomenological and mapping them onward to further events emitted by the system. The presence of while loops in the fsm model, allows for event gathering and emission. Hence computing is as much about event signalling as data processing. Hence communication and information technology could not be seperated. A computational entity much like an accounting entity is a router of events without specific preferences. In more ancient tribal settings subjective narratives involved events which propogated to create history at every increasing dynamic interactions. But in computing subjectivity, the individual is fragmented and the subjective units exist in equilibrium with others. Programming to events, involve human heuristic actors, who map an incoming event to an outgoing event with a bias towards some optimization subject to the constraints imposed by the partial recursive computing system. Hence, people optimize by programming to certain 'objective' functions subject to the constraints. They give rise to events that are phenomenological rather than knowledge based.
Development based on phenomenological event aggregation and reaction, in a closed loop game, without reference to an objective reality allows for normalization about mean positions. There are involved random experimentaions in shifting away from such means as defection (as with cellular defection) that brings forth rewards at times. Hence, rules are sometimes broken and the resultant advantage leads to evolution of the system as a whole. 
Positioning of ML: The question is whether there exist a 'natural' mapping between events. This might be achieved by looking at the point that eventhough phenomena are free willed, they still could be traced from past events or history. Thus, one might predict that a fast driver would generate an accident event, rather than a slow one. Thus the choice of speed is an event that evolves in time to an event of accident with higher probability. Thus, probability is the study of events which are apparantly random, yet have some motivational reasoning, such as copying or reading from precursors etc. It detects non linearity in the generation of events, normally of tendencies in certain frequency bands that vary in density. They also relate probability distributions where non linearities due to latent factors emerge. The presence of bald tyres and fast drivers might indicate a latent variable which is a conscious disregard for road safety and hence escalates the risk of accident non linearly. This kind of uncovering of 'intentions' allows for mapping between some events and others. It might help in predicting risks of accidents say and make game like reward moves. In case if the ML system attempts to plug into the event field and attempts to normalize and build the non linear statistical joint distribution of the whole, it had infact generated knowledge. This knowledge suffers from two problems
a) Firstly, the notion of reward is instrinsic to it. It needs an objective function to optimize. Hence, it attempts to minimize accidents by putting a high risk score on specific combinations of events. By reward based signals it does not indicate a global standard but only a relative standard. Likewise where a single individual makes a 'causal' action, virtuously by training the drivers in safety, he is not recognized. Hence virtue and vice have no meaning because, to 'cause' has no meaning in statistical inference. This is partially remedied where manual programming is used, because certain special causal events might be programmed in explicitly. 
b) Human progress involves understanding intentional actions as virtuous or vicious, which requires an 'attachment to reality' or sanity. The whole idea of sanity might be cognitively interpreted as referrence to reality, in an objective sense. This reality involves hence being able to arrive at objectively good and bad actions, through such things as theory of mind and empathy. Hence we might say, computational networks (even when programmed as with credit rating applications) drifts towards amorality. This reliance on normal signals and not explicit disturbing causation and intention and its framing on objective concepts of morality leads to a setting where the solution part of the society becomes distant and intractable.

Sanity Crisis : An intractable controller (as against a rationally tractable or rationally unfair one) leads to a situation where humans would have to question their sanity. This might propogate as paranoid or delusionary states. But delusionary states themselves is not absolute and unprovable as with Kantian Idealism. Hence, the development of computing based social setting might infact allow for rational examination but with higher perturbations. The behaviour of phenomenological populations lead to a necessary pressure to follow least energy pathways and hence convergence of local narratives to natural objective realities. Hence, ML systems converge again towards greater knowledge exposition however encoded and interpreted at a different level. Humans might loose their access or touch with reality. It might however filter through the intervening layer of actors dimly.

The evolution of games: Games evolve when orthogonal participants who attempt to optimize locally without regard to global rules, develop some rules locally. The emergence of games indicates the affinity of participants to objective truths. Once a game is set in motion, it also happens that the participants mostly cooperate rather than defect. Cooperation is a strategy of voluntary suboptimal trajectory in common interest. Hence, an awareness of the wholeness, further leads to construction of new rules. Likewise a recursive definition might arrive where rules become defined at ever higher levels. It might also require that the rules be as close as possible to real and natural rules so that the game could be scaled further. Thus, the modernist revolution might be in the making of a repetition with pervasive computing. If emergent sentience is possible, the game participants at higher levels would be making reasoned (atleast strategically tenable) moves. These participants might be higher level games as well. That is to say rule bound entities might emergently make game moves strategically thereby rising truths. 
That is to say, modernity was for individual endeavour to access reality, so that the individual is liberated. It involved a reduction of reward based games on decisions concerning nature. It allowed for accomodation and consolation to be handled at group levels that coexist with an objective problem solving engine. The realignment of individual priorities to work with local narratives leads to a dark age, where wars happen for no known reason for the individual. Even if one is to rise heroically, the game between empires rage due to different narratives. The automatons in such a situation might dominate attainment of agency, make random moves to search for further rules to cooperate. It might be a transhumanist transition, if the ultimate tendency to access reality persists, or if accessing reality is an anthropic affair, the emergent bots would plunge back civilization into the dark ages. Humans might go extinct or sustain a resistance.
Part 2
It is also that ethics could not be formalized and rendered onto automatons leading to problems of convergence.



Interpreting computing in the developmental loop of theory-games-culture-philosophy
========

Events > Let the world be modelled as a consistent system, such as the atomic model of chemistry. Models are theories when they have definitions and extend therefrom in a manner consistent with mathematical continuity and logical deduction. Models causally connect by the continuum model happenings (good example are fields).Discrete unconnected happenings also arise. While theory defines constraints, it does not predict what would happen and what might not happen. Hence specific reactions that are legal within the model might be called as events. Events are random but legal. They can assume continuous quantifications but are discrete, but we will stick to truth values alone. Events are used by humans to construct causal connections, say a piece of wood blocks the flow of a stream, causing a composite event. Engineering envisages to bring together events which might be causally connected but arise at random and at distance (seperated temporally and spatially). Knowing connections derandomizes events, in that an event is no longer random but arises from another event. It allows construction of machines where events are connected in a less noisy fashion, such as the piece of wood being well finished and used as a dam on the stream, by perfecting the mechanism of connectedness seen in the real world (empirically while being theoreticlaly legitimate, otherwise the empiricism might be spurious).
Derandomization as Development > As people work with events for engineering ends, they attempt to complete the model using a feedback loop. The more complete the model is, without compromising consistency, one can see more problems being derandomized, allowing for engineering fixes by locating neutralizing events or composing into machines.
Bias and its institutionalization > However no model can be perfectly complete and consistent. There is bias in attempting to complete the model while sidestepping consistency. The bias is institutionalized in the market. Here games are defined as self referential systems of high consistency, which work on explicitly connecting events (unlike the theoretical world where events emerge). Reward pathways are encoded as rules against events arising from the environment, or real players. Thus, players attempt to work hard and build machines by solving real world problems and give rise to events which are rewarded. This might even be establishing ones eligibility for a loan or booking cargo on a system. The event that is initiated gives rise to numerous events consumed and acted upon by other players (like banks settling). Thus, games are hooked together by the 'while' operator which not only listens for events temporally streached but also to other game outcomes. Thus, the game as a whole might participate in higher order games. This elaborate construction over self referential games and rewards encourages optimization to local constraints (or rules of the game) rather than finding phenomena in a world of real constraints (of theory). The optimization is orchestrated to produce solutions of incremental accuracy to the real world.
Statistics > Induction of theory from events without mathematical intuition is statistics. Events might distribute in a population in terms of frequency of unbiased observation that have relative differences within specific quantifiable attribute of the population or rather intrinsic event. Thus, connection of events might be recognized by spatial analysis or temporal analysis by understanding covariance. The distribution of events seems to have connection with other invariate event. The frequentist school believes that the distribution of a variant is non random naturally for any point of view from a indexible variate. The bayesian school believes that interesting distributions arise due to excitement of latent variables on the network which influence the joint distribution in a non linear way. Thus, instead of the real indexed supposed invariate, bayesian is about joint distributions.
Pure Induction > Inductively dealing with events without the mathematical and logical intuition is what is questioned. Institutionalizing bias allows for democratic participation in solving problems of choice, by selecting suitable games by individuals. The outcome of such games causes emergent events which the individual has to make sense of (say a road widening damages his house or a certain medical condition is very expensive of being treated). The individual sees a certain bias to the system based on event orchestration. It does not hold a mathematical proof against bias as with realist systems. But more than bias, the problem is one of control. The individual does not have means to control the proxy system that substitutes the original pursuit of engineering solutions by knowledge expansion.
Development > Knowledge expansion as an original agenda of development leads to ambient control of nature, leading to genuinely uncontrollable events, labelled and identifiable as such based on the objective model at hand, to be capable of handled culturally by accomodation and philosophically by consolation. This process of control plus accomodation maarks a highly developed civilization.
Necessity of Realism > The motivation to work on expansion of knowledge is a tragedy of the commons scenario and is filtered of bias by the mechanism of explicit event based games. Games motivate cooperation which crystallizes as rules. Games rely on equilibrium rather than sovereign control (absolute right to turn off and frame policies). In order to cooperate universally however, a reference to a realist agenda of non zero sum nature as a project, a culture and philosophy is necessary. Universal cooperation relies on the nature of  progress as that theorizes the world objectively and likewise hold human values (acknowledging bias and noise) as capable of being objectively reconciled (as universal morality, virtue) based on mentalization theory. From the mentalization a realist theory of agency, action, social contract and an entire polity is capable of being constructed. The uinversal agreement over the nature of progress as involving knowledge and accomodative constructs of arts and philosophy is intuitive.
Computing and its impact > Computing increases the frequency of event generation and consumption.Hence the need for universal constructs is capable of being deferred. FSM is a game orchestration and integration schema. It is partial recursive without universal theories. It is able to quickly generate and evaluate events without the risks of mutation in transmission (low entropy and high throughput) associated with other modes. Hence, it can create highly specialized games which can solve natural problems. Games can be very incomplete and membership to multiple games (of consumption, production, travel) can help find meaning from game mates. High surplus from games while equilibrium is maintained at population level, can lead to detatchment from solving real problems idealistically. Pragmatism becomes the primary mood. Pragmatism sceptically views virtuous and vicious actions and is amoral for most parts. It accepts incomplete solutions and its mode of accomodation and consolation is one of temporo spatial distribution (in the long run things level off).
Machine Learning Impact > Programming of games retains the flavour of recognizing causation of interesting events, by humans recognizing special programmatic (game) flows as arising from specific development. Machine learning is the construction of natural links between events based on what is 'normal'. It is pragmatic and sceptical of extremes - suffering or meritorious accomplishment or vicious exploitation. Pragmatic models do solve real world problems by their high frequency capable of generating games around tough real world events. The derandomization of such events become a game strategy for such games (where nature is a player - identificaiton games by Verma and Pearl). 
Idealist Conflict resolution model in the world > The dismissal of notions of soverign control and their reconcillaiton over realist models of human actions and rights and likewise the non requirement of realist theories of the physical worlds lead to a new notion of development. It also removes notions of complete intermediate units (solution+accomodation) which run inevitably into conflict with other subjectively complete units (with high degree of attachment of individuals as being their identities). Fragmented identities in a pragmatic world does not give rise to conflict. Reconcillation of conflict in an idealist world would need constant pursuit of realist knowledge. 
Anarchism > The primary concern is that of an anarchist world and its potential for emergent authorities. The absence of policy and high frequency games or gamification becoming sufficient to engage people involves frustration, but it is a political question if this is better than the anxiety and grief over random violence by identity conscious groups. A culture of reconcillation of well defined identities is a conservative stance, while the use of upto the purpose identities without grand ideals is a democratic and leftist stance. We know that chaotic oscillations arise from near random systems. Thus as everyone has equal authority and has only their games to play, there might arise situations where the ability to make random moves (synonymous with sovereign authority) evaporates to higher level of game entities. Thus entities might generate random strategies and thus come to wield authority. The random moves might in the beginning be experimental defections necessary for games to thrive, but may come to define long standing conflict groups. Thus, there might be emergence of sustained conflict from transcient random jokes on the opponent.

Summary>Knowledge as purely Theoretic. Empiricism is only observation of events. Theory is unifying, derandomizing nature and morality. Theory is completable by games, culture and philosophy. Building on empirical events in game formats leads to weakening of downstream completing entities. Completing entities feedback to need for conflict resolution leading to further completion of theory while keeping the consistency. This loop is interrupted by high frequency inductive games, leading to interruption and anarchy, risking emergent chaos or decadance.

Interpreting computing in the developmental loop of theory-games-culture-philosophy


Developmental Loop- Place of pragmatic Completeness Seeking behaviour
========

Development is surplus derived from theories which would highlight events that could be viably utilized.
Progress in completing theories involve games that work with events without need for a singular and unified theory. Games work with emergent rules and are subjective. Optimization is used instead of solutions.
The third stage of development is where the engineering and the gaming arms unify and attain completeness by culture and philosophy. Completeness of games is attained by polity and of engineering by investment into a common infrastructure. Communities and nations arise. They are united by commitment to mythological consistent system at a lower level, but large organizations need to refer to realist knowledge as a conflict resolution process.
The conflict internal and external is reconciled by referring to the further development of theories. This is a feedback for further progress.

Now the question is with respect to the third phase. If a sufficiently powerful game could take care of surplus generation and self regulation without the need for common wealth and polity, threby also cutting the fourth phase of feedback, then is development said to exist?

Another View
Is it that divergence happens only in situations of isolation, the third phase of seeking completeness could be explained as impelled by isolation. Two points stand in contradiction to this view - one that spontaneous fragmentation of non isolated populations had occured microscopically as well as macroscopically in history, secondly that the world view of humans consist of two distinct parts in accordance with cartesian dualism, one of cognition and the other of reflection. Cognitive perfection is often supplmented by reflective attempts to construct ways in which completness of the whole ie the world plus the individual is aspired. This aspiration of the self is projected as culture and philosophy.


Refer : On the Inseparable Co-operation of Sense and Itellect for Arriving at Cognitions By Samuel James Rowton
From our look at the above thesis and the works of Ferrier, we find that knowing the truth is important as a part of civilizaitonal progress. We look at it from the point of view of philosophy being instrumental to conflict mitigation. The subject object debate helping attaining temproary completeness as an entity progressing.

Public Sphere Metaphysics
======

In our reading of the philosophy as the 'speculative science', championed by the early modern philosophers such as Descartes and Spinoza and well rendered by Ferrier later, we see that there is a speculation on the nature of reality and the self constructed through rational process, intended to uncover this truth, perhaps asymptotically. The idea is however that system building is important in philosophical inquiry. This is different from the twentieth century analytical philosophy in that the latter aims to explain philosophy without metaphysics ready for empirical verificaiton. In fact in the turn of the twentieth century, metaphysics had been much discredited due to the power of empirical science in demonstrating the amount of good things it could bring into life, that there seemed to be a waste of time to wonder about forms which have no practical relevance.
The need to wonder on such things as detached from its value proposition was argued by Max Weber much seriously and was contested by Rorty(The Politics of Intellectual Integrity-Richard Wellen). The argument was with particular reference to the intellectual independence of universities as trustees of the positivist effort of mankind as detached from normative values. The need for the pursuit of positivist knowledge distanced from rewards and even partisanship in promoting policy was strongly advocated by Max Weber.
Whitehead revived the need for metaphysics in the early part of twentieth century by stating that while it is fashionable to condemn metaphysics by scientists,actually they are only condemning the criticism of their own metaphysics.
The direction we head is to see that metaphysics of intelligence, knowledge and the ultimate burden of mankind to know the truth is attained by humans by rational means, for which metaphysics provides some support but is followed by accomodation of love and faith and the reflection by contemplation of nature. This is synanomous to the ninth epoch of Grecian philosophy which vested human ability to truth on Faith and Love. Therefore, one ought to content that the pursuit of objective knowledge is what allows liberation of mankind to seek such ultimate reconcillation away from the public sphere, within a closed community and in his privacy. 
Thus, we refute Rorty's idea of a pragmatic philosophy (where objectivism is infact perfected intersubjectivity) as being sufficient for public sphere discourse. We seek that a lively, well rounded critical philosophy seeking objective ends is in fact needed and debated in public sphere in order that there is reconcillation of universal concepts (not just time and space as suggested by Kant) concerning the nature of the universe and on the nature of knwoledge and truth. 
We might in fact see pragmatically, that such an advocacy for objectivism in philosophy as an expression of commitment to find universals, so as to foster peace and allow ourselves to differ only on what is decidedly subjective expressions of our self. Such a pragmatism is what we seek to consider the intervention of computing into this cycle of human development.
This leads us into the necessity of proving the existence (since we support rational philosophy, we attempt to prove - even if not rigourously, by analogy and by mathematical reduction) of a phenomenon wherein agents who modify their states based on phenomena alone, without abstraction lead to emergent macroscopic phenomena outside of their control. This abstraction we take it to be not as a continuum with subjective phenomenological knowledge (as intersubjective extraordinaire) but as a neumological universe which might be asymptotically attempted of approaching (to cite Whitehead) in order to result in a pragmatic outcome of avoiding macroscopic phenomena (such as population splitting into rival factions) and instead contending with 'intentional' different complete systems. This would also need of proof that humans need apart from reason - charity and hope. Charity is a part of the solution framework in a society where an oracle simply solves problems by mitigating it (conservative at a social level) and hope by philosophical insight which helps framing the finite inside the infinite.
Thus, we might look at the mathematics of homogeneous stable systems and see how they might spontaneously split rather than stabilize by self balancing. We might look at the emergence of games, on Lyopunov stability and lagrangian pathways (for which objectivism provides a way of reaching). Likewise much of our attempt to look at the situation stems from a social perspective. That is to say, humans having come together as a society, how would they avoid either a complete merger (evolutionarily this is feasible, the domain of absolute equality) while maintaining cooperation at conflict avoidance levels ( by sustaining liberty and free will, without that becoming defection). Thus the whole pragmatic case of philosophy is one of maintaining stability of social groups. This would need idealism apart from material engineering. The loops of development, involving abstraction of phenomena (as suggested by aristotle - who rejected sensual experience and instead embraced the method of the logic) resulting in isolation of useful phenomena (instead of a case where everything is phenomena). The production of surplus by exploitation of useful phenomena followed by the distribution of such phenomena by duly constituted authority into infrastructure and stable states. The idea of duly constituted authority here involves notion of intersubjectivity and more importantly the objectification of human intents - by the metaphysics of human wants and being able to isolate truth from falehood in terms of agency, free will and the whole phenomenon of social grouping. This would involve appreciation of the phenomenon of social grouping by culture - proding, criticising and reflecting on why we are together here on a continous dynamic basis, so that a stable level of cohesion is maintained without complete merger. Therafter cultures though capable of high degrees of communication and replication, themselves might run into concrete divergence, leading a feedback loop to the objectification of nature. 
Thus the two phenomena we wish to investigate are 'divergence' and the phenomenon of social stability - as being peculiar of living organisms of social nature and its stabilization away from the pressures of eusociality or even multicellular analogous evolution.
Some references>
A study of an ancient discussion in the Philosophical transactions of the Royal Society discussing on the stability of vessels by Lord Atwood in 1798 addresses the point the question that heuristics of observation and practice had been the general method of naval architecture, there is a need to employ theoretical models. This point of view supports the necessity of higher levels of abstraction as being of utilitarian necessity for development of novel designs (the same case as we had discussed on the need for identifying interesting events by having a theory). 
The book by Routh on Stability of motion discusses the point that a system might be stable if it is dynamic and a perturbation is not capable of dislodging it from the oscillations about its mean trajectory while a perturbation being able to diverge the trajectory (and possibly disintegrate the system due to internal tension) as being indicative of unstable systems. Instability is mathematically expressible and hence deemed to exist.
From these examinations and inquiries we see that the major point of contention in our work is whether phenomena are sufficient of interpretation based on pragmatic expectations or that a neumenic reflection is required. All the discussion over computers being phenomenological and do not go towards constructing explicit well formed objective rules boils down to whether there is a need for coordination of the society as suggested by Durkheim or whether markets serve the purpose well as suggested by Hayek (refer Two Views on Social Stability: An Unsettled Question by Birner). This question is to seek whether there is a need for a duly constituted authority in order to objectively express the views of the society. In such case, the idea of natural justice and other realist concepts come into play which could very well be extended to their use in explaining natural history as well. If the whole realist concepts are avoided, as suggested by Rorty, we might have it like rules emerge from market forces, or games where the rewards are tied to solution to natural problems and mitigation of violence over distribution. Thus, in the former case there is a need for explicit law and the latter case of emergent rules from games.
It is self evident that humans cannot avoid metaphysics all together. They could not help wonder who they are one why there are here, reflectively (speculate that is). Hence they ought to construct theories therefor apriori. This drive causes them to formulate ideal rules and models of nature from which they relate to the tragedy of human condition in being able to percieve only the noisy correlations between self and the object (Quentin Meillassoux ). There is a constant drive to redemption over the human condition to move away from reward based temptation to the pursuit of the ideal and perfect. The Alexandrian school puts the realization of such goodness (in a Platonic monistic sense) in Faith rather than reason. But it could be aysmptotically approached by system building approach (Whitehead).
Now, if we look that everything is aposteriori, mainly a reactionary stance to the usurpation and violence associated with the agencies vested with authority (the denial of apriori conclusions stand assault starting from Kant all the way to Nietschze). In this stance, we feel that games arise from pragmatic concerns and rules boil up and intersubjective towards ostensibly universal constructs. There is also a strong correlation between libertarian democratic historic moment of USA in the wider belief in the market and hence pragmatism (Rorty).
If one talks of redemption in such situations, one is in fact asserting Metaphysics. But the more intricate question is not whether metaphysics is required, but only that whether it is required in the 'public sphere' (Rorty). Does it require of people to constitute institutions which would promote idealism in order that a collective metaphysics is imposed. Here however constructive metaphysics might find difficulty, but critical metaphysics would stand acquitted (Whitehead, Weber). Hence, as to the question whether it is a publicly sponsorable intellectual occupation to approach metaphysics is a question of importance
The points in favour is first of the need for criticism of any implicit metaphysics to emergent situations. The second one might be concerning the way in which violence from authority not warranting a complete banishment of authority itself to result in less dramatic but pervasive violence (in pollution, economic cycles etc). The third argument is the explicit problem of the lack of apriori metaphysics in non human animals had lead to evolution taking hold of the situation eventually leading to the repression of the free will of the species. Hence, it might be uniquely human to seek an explicit metaphysics. This is also closely related to the point that human socieities are phenomenologically interesting stable systems straddling between eusociality and pure liberty. This phenomenon is needed of sustenance by a public sphere metaphysics. The fourth argument is the point that divergence in implicit equilibrium systems might lead to violence. The fifth point is the historical uniqueness of computing as a phenomenon which dramatically shifts the narrative in favour of self balancing and emergent systems in a one way path. This unique development would hence warrant an objective investigation in order that it does not become the primary influential force in history.


--
An Argument that Carteisanism is Zeigteist in the Computational Society
=========
We might even say that this shift to a dualist Cartesian notion itself is reactionary to the onslaught of computing phenomenological decisions in the conduct of our social lives, just as postmodernity and pragmatism was reactionary to authoritarian violence. It might still be argued as 'Zeigteist'. (William James vocally oppossed Cartesianism Refer https://journals.openedition.org/ejpap/415, A Critique of Rortyâ€™s Conception of Pragmatism
Paul Giladi). Here James had argued the incompleteness of the system of metaphysics in cartesiansim, while that it precisely what we appreciate. The incompletness combined with rational abstract consistency, allows a unification of ideals among different people, while they can then be charitable in redistributing wealth as well as seek redemption in romantic contemplation in faith, both of which are required for completing life, which we say is not be governed by any monistic philosophy such as pragmatism.
References:
To reject metaphysics is metaphysics (Pierce as discussed in Giladi).We must philosophise, said the great naturalist Aristotle â€“ if only to avoid philosophising. (CP: 1.129)



The nature of Intelligence:
1. Intelligence is the ability to survive. It is a feature that allows an organism to survive in the planet and it is never superfluous. It is strictly what is needed for the job of survival. That is the reason why we do not identify intelligence with complexity -  A complex weather phenomenon, even ones that are longlived enough to be considered alive (such as storms on jupiter) could not be considered alive. Likewise a highly complicated mechanical system might not be considered intelligent, even if even appreciating its complexity cursorily is difficult (admittedly it might be capable of understanding by effort). Hence intelligence might be identified with life itself.
2. An organism is identified by its form, its appendages and generally its spatial existence. Intelligence with respect to an organism is expressed when it responds to stimuli from outside world. This adaptive response to signals from the environment is an important aspect of intelligence. Thereafter the organism seeks to protect itself from the environment, in that it attempts to grow - which might be in the nature of protecting itself from corrosive elements or seeking elements that could cumulate to its biomass. Say a very simple worm burrows down when the sun gets hot, it is actually acting in response to certain events produced in the environment by seeking comfort and avoiding pain. 
3. But there is another interesting aspect to intelligence. Apart from simply seeking pleasure or good (in Platonic terms), it sometimes seeks the bad. Thus let us say an organism is soaked in a sugary solution, there exists a point where the organism actually wants to experiment the saline solution considered corrossive. This might have been by the propogation of spores or probing with appendages, but we might see in many cases an inverted behaviour of experimentation, in search of better goodness, closer to the ideal. That is to say, we might account for these behaviours by a mathematical formulation wherein the organism has an internal memory which is an open ordered set of good things. Hence the organism ranks its experiences from good to bad and seeks to expand this continously.
4. Thus, the organisms internal memory is constantly programmed by events being generated in the real world, which we might not be inaccurate to consider as a giant computer. Nature or earth itself is able to produce a great deal of paralellized events and it had taken four billion years of such events to allow the organism to get the best set of goodness. In fact many versions exist, we will come to that in due course. What is to be remembered is the great amount of computing effort that had gone into the production or incubation of the organisms evolutionary tree. One might see that some of the events on the earth had been catastrophic, but they might all be very well conjectured to the strategies of adversarial event generation in a generally hostile environment with an uncertain strategy. The earth itself is an open system with the universe and hence it might be the universal computer in action.
5. As the evolutionary tree progresses, in each stage an organism is faced with an universal dilemma, that is the dilemma as to whether to expand and search for something new in terms of rewards (or disprove the pain source) or to consolidate its existing position. This dilemma is reflected in our partisan political drama as well, despite the epochial ontological seperation. This dilemma is addressed from time to time in the course of living.The enterprise of political conservatism is not regressive, but a consolidation of existing positions while that of the left is porgressive.
6.It is also important to note the universal thread running across life and intelligence. It is a popular ontological expression to quote the inability to know how it is to be a bat. In fact it is not that formidable a problem (say as against imagining a stick with only one end). It is possible to see that a bat, or for that matter, even a tiny bacterium is alive by its behaviour (its expression) in an empathic manner in response to pain or in appreciation of pleasure. Thus, we had in art had often been having bats and other animal characters in children fiction.
7. Hence, the dilemma of consolidation verus progression is presented universally and this had been in evolution observed as the competition between subjective knowledge that causes defection (or a random dispersion) and thus crossing over into an isolated territory of greater privilage causing divergence of species. This high degree of variance and subjectivity of this dilemma had been instrumental in generating macroscopic phenomena. That is to say emergent consciousness arises from multiple agents attempting to stay together and thus be equal or to seek their own understanding of the environmental rewards and pain (thus at their liberty) and seek to get a better deal and spawn a new evolutionary pathway. This kind of tension that exists among living creatures and the universal dilemma over regrets of missed opportunity, powers macroscopic emergent phenomena of intelligence. This is particularly well reflected in multicellularity. There had been a consolidation of common knowledge in the central nervous system with the surrender of local agency. We might see that eventhough there is an unicellular origin of the organism, subsequent differential expression of genes for tissue specialization, effectively creates agencies that might be adversarial or orthogonal and the emergent confluence of rules of the game might have been the conscious intelligence of multicellular creatures. Even here the defection is handled in a graceful and controlled manner, as with germ cell defection (see C.Extavour).
8. The ability of emergent agencies to produce intelligence might give us clue to the direction of future intelligence. But before that we need to more precisely distinguish natural and artificial intelligence. Given that we see that the emergence is caused by paralell programming over billions of years, we ought to see that intelligence is a hard won phenomenon. We might however be lead to think that it is Markovian given the concise way in which it could be encoded in the forty eight chromosomes of human genome. We might also see that the different brances of evolution are essentially independent and despite the practical dependency on the biome for human survival, it is not theoretically required and hence we might argue that the human state is Markovian in the development of intelligence and it could hence be created without so lengthy a history.
9. However, one needs to elaborate on the nature of senses and perception and expression in order to reflect better. When it comes to intelligence, there is primary attention concerning the prefrontal cortical processing and the inputs by well defined senses. But there exist senses that are deeply embedded in our systems, which give rise to signals and trigger behaviour which is not encoded in the rational faculties. It might be a feeling of being fully fed or hungry. These might trigger specific enzymatic pathways at the cellular level, bubbling up to produce complex influence on preferences and biases at the CNS level. All these are parochial and relate to our legacy of millions of years.
10. In more visible terms, we handle this heritage by expressions that involve our fellow people. A smile means something, that could be inferred from certain learned behaviour, but deeper and more influentially it has roots to our, say five million year past of percieving facial reactions and perhaps a billion year past in appreciating symmetries. Even though the argument of Markovian nature of intelligence might be tenable, the selection of individual traits are from an extended sample of a billion year temporal and millions of units of spatial surface. The cumulative effect of all this parochiality and heritage to our 'gut' intelligence is figured out in the concept of culture. A smile in a market place has so much meaning encoded to it, that reflects back to our microbial past. That is why culture is considered a highly loaded word in english language. Likewise when Turing figured out a test for intelligence, he did not postulate it such that a computer that is capable of answering factual or logical questions that could match a certain category of person to be truly intelligent, but rather said that a computer should be able to fool a person to thinking it is infact a person. Now this is a cultural factor and this parochiality is what Turing cleverly and concisely put into his test.
11. Hence we might say intelligence of an artificial nature is in any case shallow and sparse than natural intelligence (unless a cheap universal simulator is found). We might however look forward to intelligence that can simulate human intelligence to a certain extent, as Turing expected. This is captured in the idea of weak artificial intelligence.
--
Goal based intelligence:
1. In case if we do not express a given goal as derived from the survivalist goal and the dilemma of consolidation (could be in philosophy extended into the existential dilemma, the political dilemma of equality vs liberty and so on), then it might be a  narrow goal. A system that attempts to solve such problems might also be called intelligent in a limited sense.
2. If we start with the assumption that universal apriori truths are not perceptible directly. They exist in fact, but due to the work involved in accessing them, that is the reliance on phenomena to access the neumenological truths, distorts them. This is due to the fact that the work involved is difficult of estimation and the reward seeking behaviour of the expressing agent naturally distorts the truth. Hence, we might say context free truths are not useful. This axiom could be extended to mean that there does not exist a finite set of truths which could be mapped to a finite set of contexts in the form of truth table does not exist as well. Hence, all that we have is belief.
3. We might intuitively say that in that beliefs in that they are biased, might be surmized that they intend to point to the universal and natural truths when a large number of active agents attempt to frame and promote their beliefs. Hence, a critical frame is important to make sure the beliefs are as close as possible to realist truths. The development of a belief propogation network, wherein beliefs are simultaneously propogated along multiple alternate pathways to result not in a binary decision, but a shape of a joint probability distribution is hence a good step in emulating intelligence for narrow goals. The directedness and acyclicity of the graph is a requirement in Bayesian theory. However, reflexivity and revisioning of past evidences either from an impact of the organism or by paralell test moves is closer to the real world. Thus, a system of agents could only produce intelligent perception of an environment. How this happens in unicellular adversarial search (or game rule emergence) might be further inquired.
4.In case where a realist model could be approximated apriori rather than constructed a posteriori (in which case we assume that the emergent rule system corresponds with the universal Lyopunov attractor which represents the truth). The concept of aposteriori truth is captured in such expressions as truth alone triumps, where truth is seen to be animating and representing the telos or ultimate stable state of a system irrespective of initial conditions and storng perturbations. 
5. A causal model preponders a system of unified connected completely like components (quantifiable assortments of finite set of components) each of which could be expressed either as scaled versions of the other or aggregations of others. There is a notion of observer and frame of observation where the components interact to produce quantifiable results of the singular quality in the pursuit of the observer. This could be framed as an equation such as (ax+by+.. =n). A system of equations of polynomials reduces algebrically the world capable of being perturbed arbitrarily and observed for outcomes. Notions of system of equations denote incomplete systems which are represented by finite observations and with a knowledge of required conditions (constrains) could be optimized to produce sufficient conditions for a certain outcome. Smoothness and continuity are essential for the causal model to operate, allowing the calculus of infinitesmals to be able to represent the system coherently in a rationally accessible manner.
6. Metaphysics had dealt with the merits of apriori assumptions and that of a posteriori reflection. There had also been movements that question the necessity of metaphysics itself. This had been the case with pragmatists. Some of the pragmatic movements themselves, such as with Rorty had been criticized to be dogmatic constructing its own metaphysics (Giladi and others). If we look at pragmatism as a metaphysical stance, we see that the dilemma of existence itself is called into question. All the apprehensions and anxieties in the world over regret of missed rewarding opportunities are constructed in this metaphysics to have been introduced by philosophical ruminations, metaphysical rants and power structures which tend to harvest such inadequecies and anxieties. Postmodernity in fact proposes to reject any authority, including appeal to reason itself. Hence, metaphysics itself had been criticized in favour of systems where whatever the truths be, they are embraced as being completely historic with no element of possible ergodism to them.
7. The assertion that there is no apriori reality allowing an observational frame of reference divests  rigour and custodians of truth of power in the social domain. The rejection of universal attractors of aposteriori truth, rejects any pattern to history, of any narratives from history. It lets history be what it is. The complete historicity of intelligence likewise also disallows the asymptotic convergence of natural and artificial emergence. But we see that the question of existence of either of these frameworks of reflection is concerned with the question of whether evolution is to be regulated.
8. The most interesting domain of evolution as of now is social evolution. The society had been discussed as being a spontaneously evolving system in vanguard literature. Hence, the question of whether there is a need for metaphysics is to be evaluated with respect to the pragmatism of control of an evolving society. The real pragmatism (such as with Pierce) is not a dogmatic rejection of metaphysics but it is a question of optimal choice of control. In that the most domiant question of the consolidation dilemma is social today, given wars, economic slowdowns, pogroms and pandemics and the choice of progression of science all concern social decisions, public opinion and policy, the fundamental question of metaphysics might be one as to whether there is a need for an objective observation, either from a regultory point of view (as with Durkheim) or in terms of emergent markets requiring nudges (Hayek). In art, the rejection of metaphysical anxiety had in fact produced some heartwarming tales of happy go luck heroes and the age of innocence. Theology suggests the movement towards such points of renunication of anxiety (as with the Alexandrian school). It is also the concept of redemption from the human condition, wherein the dilemma is rejected at last, so that one is able to do the right thing and attain the highest possible moral state.
9. Thus, we might speculate that there exists a sentiment where there is a fear of going down the wrong road. The presence of such sentiment ontologically, introduces a need to collectively speculate on the control of the organism of the society, as projected from the individual dilemma. 10. Social constitutions in their most stable states are the most peaceful. Desirousness of certain state for any coherent intelligent unit (which at this point is the entire humanity) would need control, either by external coordination and allowing an emergent coordination as with markets.  Even in the latter case, there is need to visit redundancies and needless oscillations. Thus in the true sense of pragmatism, we maintain an ambivalence, as to whether apriori truths exist or not, but we need to take a stance that would be reactive to specific drift directions, constituted by specific beliefs ascending the role of absolute truth, both by the weight of authority or by feedback loops. 
11. Hence it is with response to the specific phenomenon of sociological integration in the present moment of history, that prompts us to adapt a particular stance towards control and collective reflection. There does seem to be a specific dogmatism to postmodernism that explicitly guards against metaphysical reflection. Secondly, the development of the electronic computer dwells exclusively on the phenomenological world, in that events are generated as  proxy to the universal phenomenological events and adapted out to feed onto event mapping agents, who work with various degrees of autonomy. 
12.The autonomy of computing systems are reflected in the time parameter to their functional expression, implicitly introduced by the open ended 'while' loop in the FSM model as it works over networks to produce results that are unpredictable to a good degree, making them difficult of appreciation and hence coordinated control. The proxying of events which is then funneled to intelligent agents might result in emergent collective intelligence which would favour extending the bounds of experiential set of life itself, a higher consciousness at the level of the society. This is probably the path of artificial intelligence in the future and in our opinion it is historically unprecedented due to which the most pragmatic course would be to revert to the notion of apriori truths of the early modernists. 
13.This would likely allow construction of coordinated control, so that computing could be interpreted not as emergent intelligence, but be capable of answering the query of an individual to the strengthening of his liberty.
14. Hence, this might be a construction towards self determination of human agencies and the telological nature of convergent redemption as being asymptotically far away, yet accessible by romanticism and spiritualism in the sense of yearning. The yearning itself might be redemptive and hence might reject concrete effort to realize the ultimate path. This would mean a consolidation of present position, using a criticism of emergent clouds. The emergence of automaton agents which do not explain to individual queries, might not be superintelligent. They might simply contribute to the emergence of a collective system of intelligence, which is akin to multicellularity. Metaphysics of modernism allows the enlightenment of the primacy of reasoning to deter such progression.
15. The computational conception of decisions is essentially normative, tending to embrace what is normal and rejecting the interesting cases as being irrelevant. That is to say, profoundness is sceptically viewed and ignored. On the other hand, control is dependent on making rules about extremes and leaving the normal out of the public sphere. There is a reward of extreme accomplishments and penalization of extreme transregressions, as for the wide band of normalcy, people are expected to negotiate and reflect as libertarian beings. In a computational network taking decisive direction (rather than cancelling out noise) on the course of evolution of the society itself, we might see the rejection of extremes to be impeding the development of theories by supplying missing pieces, which would come back to promote normative goals. Thus, there is low latency in normative realizaiton in the system, leading to an evolving systetm that does not plan and hence does not control. 
16. Thus, it is essentially a destruction of planning and construction of ideals expressible as hope culturally. It arose paradoxically from the establishment of inclusiveness and equality around globalization, civil rights of the late twentieth century (network theory of Castells).
--
Part 2 of Goal Based Intelligence /Transcendent intelligence

1.We need to start our inquiry into intelligence from the point of temporo spatial intelligence. Firstly if we look at the spatial intelligence part, we see that as of now, some machines are able to identify discrete objects in its vicinity, only by drawing a random sample from the environment. We have seen that the MCMC method of random sampling is extremely successful in many cases. But in many machine  learning cases, there is reliance on humans to sample from the environment and provide it to the machine. The fundamental mode of operation is however the same, whether it is machine drawn or human provided. The machine performs a neighborhood search assuming a statistical gradient of smoothness and idenitifies discrete breaks and marks them with boundaries. Thus instead of ending up with a multimodal mixed distribution, the machine is able to perform a neat classification job. 
2.In machine learning tasks, we see that there is a classification between classification jobs and continuous regression jobs. We might say that classification jobs when applied densely and quickly might replicate a regression job, by inferring a line of seperation that does not comply to any mathematical smoothness standard. But in practice, the reliance on smooth delimiters or trend setters as classifiers are useful. The establishment of the trend line or some such sample also helps appreciate the continouity and gradient which itself serves as inputs for identifying discrete breaks.
3.The normal computational task, we might say is control oriented. The computer attempts to create the spatial reality by using recursive classificaiton. That is to say, if a computer is assigned a task of allocating the fertilizer to a heterogeneous field, it first attempts classification of the crops involved by observing continous and discrete variance and once such discrete parts are identified, they could be tracked temporally in order to decide on fertilizer allocation. We might see that in order that the problem does not run into combinatorial explosion, the following are required. One is the need for constraints (supplying the required condition and then searching for sufficient condition) which makes the problem optimization problem. Secondly there is a need to classify heirarchially. A high level classificaiton ought to arise form immediate classifications such as maize and rice going further on to classification between fields and woods. This kind of classificaiton reduces decision making such as the decision first being whether to apply the resources to woods or fields and in the case of latter further evaluate whether to select rice or corn.
4.But the tricky part in temporo spatial intelligence, is that the classificaiton is not entirely divergent as in a tree. It frequenly converges. When a disease is tracked temporally, there might be multiple pathways that converge on to cliques. These cliques are interesting only because we denote them such. They are the goals. It might be that the when there is a heirarchial classification available in a forest, such as tree based food sources, root based food sources and in both cases, such as badam and groundnut, there might be a convergance that a single outcome is present (semantically a single object, say a nutricious nut). The nut is interesting only because it has some goal based significance.
5.In AI research often there are reference to accurate goal specification. At a higher level, the goals might entail multiple temporo spatial pathways (such as the goal being obtaining nutricious food source rather than gathering nuts). There might be a cost effective search for the best solution. The search might be a breadth first search, where diverse routes are picked up or a depth first search, where some preliminary attempts are made in test mode to futher investigate a route. The point is that in order to control the environment, all problems eventually get reduced to logistical problems. Hence temporo spatial intelligence explains a great deal of what is intelligence.
6.However humans and other creatures seem to be pursuing goals which are only intermediate to a higher level goal, of biological satisfaction. The itermediate goals are dynamically arrived at by the interaction with the environment. Much of the effective environmental feedback is obtained in the case of humans from fellow human human beings in contemproary times (most problems are distribution problems). This feedback allows specification and ranking of intermediate goals to helps satisfy the biological goal which might be simple but colourfully rendered by environmental interaction with the world at large and peers. Thus a machine that needs to emulate human intelligence ought to be specified such high level goals as survival. But the goal of survival requires a specification of pain before that of reward. Hence, the feedback is to be supplied by human agents, for which case, the human agents ought to be more powerful than the computational agents. They ought to either provide physical feedback by interactions in the real world or have to provide control signals which are intelligent enough to be considered completely random (as we infer the environmental signals from the universe at large, barring a few well known ones). This is the primary area of challenge, which leads us to suspect that we might not be able to provide selective feedback, leading to our being overwhelmed by a machine specified of goals similar to biological goals.
7.But it might be speculated quite reasonably that human intelligence is transcedent. In that all human goals could not be specified by biological goals. There are instances of pure altruism, which might be inferrred to indicate that human intelligence had transcended biological goals and are now referring to goals higher than the biological goals themsleves. Apparantly the goal of biological process is to build order and knowledge from randomness and disorder. If such a higher order goal, say of justice and liberty could be followed by humans, then they are circumventing biological goals. This requires a belief in objective high level goals, which we donâ€™t have methods of encoding into machines. Perhaps it is an emergent outcome of sufficient intelligence, or it is historic, we can only speculate.

Dilemmas of control and the question of method
1.We have seen that control might be feedforward or feedback based. One important dimension to control that is often ignored is that of reflexivity. That is to say, the system attempting to control another system slowly harmonizes with the other system as well. Let us take the example of a village in a forest both being distinct well bounded system (even literally). The forest is likely a highly complex system, in terms of lot of microscopic and macroscopic paralell process (microbes inhabiting the forest bed have their own cycles as do flowering plants and migratory birds). The village however converts the forest into an orchard, by selecting a single species of fruit providing tree and weeds and clears the forest floor frequently to avoid subjectively unnecessary cycles. This makes the system less complex and more controllable. But it also has the effect of reducing the complexity of the village. Now that weeding and fruit gathering becomes the primary occupaiton of all, fortunes become less heterogenous and everyone synchronizes over activity, making it factory like.
2.Thus we might say that the idea of control over an environment might itself indicate a harmonization. There are often themes in culture where the latter is suggested over the former and a strong indication that the idea of control might be in itself futile. The idea of reducing the complexity of an environment by reducing paralellsm in the environment is the seleciton of certain process over others. Therefore while macroscopically more units of the same work is seemingly done, we might not say that order is increased. The idenity of a product in the forest is now vauge, eventhough we might group it with certainty (say as mangos or guavas). Therefore orderliness itself is a macroscopic machine which crushes a lot of microscopic processes. Likewise, even in the case of any mechanical arrangement such as refinement of components and their being coupled to create a mechanical advantage, the point is that the macroscopic order increases microscopic homogenity. 
3.The homogenity of the village is offset by the exercise of choice. The fundamental choice is whether to consume or save. That is to say, people decide on the surplus now flowing into the village as to be capable of being consumed or be stored away (in expectation of worse conditions in the future). This might not have been relevant where there had not been any surplus, or where a forest is foraged, the forager typically stops when is tummy is full and relaxes or frolics. But in a society that is excercising a control over the other system, there is a flow of surplus, which is capable of stored. The stored surplus becomes the capital. In such situation, where the future becomes worse, as it is natural for poulation to grow, even if the fortunes donâ€™t change, the capital plays an important part. The choice is presented between whether an individual would like to borrow someones capital in order to wade through the worse time, in which case there is an obligation involved, which would be a transfer of reward in order that the past forbearance is compensated. This penalty is exacted in an unequal fashion,because of the assymetry of choice. In forbearance free will was involved while in the case of resorting to borrowing there are circumstantially strong elements. This behavioural trait of mankind might have been a powerful determinant of civilizaitonal growth. Hence, the individual set to borrow is presented with a choice to improve his technology so as to get additional outcome in the tough times (or with more mouths to feed) or borrow. He might choose to innovate and experiment and thus it might happen that value flows from mechanical advantage. In that value does flow from nature by mechanical advantage, one also expects that a value to flow from reserves which could be translated to technology. Thus, capital and technology are mutually transferrable and since the latter can appreciate, the former is synthetically composed to be capable of appreciation. 
4. The nature of understanding of the world in humans as we had seen involves the use of statistical gradients and their discrete breaks in order to identify distinct objects. But there is a third element needed in order to arrange these distinct components heirarchially. This is the notion of quantiles. If the stochastic search is capable of distinguishing between different standing crop, the discrete jump in height might provide an objective basis of classificaiton (such as height being a linear quantile itself). Thus, there is an imperative for reference to objective organization of the universe mathematically in order to be able to arrange components heirarchially and thus be able to overcome combinatorial thinking. This is strongly in suggestion that intelligence could be obtained from sampling randomly or statitically, that is to say by empiricism. Empiricism involves understanding the universe by sampling widely and also in neighborhood based refinement. But it involves theoretical notions in order for the things to be connected in the form of sets. This presents the everyday way of understanding the world, to sample and classify into components and arrange them heirarchially in an objective quantative basis. 
5. This is actually formalized in the scientific method, in that empiricism and theoretical reasoning is decoupled from feedbacks. That is to say, in common practice an individual reasons on the go. He is able to combine empirical observations (wide search) and experimentation of specific components (deep search) with some preliminary theoretical format and corrects it based on feedback obtained subjectively. However, in doing science, the individual is expected to present the idea obtained in such a way that it does not involve subjective feedbacks. It is presented to a reasonably objective audience and capable of being reflected upon as a necessary conclusion from the situaiton, irrespective of what it entails. Thus science promotes a method of feedforward control, where an ideal image of an environment is obtained before sending signals of control by predicting inputs (rather than responding to it). It is like braking down, seeing a curve ahead. This would involve a non subjective model of angular forces coupled with specifics of the system being managed (say their adhesive or traction specifics).
6.The major limitation of feedback based control is the problem of subjectivity, in which case only equilbria might be reached (due to the dependency on the states of both the systems of the observer and the observed) and not a mathematical truth be emulated in practice. In case where feedforward is relied upon, reference is made to pure and consistent forms of knowledge. For instance mathematics relate things by equality over scalar or angular coefficients. Likewise, it attempts to establish symmetry of the units by aggregation (which would necessarily involve a null space where nothing exists). In logic there is a stress on inequalities rather than equalities. An existential parameter is specific and null spaces do not exist. Thus, these two formulations of the ideal, abstract world helps in constructing ideal instruments which could provide future states of the system which would hold irrespective of time and place. Therefore, they could be freely scaled or transformed to suit specific situations (thus establishing their smoothness and continuity which are hallmarks of mathematical functions). The application of mathematical functions to situations allow for appreciating fixed structure of mapping between entities, which are smoothly transformed in dimensions. This appreciation of the whole, removes paralellism and simplifies (functional composition is possible) the interface with the world. 
7.We might also digress a bit to note that the idea of speculation over uncertanity is captured in savings, which gets transformed into capital if the risk does not materialize as foreseen. This capital hence is capable of freely flowing to finance technological innovation. It might be simply fruits or seeds. As we had earlier indicated, currency is not caused by the soverign will, but by the necessity to hedge or buffer uncertainties. The negative outcome is absorbed by the reserves, while a positive outcome allows growth. Thus growth is a side effect of this strategy of reservation to worst case complexity. Reserves which are not applied as investment in good times, but simply kept as reserves, is the hallmark of non living things. Living creatures speculate on the environment, reserve against worst case situations and grow by mechanically leveraging the reserve. The loop of not growing in suboptimal times and reserving it to be applied in optimal times, is one of control, by feedforward speculation. All control strategies are aimed at reducing complexity. Reduced complexity means simple machines which pump heat in a steady flow.  
Part 4
The idea of dilemma of control â€“ the role of Computers

Where we start out is to state that philosophy is the study of universals. Universals truths are not possible in any scientific inquiry or normative inquiry. They are all constrained by the contexts and intents. We might also philosophy as ways in which we get the intellect to cooperate with the instinctive, or the innate. That is to say, while philosophizing, we invoke rational constructs, which helps us see the nature of truths, which could not be reached by material means of experimentation, because it involves either the observer or it is universal. It helps people to see things, which they already know, that is to say, what is understood sensually (not restricting to the five, intellectually comprehensible senses alone) to be supported by a transcedent intellect. That is why Socrates did not believe in documenting his works. He simply believed that he was teaching things that are already known to people, which they would anyhow realize (it being unique and singular) at any point in time in the future, if people were to put their mind onto it. Thus, he was in favour demonstrating what is philosophizing rather than philosophy itself.
Plato was also in support of the existence of perfect knowledge apriori. All the study of philosophy was for him, simply remembering the times of perfect knowledge when one was one with infinite goodness of God. Thus, Plato was supportive that the role of philosophy is only to enlighten us to the fact that we had already known and the distractions of the world had just clouded it temproarily. Kant, who was unsparing of the possibility of external observation in a Cartesian sense, where he proclaimed that conclusions from rational sciences are due to only the construction of schemas and axioms to support the conclusions, was still accomodative of the presence of apriori moral truths. This was in fact his subject of critique of pure reason, where he said that the rational process is made in self referential framework, while the real drivers lay outside and away driving actions.
In the postmodern times, we see that there is an emphasis on the nature of truths being subjective and highly contextual such as like doing the next good thing, instead of planning and meditating on what is goodness. But the availability of knowledge on the next good thing, for an arbitrary time interval does not logically preclude the availability of similar knowledge on goodness for a variable length of time, to varied people and varied times. This heterogenity of access to truth, eventhough the truth is one, allows, hence for people to gain greater authority over the truth over contexts and hence be capable of exerting power and authority on people. However, the problem with the awareness that one is better aware of the truth, distorts the truth itself and the authority and the symbolism associated with it, either in terms of rewards in material or spiritual world causes the truth that is disseminated from authority to be paradoxically corrupted. 
Thus, we might see here the importance of the rational method, to cite Ferrier, Whitehead and Russel, to construct philosophy from Logical components, so that one can frame it in rigourous formats. This ability to do so, allows the rational to seek the absolute truth, rather than being open to only short term rewards or survival in an evolutionary sense. Thus, there is seen an effective utilitarian case for philosophy, apart from being simply consolatory. This rationality allows an influence on policy and techniques so as to produce values, eventhough philosophy itself is an exercise in self referential way, such as enacting a drama or a story to lead people to the knowledge, they had already known and only strive to remember.
Engineering we might say serves as a moderator for peoples desire, or fantasy and the finitude and the definite bounds of scientific phenomena. Engineering allows optimization, in that some part of the wish is realized within the constraints of science. Thus, as people wish to solve problems and they are encountered with complexity, in the nature of small things leading to catastrophic results, as some small move encouraging a crime or accident, people have no way of knowing or avoiding such developments other than to rely on providence. In that philosophy enlightens us on the falliability of rational processes, people would seek the guidance of providence and praise its glory. That is to say, as one philosophizes and sees that the rational methods are intended to actually convince the intellect in helping see the apriori fact, that is realized when one puts ones heart to it, one starts paying attention to the finitude and falliability of rational knowledge. 
But it might not be that one has to entirely rely on divine guidance throughout. Even Descartes himself was propounding that people were endowed with a capacity to self determination and free will by providence. Therefore, humans might attempt to build techniques in order to encourage the situation where a person could with less distraction listen to the inner voice. That we also appreciate the localization of knowledge and context sensitive decisions. However, as we discussed such distributed intelligence might also give rise to oppressive structures, such as caste based structures. It might hence be a useful technique that such structures are weeded out. That is to say, if there be such oppression in a certain region and a central common sense says that it is evil, then centrally resources might be directed in order to form local groups and finance them by state budgets to serve as a vigilance group over such developments locally. Thus, we see there is a promotion of monopoly and centralized knowledge in order to achieve the ability to appreciate the falliability of such knowledge and delve on the conerns of the soul.
But such rational and singular constructs, themselves are rooted in complexity. A government that believes in singular truths, such as to have a motto of truth alone triumps still relies on multiple verisons of truth, by checks and balances and paralellism to stabilize the setup. Likewise singular pursuits of truths ought to coexist with local and plural perceptions in order that the narrative of singular truths remain stable and unbiased. Hence, we might say, philosophy is difficult of application technically, mainly because philosophy is not definite knowledge, but speculative. One has to argue on the basis of invoking Plato or Aristotle or Ferrier and not on the basis of findings in the material world or deducing from theories. But specualtion has its place, as was discussed in Russels Kettle analogy. The point is that while philosophizing could not happen to be definite knowledge, due to its speculative nature, which itself prevents philosophers from agreeing among themselves and in a more contrasting manner the disagreement between people who idealize and who are pragmatic and content with practical reason. Therefore, this nature of philosophy prevents development of theories so easily. But in fact they had been important in the development of theories as was Cartesian Geometry and the important contributions of Leibinitz, Aristotle and people lke Bentham, Mill and Hume. 
Thus, philosophy inspite of its speculative nature, leading to divergence, has as a convergent core, the need to reflect on the falliablity of knowledge and the need to find such apriori truth, by describing the singular problem of people needing of ways to be reminded of what they are aware and keeping off distraction in the form of intrusions from the physical world. Hence, all philosophy might be zero sum, self referential, but it is an exercise in strengthening the brawn of the mind to be aware of the inadequacies of reason and hence the need for complex roots which could serve as infrastrcuture for rational superstructures. Thus, if rational superstructures of a society should fail in edge cases, such failures are durably absorbed by the population in charity, consolation and even as a challenging force for further inquiry. 
We might summarize this necessity for paralellism and complexity as the root for arriving at singular bodies of truths in the idea of liberty. Thus, we say people be let free to think in their own sweet way, inquire from there, desist from inquiry if they will so and listen to their inner voice, understand the importance of apriori moral knowledge. We see that the principal problem in their doing so, is the emergence of authorities, where the embrace of liberty is heterogenous leading to emergent heirarchies. If there exist a singular structure, a way of rationality, which could be deliberatively stabilized, along with its antithesis of plurality, we might speculate that both could coexist in a synthesized equilibrium. 
We had also discussed the work of Ferrier in asserting the inseperable cooperation of the intellect and sensuality in the production of knowledge. Hence, we might say, that it is for philosophy to produce such techniques that this balancing act of maintaining plurality and singularity together (where everyone agrees on subjugates oneself to what is right, as an universal pure form knowledge) is possible. 
We might look at the theory of control systems in order to create a technical layer to understanding the balancing act, or optimization over this paradox. Theories could be developed in order to correctly interpret phenomena in order to be conscious and rational about understanding the way the paradox is being handled emergently and thus generating instability. The use case of philosophy is hence to produce such theories as to allow a rational understanding of the phenomena to rout out what is simply an emergent and populist stance and thus falsehood. It uses techniques, such as the scientific method (which has its origin in the philosophy of logic) in order to rigourously understand what we attempt to know and thus control.
We approach this problem as a problem of control, which is unique in our philosophical approach, yet convergent in promoting the principle of approaching the pure form of knowledge to such as extent, so that there might be free wheeling attempts in a paralell sense, that could be carried out withough people having to fall in line to heirarchy without their agency, in order to satisfy their biological needs. If they do enroll under emergent paralell structures without coercions, which the singular structure ought to check, then the technique is succesful in promoting the environment of liberty through true knowledge. It could infact be seperated from the material side. If an equilibrium of this nature is attained, a society might be said to have reached a high degree of development (in terms of existence of pure forms â€“ state, institutions and the general public), irrespective of the level of surplus and material dimensions to the development. Hence development might be framed as a purely idealistic process. However, it also happens that liberty has in its fold, the drive to inquire and solve material problems to be best of the ability of people. Hence, the equilibrium ought to empower the people to realize their full potential in seeking the truths and if they are working on it to the best of their percieved ability and the truth is becoming more and more visible, then development is reached.
If we look at this situation in terms of the theory of control, we see that there is a preference for singular and clear control which is the prefernce of the singular source of truth. That is to say a feedforward control mechanism around a central plan becomes possible if the singular knowledge is constructed to such an extent that the presence of paralellised pursuits is able to consensually relate to it. The other side of the central control, is a self stabilizing system, of equals who are entitled to opinions of their owns, adjust their game strategies, form transcient willing contracts in order to genreate a substrate of understanding without any coercive structures. All coercion is monopolized in the obedience of the singular source of truth, which itself is capable of being deliberated by the rational process. The resulting stability presents a situation of feed forward control that has its goals specified to such an extent that it is capable of being supported by an emergent consensus among completely free individuals. That is to say, the emergent nature of central truths, should exactly match the feedforward plan as deduced from the central truth. The meeting of these two origin points provide the dialectic that ensures that the truth is in fact real. In the former case, the belief is that the truth is already present latently in the mind of all organisms, which they â€˜prayathanâ€™ or strive to align their biological need close to (see Kants essay on the helplessness of respect and guilt â€“ The analytic of pure practical reason). In the latter case, there is a situation where the point is that such universal truths (or natural attractors) anyway emerge in a situation irrespective of there being a conscious striving therefor.
In feedforward control, some people subject to rational deliberation are able to idealistically and abstractly generate pure truths or almost pure references thereto and use it as a plan to which the individual in the liberatarian backdrop would essentially comply with, and presents it. The control works on a constant ambivalence over the way the plan is a representation of pure truth and as a result, the plan itself recieves a reflexive feedback by means of critiisms, developments of divergence and ritualism, counterculture in the private, liberal domains all serving as feedback to the rational planner, so that central control could be adjusted. On the other hand feedback driven control, of which the market represents a near perfect case, does not rely on plans as a yoke over the individual will. It is the feedback, anonymous and  incapable of being tracked to elite personalities that provide the control signal. There is no separate channel to produce feedback and reflexively control the market itself, other than to respect its incoming signals and use reserves to attempt to nudge it away from its inertia. Thus, the market has a dynamic equilibrium in the nature of inertia, which sometimes is detrimental to the liberty of the people, say a nostalgic watch repairman, and removes resources from him. His only option is to sell something else (such as the appeal of nostalgia itself) to dynamically adapt the feedback in order that there might be generated synchronous signals from other parts of the whole is generated to let him continue doing what is in his free will to do. If the market does not listen, he does not have a course, whereas if the state precludes him from doing something, he could still debate it deliberatively. But with the market, his options are not to meditate on the goodness and badness of people but to adapt to the feedback.
There does not appear to be an inherent problem to pure feedback driven design, that the world is culturally embracing in the present point in history. One might see this embrace in neoliberalism, a discouraging of planned economies and policies, a general preference for consumerism and packaging and marketing as reward packets, even spiritual experience and so forth. The design of feedforward and deliberative control is argued to be superior to the feedback driven control on one respect. That is to say, the ability to judge what is good and what is bad and the idea of pressing oneself to aim for being good all hinge on seperating value based thinking from free thinking. This attempt to self perfection in aligning to the truth and this attempt to appeal to goodness and even the freedom on be ambivalent and continously exercise in building stories of goodness is something that becomes irrelevant in a completely feedback driven model. If there be a controller and a subject system in a reflexive loop of feedback driven control, there is nothing but coevolution and symbiosis and the whole system monistically converges to evolution and evolution is not idealistic progress but only a material one. Hence, it is a crisis of idealism and the crisis of being conscious that is being meted in the preference for feedback driven control. In the past, grand narratives of truth and authority has had records of instability, of being catastrophically falliable that motivated this preference for feedback driven control. It was argued that the licence raj itself was feedback driven, in that the plans were made not to the satisfaction of the intellect of the bureaucrats but of their pockets. But that is universalization of banality and the mediocrity of human nature. There does remain idealistic people in the bureaucracy who were visionaries. Hence, a categorical stand is not possible, a dualistic one, where feedforward ideation is made and a feedback by cultural rhetoric which might genrate by populist force, a deliberative engagement is what is unbiased. This is the point of philosophy, to avoid the dramatic shift that we are experiencing culturally. It is rarely that contemproary populations are ever aware of that they are witnessing a historical shift, but we might endeavour to it. We in fact go ahead with this inquiry with a premise that computing presents a shift in history, in the same way as enlightenment once did. It we see presents a new way to approach reality. 
Thus, the development of a society might be seen as a way in which it buffers the plural emergent truths and the singular universal truth. Money had been a powerful buffer in its times, liberating people from the constraints of space and to a lesser extent time (they could save and not work for some time). 
Computing as a buffer is able to liberate it from the yoke of time as well. That is to say, let some train driver be driving a metro and let three of them rotate in a day shift. A computational scheme might allow a more fluidic assingment of tasks, which might reduce the bench strength. But the heuristics of each driver is lost in the process. The knowledge would then have to be encoded on to the machine. The computer is a machine distinct from any other. No machine has as a possibility that it might fail quietly. All machines such as complicated gear boxes fail but it is an uncontained failure rising from an individual part. But the computer by deisgn has failure situations hidden within it, as the halting problem. It is basically just a switching system, where the circuit is encoded by the business expert (the coders simply interpret it technically for them).The experts then run the static model by different sequence of events (which are like thowing switches, either by clicking screen links or by sensor inputs) to produce unique end events (like acclerate/brake inputs). Thus, we might say that heuristics had been reduced as a set of reactions to a finite set of events. If all these event pathways could be encoded into a circuit then all possible switches could be handled gracefully by the computer and thus heuristics had been layered over the machine. A machine might as well fail under the right set of events, to which it had not be tested for, but it is proven by mechanical connections. It ought to work under specified conditions which are finite. But the number of parameters as events increases in computing systems, making them practically unprovable. That is why they can emulate heuristics and risking failures and stalls. 
The computer as a heuristic encoding machine, that releases from time and space (in a gig economy say) of people engaged in productive work (obeying singular truths) is what is appaling to the conservative. The conservative involves division of work and free time in a village blacksmith style, sotic and epicurean at the same time. But a seamless merging of work and recreation and of space and time, leads to situation where the computer becomes inevitable as a holder of heuristic information. It is an enabler of the cultural shift. But as we said, heuristics encodes human skills in problem solving referring to some apriori knowledge. The computer has no apriori knowledge but attempts to reach it by emergence. It is hence purely feedback driven (see the recent success of feedback driven control in googleâ€™s search engine), experiential and event driven. 
The ultimate failure of a control system is when it induces a wild oscillation, that makes the system so unstable that it breaks apart. It is like the allied axes divide in the world war. It happened when central narratives of control (of libertarianism on one side, of the essential natural truths on the other) lead to instability.Past wars were fought because of the impossibility to agree common grounds due to the difference in beliefs that were interpreted as reasons for material action, to destroy the falsity. It was not a fight for resources, but against falsehood, which the two sides had different opinions on. There had always been wars on ideas (even of leadership or cults in pre rational times), but they had been well appointed. The development of industry and fragmented modernity, set the idealist machinery unrooted and prone to clashes. In a democracy thus war is less possible. But free markets promote a less dramatic violence in subtle powerlessness which is worse than drafting to forces, in pollution and creep of wasteful cycles. This kind of instability is when the system stagnates and there is no notion of control to make sense of stability itself. Thus, attempting to control renders the system instable, at times, but a clever and dynamic control might help. Not controlling it, would be nirvana, but not everyone is ready for it. One still thinks, techniques can work in better reaching spiritual enlightenment. A local nirvana would only make things worse by conflict, invasion and the like. 
Thus we deal with what is possible, or what can exist as a truth in a given theoretical model. That is to say, if given a,b,c is d exists is the question we propose. Lets say that a control system signals plans and deliberates on feedback from the system and then it stops giving plans and allows S self stabilize by emergent plural authorities, which converge over natural truths. It might then present a greater surplus which is the natural optimum. However, it is possible to produce spikes in performance by singular authority, as did Ghengis Khan and they would raid and thus oscillate the peaceful system. Thus, the natural state itself is a dialectic between firm control and feedback. We then, in rigourous terms study the gradient that emerges as systems in neighborhood promote provisional plans and those that hold short lived provisional plans. If we see this dialectic emerge in simulation, we might suspect that dual mode control is superior.

A Brief Note on the inquiry 
Given that we had said knowledge is synthetic of the apriori truths and the pluralistic versioning, there is a need for reflecting on the current state of history in perfecting this balance. The enlightenment was biased towards the acquisition of knowledge from apriori truths. We interpret computers as biasing the balance towards seeking pluralistic local truths. Computers are switchboards which work closely with real world events, material and not ideal hence. 
On the method, we might interpret computational intelligence as arising from the construction of automata with finite states, which could be switched on or off, depending on the previous state and events occuring. It may have a very large number of behaviour from a finite set of starting points, states and end states and possible transitions. That would get us to inquire into automata theory to be able to construct a formalism of emergent knowledge, as well as game theory, because typical intelligent conclusions arise from paralell mechanisms, such as competing cells voting. Where an intent is not available apriori, one might be constructed from an internal game. If the game needs a payoff which can be stable is a point of inquiry, for which we need foundations in game theory. 
In our case, we see mathematics deals with singular subjects. Everything is an equation, only because it is framed in a line, field or hyperplane. Everything such as y=2x signifies a relationship about a null space, without which such equalities are not possible (Logic deals with inequalities mostly). It can analyze functions, which are continuous relationships between domain spaces and the rate of response of functions by calucusl. Mathematics thus finds itself difficult to accommodate objects which involve paralellism, such as non equilibrium thermodynamics. Systems even if plurally recognized are expected to be in equilibrium with each other, either dynamic oscillatory or static, but not involving random hunts for equilibrium. Cybernetics and Random Walk theory might help frame our notions of having two systems capable of formalized. Wieners Non linear problems in random theory is a good place. 
We need a formalism of paralellism, because we are looking for theorems which can be proven, instead of philosophical speculations alone. Such theorems could be built based on control theory. The dualism that we seek in one system trying to control another, as with Cartesian dualism would need to be contrasted with self stabilizing systems and their mutual efficacy or stability compared. A dualistic stability theory. Our research might look at personhood of artificially intelligent agents, in this work as a particular policy and technical problem. For this, we would hence need to look at proofs that the AI agent would need to be placed under feedforward control, in the same way as humans are, in order to avoid normatively undesirable results in political theory. For this, we would need to look at the rigour of mathematical control theory and the mathematical defintions of free agency. 















Possibilities of SuperIntelligence
======
1. The notion of truth, whether as something that needs to be reflected upon and accessed intuitively, reinforced by empiricism or as being something that would emerge in the natural course of conduct of the normative affairs of the society, postulate truth as being distinct from the phenomenological interaction between people. Phenomenological interactions concern rewards and may cancell out often. They are considered immediate gratification without any inclination towards truths and untruths. The concept of truth involves these extremes, of what is utter truth and what is not and thereby appoints authorities to regulate the dissemination of the truth (a crime is an expression of untruth, rather than a physical wrong that needs to be rectificied). It is the argument of the postmodernist that the truth is a construct of the authoritarian and a clever trick, that has accidental origins in history, which needs hence to be rectified. Authorities and truths in postmodernist philosophy are not natural.
2.Given that authority is inseperable from truths, as being custodians of truth, we might speculate as to whether there existed a golden age of no authority or that authority and its usurpation is a part of the human condition. The human condition itself is strongly sociological. All the philosophy of Kant in the agonizing imperceptability of pure forms and the distortions all belong to the realm of the society. An individual free from social norms is capable of organizing his world around such distortion, by the application of reason and contemplation of a graceful orderliness to things that are to essentially remain mystical. But even without social taboos people are guided by moral sentiments, which arise from the gut and might be termed as inherent to our perception of a long term balance. 
3. There is always needed of an individual of justification if he were to seek short term rewards in favour of stability of the system as a whole. The system itself might be social or even involve animals or inanimate things. Morality is a tendency to fairness and stability and careful regulation of generation of spontaneous events to be in synchrony with natural events. That is to not give into temptation and reap what the woods could sustain. The use of machines by themselves is not immoral if they are not to upset the stability of an underlying system.
4. Morality could be deferred by a narrative of progress, where a definite end might be the settling of things all that had been unsettled in the progressive march. Slavery, Animal mistretment and aforestation had been justified along these lines. Moralists tend to appreciate the need for individual reinstatement of balance at a personal level, rather than embarking on a collective, say national project of progress. But once such projects have come to be in a phase of history, it becomes militaristically dangerous to dissent.
5. Given that moral sentiments exist as being beyond rational comprehension or expression, the individual might seek cooperation of others in establishing stability through civil process. Hence humans cannot help organizing their society in a civil manner. They could not leave a self evolving social organization without reflection, control or plans. But the rudder of social organization is in postmodernist theme seen to be an intersubjective emergent signalling by appreciating truths and condemning untruths in an adhoc and in the stir up in the nature of storms. There seems to be a natural difference and historical circumstance in which some person is able to get a farther view than others in a given context, in which case they might rise to be custodians of truths (even in a business organizational level) and accrue authority. The situation where the truth arises a posteriori, as multiple people anarchistically compete on immediate knowledge, leads people to trust the need for a plan, rather than course correcting the march as we go. Plans could integrate with machines. Plans allow for description of ways in which phenomena could be interpreted and combined to produce surplus. This surplus or value becomes the value proposition of the Plan and hence authority and of objective truths. This might be a materialist interpretation of authority. What started as an urge of moral sentiments, found that the generation of value might be an ultimate moral rightousness and legitimate action (to cite Atlas Shrugged). That is to say Artha becomes the ultimate Dharma.
6.The question is whether the value outputs are merely a side effect of the pursuit of truth, which shall be strictly used for satisfying the biological needs and allow for a culture of seeking truth, ritualistic cohesion of the society and moral contemplation forms the foundation of the moralist debate. The economic enterprise hence is instrumental in allowing individuals to seek their moral truths and hence not tread upon it is a metaphysical proposition.
7. Hence the discussion is about the dilemma of primacy of moral sentiment versus one of utilitarian pursuit of value as being rewarding to the morality as well. We have seen both in practice, as neoliberalism bailed out numerous people from povery for the latter case and the case where slavery and imperialism was abolished in the case of the former. The fundamental problem with the approach of utilitarian pursuit of value is with the emergence of value free truths over time. These might be mythologies, but mythologies bear strong resemblance to universal natural truths. 
8.In the computational society, there is an embrace of value based truths. Information is the material of value, in which sense authority is actively juggled and traded. Decisions are made based on the complex chain of value proposition on the network without regard to rules of authoritarian safeguard of truths or moral sentiments. The use of the computer as a language that is capable of expressing value propositions can frame questions of such high nestedness that it could only be interpreted by automata due to their numeric ability to collate these rules. The decisions becoming increasingly numeric and less qualitative. Numeric decisions made by automata are hence intractable by human agents attuned for qualitative evaluations. 
9. Automata essentially end up making a great deal of decisions and hence wield authority to the distributed, practical truth web. The non verifiability of the automata agents might cause a central regression, where the automata search (there are choices involved in searching for open ended expansion of value propositions) for value closer to the median. There is no way of knowing of such central regression in phenomenological terms. One might witness a narrowing down of choices pervasively and marginal value outcomes stagnating. But there might not be ways to control the system. 
10.It might however not react jealously to abductive actions. An agent defecting the system if able to strike a value stream that is outside of the purview of the system and hence start starving the system, he becomes capable of controlling the system by committing it to sepcific narrow ends. There might be a specialization of intelligence that is controlled by an external system. But the external system must not only possess the qualitative context but also the numeric ability to evaluate, tame and control the system. Hence, a computational consciousness relying heavily on the internal compliance of biological systems might be a transhumanist future. It might materially alter our physical constitution, but that might not be material, if a perfect simulation is possible. What is material is the divestment of consciousness, where the ultimate parochial needs are projected onto newer domains involving high frequency computing (just like the existential dilemma of individual cells are handled as political domain problems by humans). 
11.Open ended domian extension of knowledge, building upon basic facts in a self referential manner is a possibility. But knowledge might also be viewed as an approach to the realist notions of truth, of an essential order to the world. The approach would mean a reduction of noise, rather than an absolute discovery of elements of truth. The simplest creatures are aware of the workings of the world, of universal order, but with a lot of noise and imprecision. There is a possibility to obtain refined models of reality, or superintelligence, where the sparseness of reasoning and the numeric abilities are enhanced enough to reason much deeper and reflect much deeper. If this is fundamentally different from parochial intelligence, is hard to say. The realist argument might be that it is an emergent consciousness that continuous progression, for progression is singular and perhaps anthropic. An altnernate narrative is not epistemologically approachable. If there be other ontologies, or paralell universes, then the transformation is uncertain of commentary.


Note: All that is defined as surplus and as the creation of infrastructure might be interpreted as the generation and the employment of capital. Here we had indicated that knowledge is something the shape of which is perceptible by a mix of epistemology and poetic awareness. It is the increase in the density of the knowledge that is accomplished by education, reflection and increased intelligence. The increased density allows one to comment upon the actions of those entities holding sparse knowledge models as phenomenological, without being directed to some universal overarching goal and hence capable of being utilized towards a larger project. This might be seen in slavery and in domestication. The incremental density is essentially a zero sum process, pouplating about the central nullity, evenly on all directions.
What we had held is only a dystopian view of the network society
--

Recalling the social contract
To continue in the tenor of the argument of parochiality of intelligence, we see are eventually lead to the point that there does exist dilemmas of resolution between higher truths such as dedication to art, of honor, of love and those of immediate needs of biological and ambition such as seizing control of a certain resource. These dilemmas are meant to be a part of human existence and are to be solved by the individual concerned, as a step of redeeming himself. In other words this cannot be institutionalized. 
But this point of dilemma as being perenniel is uncomfortable for many. In the past, there had been movements to reclaim the dilemma from being institutionalized but they had not been successful. Say the Bhakti movement in south India a thousand years ago or the recent counterculturist hippie experiments. Both these were bold moves to commit to basic, intuitive higher truths without the guidance of authoritarian agencies like those of vedic scholars or in modern times the authority of religious and business leaders. But in either case, it lead to collission with the world in general. The world with its challenges would not let a person pursue his hearts dream contentedly. Invasions shook up the bhakti soaked medieval south India. Likewise the hippie movement would have to be constituted by bums (the dharma bums - kerouc) looking up to the general society for being fed and clothed. An attempt to assert an independent existence, lead to more mortal problems, leading to the need of authority and thereby an implosion in many cases or a dispersion by external forces. 
Thus we might provisionally say that there exists a need for a man to work (like Candide or the village blacksmith) to earn the luxury (or right) to romanticize, reflect and redeem himself. The postmodernist attempted to merge these two and deny the dichotomy. Hence, it let go of the metaphysics of the need for redemption itself. It attempted to demonstrate that the dilemma itself is constructed by instruction and conditioning in a society and not natural. It however, met with various degrees of success. It mostly took off not in its ability to retire the dilemmas but with the catalytic uprising being the critical mass required to transition into a network society (Castell). The network society as we might characterize the information society with knowledge workers, involved the constitution of transcient fleeting authority and in effect merged morality and selfishness, without the concerns of authority. In the absence of authoritarian burden to rational solutions to mundane problems, there was hardly any concern to keep the dichotomy intact. Everything flowed as one monistic, immanent (rather than speculative) metaphysics. The result, we might say that natural problems do present themselves at various points of time, leading to the emergence of authority, even if of superintelligent agents, as we had seen earlier. Besides the divestment of the dilemma to institutions rips humans of their basic right to yearn and hope.
We might now recollect the idea of instrumentality of the society as regarded in the Social Contract of Rousseau. He is considered the father of the romantic movement. Just as we had been reflecting, he considers the duality of the right to romantic day dreaming and the necessity of toil. The former he calls libertarianism and the latter of the necessity for social cooperation. If a man could handle the duality either in person or as a small family or clan, it might not merit a metaphysical reflection. Only when there arises authorities to restrain a mans liberty for solving problems in the state of nature that the need for a public sphere discourse and a holistic reflection of the metaphysics thereof arises. Hence, Rousseau postulated that the extension into the civil society was a development out of an explicit contract dedicated for a set of specific purpose, rather than the more recent interpretations of society as a spontaneous organism (Durkheim). The hypothetical contract had stirred many a revolutionary rise of great deal of violence, but it was the product of the tension of the times, rather than the philosophy itself. 
The social contract is also a legitimate extension of the Cartesian duality, in the seperation of the observer as being indepedent and endowed with a soul. He rejected the organic nature of the society and considers individual observation and agency as material, reflected also in Rousseau's philosophy. In fact we might trace this monotonic rise of an awareness of a need to reflect on the nature of civil engagement from early modernity till the early pragmatism (in fact idealism and pragmatism was placed in a continuum in the critique by Giladi). Paradoxically this was accompanied by an ever increasing integration of world order over norms and in the decreasing value of qualitative truths powered by technology. There is expected to be a strict seperation of technology being applied to natural problems and the use of technology to buy the balm for the dilemma of mankind over temptation and virtue. Whereas over time, one sees the value proposition being the universal quantifier of academic direction (as lamented by Weber) and of artists and spiritual leaders. 
Hence, we might frame this as a crisis. It could only be reinstated by dispelling the ostensible contradictions in the modernist project. It ought to be seen in the light of a strict formalism to value based activities which is loosely coupled to the indvidual drive to address the dilemma of morality. It might have been the theme in Ashokas philosophy two millenia ago. Ashoka enacted edicts with scarcely any mention of spiritual authority, not even Buddha was mentioned in his edicts. Hence, the concept of dharma might be narrowed down the reinstatement of individuals to pursue the resolution of their dilemma and the strict seperation of artha in the pursuit of duties of solving natural problems and generating surplus, limiting it to stricly logical and verifiable process, governed in the spirit of the social contract.

Possible works: 
Evolution of infinite loops in large networks. Cyclical redundancies as an instrument of control.
============
Defection is minimized by inventing a game of crossfeeding as concieved by Mol. BioSyst., 2014, 10, 3044--3065.


Theories define 'necessary conditions' for materialization of phenomena, but not the sufficient conditions, for which causal interconnection between phenomena is required by empiricism. UV and oxygen might be necessary conditions for the emergence of ozone but the arising of ozone is not explained sufficiently by the combination of the causative agents. There is an inherent stochasticity involved due to which the set of sufficient conditions (to reproduce the event) might carry a stochastic noise. In such cases where the stochasticity is a cover for latent factors awaiting a complete enumeration of the elements of the set of sufficient causes. Empiricism helps in establishing temporal sequence after excluding other factors (in controlled conditions) which might have latent effects. Empiricism is hence a connection of phenomena, finding its root at the experimentor out of his free will performing certain actions. The intentions and biases of the experimentor, as well as the feedback loop flowing from outcomes is not generally considered material.
Connecting phenomena by means of a stochastic belief network or empirical sequence of phenomena is a bayesian network. If causality or theory could be inferred from was discussed in Pearl and Verma.


Decisions one would regret - Regressive tendencies in network based intelligence
======
It might however confer dynamic advantage which might be balanced at crtical points of regression.


3. Consolation by reflection as being an essential part of mental health. 
======
The cognitive roots of mental distress, where the perception of a person is unharmonized with the general population arises due to the difference in the cognitive objectivism over the degree and place of randomness in the world. Thus, mental health is not only a construct of norms, but of the ability to percieve the objective common reality, that 'is'. 


NOTES2
An Instrumentation perspective of Computer
===
A theory of computing as a cyclic truth deducer (and not a machine)
==
1. Science is a way to establish necessary and sufficient conditions for phenomena. Phenomena are crude observations, which are deemed to exist by virtue of their observability. The necessary conditions for the phenomena is established by a logical necessity following a certain principle. A good example might be that there existing a postulation of positivity and negativity and an empirical confirmaiton of electron's existence, would naturally cause the scientist to predict the existence of positively charged particles in the atom. The presence of the electron constitutes the necessary condition for the existence of the positively charged particle. But it might not be sufficient, say what if all the atoms are negatively charged and the balancing charge exists somewhere centrally at a distance. Sufficient conditions are established by empirical verification. 
2. Empirical verification is essentially statistical. Crude observation of phenomena which triggered the inquiry might consist of confounding elements, due to the disturbances in the environment and the intents of the observer. In an experimental setting these are filtered out, so that verification could happen. But absolute noise free environments do not exist and hence all observations are statistical to a good degree. In medicine say a 95pc confidence interval is sufficient to establish the scientific explanation of the phenomenon (a hypothesis seen as H0).
3. Science relies on its six conservation laws and the empirical law of non spontaneity of order creation (law of entropy) to establish necessary conditions, which might itself be expressed mathematically or geometrically. Thus, the discovery of electrons do not supposse that orderliness is established spontaneously by the atom seeking some other positively charged particle in an intentional manner.
4. Accordingly order creation ought to arise from machines. A machine is essentially a heat pump if it creates order. It simply removes the heat (or homogeneous randomness) from an enclosed space onto a heat sink (say the outer space). The act of creation of order by observing the heat (say trapped in the atmosphere) is designed in the machine by a process of mapping discrete events (say the photelectric threshold) to other discrete events (say the critical energy for ATP bond creation). The machine consist of switches and gears which moves and meshes with other gears to create this effect, in a simplistic mechanistic analogy.
5. Complex machines involve paralellism, in that there are multiple components which do not strictly collaborate, but compete or rather orthogonal and coexist on such pardigm. Paralellism would hence involve a multiplicity of systems which optimize locally. The second aspect of complexity is latency, where actions get absorbed into some local sink, which on reaching critical threshold might explosively propogate these actions. From a view point, this latency is akin to stochasticity, in the sense that certain actors (rather than components, since now there is local optimization due to paralellism) hold their appropriate actions till opportune (subjectively) moments. Hence, we might say stochasticity and paralellism designed into machines make them complex heat pumps.
6. Complex machines generate phenomena such as biological systems, which could not be traced to necessary conditions, but only statistically mapped to certain activity in different parts of the system. That is to say they might be modelled as event driven systems where critical events might set the course for certain flows where certain phenomena are anticipated (and not others, we had earlier postulated that phenomena are deemed to exist if we observe them). Hence, divergent outcomes arise in a given model of a system when mapped to the 'real' state of the system (which is unobservable, since the system does not have a central state). We only get to read portions or angles of the system from our model. 
7. As against classical mechanical modelling, these models are state machine based, where for a given cycle certain pathways are traced. The machine or model are hence cyclically convergent on essential truths about the complex system. We presume the complex system is in fact centrally orchestrated and has stochsaticity and paralellism as instrumental features only. Thus, computers look for emergent order in a system, by allowing different sampling from the system by different users over cycles. Likewise the FSM architecture also involves the temporal extension of decisions, in its 'while' loop. It also allows collaboration not only of human agents, but over a network of systems. Hence whether it involves multiple humans or other automata systems, it might be termed as a network.
9. The objective of science is to discover the truth with respect to the originality of a phenomena (rather than it being an effect). The objective of computational networks is to be able to approximate the central intelligence of a system. The utilitarian argument of classical mechanics is to be able to model natural machines, so that these can be emulated in more perfect engineering endeavours (say emulating plant hormones with nitrogenous fertilizers). 
10. The question as to whether utiity is the end of science, becomes a philosophical quesiton, since we are examining the phenomenon of doing science itself. Any phenomenon to which we are party to, triggered by us (humans) and appreciated by us, arise from motivations that transcend immediate concerns of survival and being 'driven' by evolutionary forces. This transcedental drivers of actions are the subject of philosophy. Philosophy is hence beyond empirical observation, becomes it does not seperate the observer from the object, but it is rather reflective. It does not presuppose absolute soverignity of the self, but rather predicts that some essential objective drivers ought to exist that could explain our actions. Such rules might be beyond rational deduction from higher truths as well. All that might be done is to speculate.
11. Speculation might be legitimate, since it might uncover the objective core of a seemingly random crude observation. A speculation is not falehood, only an unbased truth (for which no science - or necessary and sufficient conditions exist). It had been discussed in Bertrand Russels Kettle paradox. Say the various explanations of the Fermi paradox is highly speculative, but given the complex and historical nature of interactions between life and the environment, one might find a great variety of possible speculative pathways. Some of these speculations might be hypothesized and worked upon scientifically.
12. Speculative truths which are not submitted for scientific queries are generally attributed with untruthful motives. But science is not an absolute truth as it might be speculated philosophically. Thus philosophy is disruptive of the normal conduct of affairs of humanity. But modern science originated from philosphical speculation of Galileo and Descartes. The only way speculation can be avoided to undo and backtrack is by the narrative of progress. 
13. There does exist systems which are highly intractable to science and thus making the profitability of engineering innovations in those fields low. These might be economic, biological or political. The central tendencies (the core of central intelligence we talked about) for specific problems in these domains might reveal a mechanism where certain events are mapped to certain other events by design and these designs might be refined and emulated in engineering. This is the grand project of computer networks, where events are mapped by local optimizers (by encoding to partial recursive aximoatic systems) and let to interact with stochasticity and paralellism. The emergent central tendencies are picked up evolutionarily where certain pathways are strengthened by feedback. This leads to an emergent direction of progression of orderliness.
14. The fundamental program of civilization (seen as a network) is however a more plausible query of philosophy. If it had been encoded statically signifies the presence of an apriori truth system. An imperative to embrace the steady gathering of central definitive rules (with additive noise) signifies a definite preference or program. The search of philosophy is to see if there is a contradiction built into this program, as when the program becomes self limiting, such that the drive could be sabotaged by the driven and perhaps reversed. This speculates the critical point where the universe does not allow further progression of order gathering and pushes it back by philosophical questions over the model. 
15. Computer networks allow for additive truths, which would hence act as filter for noisiness and reveal central logic to a system, particularly systems involving humans (as a complex system). This is intended to allow for its refinement and hence progression of orderliness, such as terraforming other planets. Philosophy questions this apriori truth of progress. If there be a materialist program to seek out progress, then is there a critical point when the frustation with meaning (on the ultimate falliability of the material progress given infinity exists) might lead to stopping vain attempts. A stage of philosophical development might hence consign human progress as instrumental to sustaining (given intrustions exist from less intelligent creatures and the universe itself) a campaign of meditation and doing zero sum things or more meaningfully to reflect and contemplate glory.
16. Material progression is still a pertinent and immediate concern of civilization and hence philosophical ruminations of its demotion to an instrumental status might not sell in many parts of the world. Thus philosophy is an absolutist or idealist view of the world, as oppossed to the normative view.
17. In this specific idealist view of computer networks, we might see that computers attempt to involve the observer with the observed in producing a world view, normally a subject of philosophy. This worldview emerges as specific behaviour rather than text encoded truth statements. This view has been discussed in Cybernetics. In evolutionary progression, the subject and object interact to produce persistent behaviour that is no longer dynamically explained by the actors, but are fixed onto to the interaction context.
18. This persistence of behaviour is what defines the uinversal truth to a system, as rules to a game. We say that emergent behaviour beyond critical thresholds might encode itself as definite rules (such as eusocial heirarchies). These rules are intractable to the subjects due to their being encoded with greater degree of noise. Noise might itself be subjective to the observer, a lot more randomness exists to an unintelligent observer such as a dog, than a human. Both possess fundamentally similar contours of reasoning and derivation of meaning, but humans differ in the ability to reason more densely, such as to be able to hold a large memory.
19.Lets take the example of sun rise and sun set being a phenomenon of interest being observed by a human and a dog. The dog sees it as a complex system due to the clouds dimming the sunshine. A long memory might help smoothing the noise to the signal. Paralellism arises from not being able to serialize a situation in high frequency. This in the case of cloud movement would involve serailizing everything even a butterfly's flutter (and farther on, since earth is an open system). If paralellism is essential, then analyzing a complex system is independent of processing power, NP Hard would be NP Hard irrespective of the observer. Minor improvements might be done, but statistical noise would be absolute. However, if one is able to observe a system in isolation, by filtering noise using adequate memory of stable states and oscillatory noise, or one is able to observe a system by physically isolating it by adequate force one might be able isolate components, but the components themselves produce interesting phenomena only holistically. Thus we might say complexity is real, arising from systems interacting with one another, isolating and studying them would not help. Likewise the exponential wall of computation, forbids any increase in intelligence to only make a nominal dent in the construction of meaning of complex phenomena. That would question superintelligence possibility as well (holding on to the serialized definiton of intelligence).
20. That is probably the reason why a lot of knowledge we have is self referential. It simply simulates the universe in terms of known entities and the emergent entities would make better sense in the simulation than in the real world. Hence monsoons are simulated as market prices, compensated by finance hedging and so order is produced from a complex interaction, by additively cancelling out linear actions. The net meaning of the system is attempted to be read. The meaning of monsoon is a certain confidence level of rain outcome, which could be buffered from a reserve to produce a well behaved simulation. Hence, buffered adaptation reduces complexity from a system. It is in effect enclosing the system in a container which is controllable.
21. Buffers are automatic noise observers (even mechanical springs are such). Question is whether a system could be stabilized from within rather than being enclosed. If a buffer is placed within the system as an observer of signals and operating on cancelling them out, such as a good distribution mapping intelligent system ( an exchange say), we might be able to stabilize from within. Computing solves primarily distribution problems by allowing communication while at the same time being only locally optimal. These signals are responded or mapped by the network to stabilize the system. Likewise computing also allows via sensors to collate signals say from buoys of multitude of data, which could be sifted to seperate noise (which could be cancelled out as local disturbances) to long term trends, thereby stabilizing or buffering the system from within. There is no difference between whether the system is placed in a human participating system or a biological or weather system of complex nature. Stability conditions help interpret complex systems.
22. The question is drift. That is to say, once stable conditions are discovered and a system stands effectively buffered from within, an observer at the boundary of the system would be able to observe macroscopic noise filtered changes, which he starts emulating. A spring might resonate micro events. We might observe that noise cancelling by additivity of computing produces a smoothening effect. As long as the whitenoise is seperated, the fundamental signals makes itself apparant to the external observer yielding to control. However, say a certain human population is being observed, if movements of average urbanite is discounted as white noise and the presidential address allows for predicting certain pathway of events, thereby allowing preparing a control strategy, a small statistical rounding off can produce dramatic result. Say an average man pulls the gun on the president, perhaps by impulse. The whole system of control collapses and the system is perturbed dramatically from its stable course. The false sense of stability is only a deferred collapse.
23. The stability obtained by filtering white noise is hence falliable in systems with high heterogenous structure (such as critical points of collapse internally such as the president). The stabilizer hence could not afford simply seperating white noise, but also to internally buffer specific vulnerable points. This would need an understanding of the command structure of the society, where one mans will might by virtue of a heirarchial arrangement be obeyed by others. A more generic analogy of this is the presence of causal relationships, where on one end is a cause and the effect might have no discretion but to necessarily follow the cause, as with the presidential decree. A knowledge of causal relationhips allows understanding heterogenity and all heterogenity is heirarchies of designatable boundaries beyond which causal events dont propogate that well as within the denominated heiarchy. Thus, the presence of causal relationship between raindrop condensation and mountains in the course allows to make a stabilizer or predictor which could incorporate these strong causal patterns to the action. This involves knowledge of heiarchial arrangement of a complex domain, the self contained ordered elements which participate in producing complexity and heterogenous relationships among them.
24. Computing emphasizes seperation of noise to the detriment of derivation of causal relationships. Traditionally knowledge involved qualitative and seperated inquiries into specific associations, involving heirarchies and rules. Such structures permit understanding well established parts of the system but not the whole. In physiology some parts might better understood than some other. To understand a phenomenon, one needs to build a noise filter to buffer and present clear trajectories of health as well as to find specific command relationships so as to be able to approximate them in classical mechanics so that intervention might be provided. The whole however escapes the formalism of mechanism, due to the emergence. Its irreductability hence causes holistic problems be solved inductively from general observations by heuristics. Heuristics is essentially noise filteration, paralell reasoning along possible paths in a Bayesian sense. Humans are good at this apart from causal inference which could be delegated to reductive models.
25. Supplementation by holistic modelling might thus take the burden off heuristics of specialists. Expert systems might hence stand to be supplemented by holistic models. The question is whether definite computational advantages could be wrought by using machines instead of people. We had seen that intelligence hits a wall when dealing with paralellism and a high frequency system is only marginally better than a low frequency system. A low frequency system might be able to passively isolate noise and apply it. 
26. In that high frequency noise filteration is only marginally advantageous it only fuels humans from abstaining from work that they could very well do equally efficiently and substituting by machines. This has economical reasons for substitutions, since long term risks of collapse are hard to come by.
27. It is critical how falliabilities of expert judgements are normatively handled. A falliable decision flows in as frustration, but the good faith with regard to the application of skill by the professional, allows pursuit of consolation either by social accomodation or by spiritual reflection. On the other hand, a computational failure might be interpreted as being biased or having been conjectured not on a professional with reference to an objective framework of ethics (arising from shared heritage as humans etc). Computational holistic models involve work in their construction and hence have value propositions. These value propositions involve preferring work that could be solved quicker rather than slower and the outcome is quantitative rather than qualitative (as with human agents). This had been the trend of additive entities like corporations where a certain degree of quantitative integration had been achieved. Qualitative judgements are generally ignored, requiring people to control them on ethical basis.
28. Hence we present the twin points of any further intelligence in resolving complexity as essentially unattainable and secondly, an algorithm that somehow produces advantages actually defers risks of catastrophes or it actually buffers risks by selecting quantitatively advantageous decisions. Qualitative decisions which could be examined in the system of ethics could not be produced by such machines. Higher frequencies provide short term advantages to seperating white noise, cyclically determining the stable course of the dynamic system, but an ignorance of inherent relationships which have high potential and low inductive representation, requires an understanding of structural principles and initial conditions.
29. Structural principles are discovered not only by an investigation of sufficient conditions, but necessary constraints to the system. This involves deduction from known concepts of generality and specialization, which supposses a priori knowledge, of discreteness of numbers and such principles. A repeated recursive application of these principles to observations produces the effect of seeing them as constituted from parts connected heirarchially and associatively and as enumerations of contained types. On this plane of abstract reasoning a human is able to make sense of the world.
30. The construction of a system of abstract knowledge, could be inductively enriched, but it would need a framework of logical reasoning and mathematical continuity. The objectivism and realism of these entities, might suggest their declaration and capacity of computation on a machine, which we very well do, embodying them in logical structures such as physical machines and logical system of rules. But the computer is different in that it does not represent causal relationhips between entities such as a system of rules, but rather a system that responds to events. A computers course involves a parameter of time to it, making it an automaton but not a classical machine.
31. A computer responds to events, it foresees and alters its internal state till such a state actually involves emitting events. Thus, it maps events to events as an exchange would. It is not a heat pump, it dynamically buffers events to produce stability. It has no notion of heiarchiality or boundaries. It evaluates events (waits for them) and determines if events are there in certain categorical lists and emits events regarding their subjective truth. There is no master plan to their actions (or outgoing signals to actuators). Hence, from the initial state the computer buffers noise but has no means of knowing specific pathways of 'potential' as with causal reasoning. 
32. It might allow development of simulations of human heuristics and adversarial search and thereby some heirarchies might emerge and this preference between good and evil as the dualistic tendencies play in its core reasoning (reward and penalty or truth and untruth either of these dialectic is tenable). Thus, the establishment of some dialectic is required for the development of the computer to reason as humans do. Coupled with inductive noise filtering, they might become crude reasoning machines, but their intelligence might not exceed human ones nor they be highly helpful to human endeavours. Even for this we presume that once a dialectic is present, an universal sense of goodness is available to any open system and could be projected on to the understanding of the world around.
33.From an instrumentation point of view, the computer relies on additivity. There is hence a  deep reliance on the signs of the numbers, without which noise filteration by additivity is impossible as well as the angular directedness which helps vector operations in a plane. Thus, we might say that from an instrumentation point of view the computer is a deresolver. Just as there had been instruments like telescope and the microscope that enhance empirical observation to the benefit of science, the computer does the inverse. It removes distraction from the observation and thus helps scientific approach to truth. A good use case, might be that a computer can trivially produce the average day time temperature for a city over ten years. This helps the scientist avoid any short term bias and actually interpret short term bias.
34. Apart from the instrumentation perspective, where we actually attempt to state that science is the path to truth, there is also the commercial perspective, where economic activity is also seen as a path to truth, one might imagine, people applying deresolvers to everyday use cases and acting based on it. In science experiments should result in theory which would then be perfected for engineering adaptations. There is an attempt to logically reconcile before applied science. On the other hand, a shortcircuited direct heuristic adaptation of deresolved truths could be automated to a good degree and even if made by humans are repetitive and trivial. The result of application of deresolvers is an ignorance of causal relationships and stability attained by a need for ever increasing growth.
35. That is to say, a stabilization requirement of a society that works with buffers to sustain itself while ignoring 'special' relationships of high potential might in practice be attained by expanding the boundaries of the buffered system, so that while there is no central surveillance to logical constructs of control, there is an elaborate process of peer balancing, which discourages development of potentials and schemes. This homogenity might be speculated however to give rise to emergent dialectic heterogenity.

==
Regarding the nature of intelligence

1.A lot of allusions had been made in literature both formal and informal about the case of human intelligence being dramatically superior to animal intelligence, with such examples as lizards not being able to make sense of tubelights and anthills not knowing of highways that pass beside them. For the lizard, a tubelight is only suggestive of a situation that flies would emerge to circle it and makes a good meal. But here there is something missing in this analogy. Even a human is not able to reason much about the tubelight. Of course he can answer questions about it, posed to him in a grammatically organized language. He might answer such questions of causality such as the switch being thrown causing the light to come up, he might even answer questions such as to that a certain energy called electricity flows on when switched to create the phenomenon. But beyond that his cognizance dims, for a lay person has no further ways to explain what electricity is. He might draw weak analogies in the flow of streams and rivers, but definitely his knowldge becomes hazy just two level of questioning away. Thus, we might say, human capacity for epistemological intelligence is seperate and distinct from the common way of percieveing things that has deeper roots and more commonly distributed.
2.Human intelligence hence might be said to enjoy only a minor advantage in being able to use the epistemological way of knowing and percieving things. Even genetic wise, their makeup is not more than two percent different from the nearest primate relative. However, empirically we see that the epistemoloigcal capacity housed in the prefrontal cortex had dramatically altered the world. We have to realize that such an alteration is caused by a strong positive feedback loop, which might be called as 'Capital'. This capital allows humans to construct civilizational tools of such great mass, that it is able to very much reduce the complexity in nature, by building dams and irrigating fields (as compared to the complexity of water rushing through a forest stream).
3. As we had seen all complexity arises from paralellism. For instance if a human is suppossed to cross a ten lane highway, at a given instance, he has to analyze the paralellism of different entities at play making it riskier or easier for him to get across. There might be drivers rushing to get to work at particular times of the day. The road might be slippery after the rain, the human agent's constitution at particular times might be more vulnerable, such as early in the day, a fog might be descending and so on. In  fact it is combination of these paralell developments that give rise to intractable phenomena such as near misses. Therefore, the person crossing the road, even though equipped with epistemological intelligence, would have to serialize everything in order to present to his rational faculty.
4. If we look at paralellism per se, we would need to recognize the presence of an very obvious and often overlooked massive computer, the universe itself. All the paralell events make sense only because they play out on a physical world, with almost infinite density of time (perhaps a clock speed of plankhs constant intervals, we are not sure). Given the massive potential for physical events to happen, numerous subjective aspirations (even free willed subjectivity such as drivers rushing to work) come together in a dramatic fashion of execution in the physical reality or the physical world. Thus we might as well say instead of A and B collided, strictly speaking it is either A collided with B or vice versa. 
5. It is this paraellism of the real world that the capital gained by the intelligent actions over millenia endows humans with. They might put up rules, controls all by brute force and contain the paralellism to managable level and live with buffers that could absorb any estimation errors. 
6. In fact even while reasoning on a serialized basis, human intelligence does work with probabilities (such as the speed of the average driver, while crossing the road), where every event sensorially observed is coupled with a permissible white noise, while it is being slotted to specific, exclusive types. Humans are able to construct abstract serialized models based on such exclusive membership systems in their rational process. This as we might see is quite limited in scope and hence many times, human reasoning relies on heuristics, which do not have much of formal denotation.
7. For instance if a human is given a task of arranging several geometrically shaped objects (solid and hollow), he does not go about in a serialized fashion on a combinatorial basis as a computer might, but works with certain predefined rules, such as the heaviest object being at the center to produce more stability and so on. These innate constructs arise not from the reasoning faculty, but from the innate sense of balance and symmetry that is hardcoded in the non epistemological intelligence, inherited over billions of years of evolution. Hence in the lines of Chomskyian tableau rasa argument, we might say there exists a deep subjectivity that operates behind the epistemological faculty to be able to overcome the combinatorial burden by picking up what is 'good' and what is not. The dialectic probably arose deep in the experience to keep alive and away from pain.
8. The nature of epistemological reasoning might as well involve simulating the physical world, where the degrees of freedom are limited when dealing with objects and without running into a combinatorial explosion, the human mind might be able to backtrack and playout different possibilities to decide on the proper course. Many people like Ferrier had argued of the necessary reference to the deep subjective goal of development in order for humans to resolve problems epistomologically as being the frequently used shortcut(they consider objectivity as extended subjectivity with deferred rewards). They infact suggest there being an incapacity to seperate the subjective from the objective. But for practical purposes, it appears that human epistemological reasoning could in fact hold objective grounds away from subjective influence, as proven by their ability to percieve justice and to practice science.  
9. The non epistemological dimension to intelligence is not appreciated as often, but they form the foundation for the argument of parochiality of human intelligence.
--
1. We had argued against super intelligence elsewhere because of the limited computational resources of human mind that it quickly runs while applying its epistemological method into a combinatorial wall. A system of higher memory might as well reach the wall, albeit a few levels later. Therefore it might be said of intelligence that scalability by adding memory is not that fool proof. We had also seen that intelligence might be elevated, such as the degree that prevails between isolated tribes and urbane humans. There are more concepts to the expressions of the urbane person, distributed logically about a mean core of knowledge (or perhaps a null center). Introduction of more concepts allows modern humans to reason over many more things, but a lot of the difference is not due to this ability, but by the generation and application of capital from the surplus.
2. A more resilient way to intelligence is with simulation. Humans might simulate the physical world in a system of high clock speed and thus be able to emulate the nature of the outcome from a sequence of paralell events. The computer apart from being a pure deresolve (or statistical smoother) also simulates reality (Langton). This simulation allows to serialize in memory to produce interesting estimates of outcomes, in an abstract way and wihtout relying on material reality. 
3. Optimizations work this way where the physical reality is impleaded as constraints and subjective preferences are let to play out in order to take proper positions that would hold well in real world. In fact we might see that humans are already reasoning at the end of their memories, stretching themselves by training and education. Over time, they seem to be distributing their epistemological capacities, by the use of capital and markets and thus creating distributed paralellism from what is essentially a faculty for serialized reasoning. Hence, they seem to be constructing an augmentation of the core intelligence.
4. This distribution of intelligence, would need some kind of physical interation or a sufficiently powerful computer to simulate the new reality. The system might itself be subject to some searches for compression, which might lead to an emergence of objectivity (or a reemergence thereof). It might go forth to a distinction in a dialectic sense in order to gather the reasoning based on pleasure and pain and hence allow a subjectivity to work with the simulation to produce a new level of civilization. This emergent intelligence from distributed epistemological processing, serving a core intelligence which might bifurcate by dialectic over what is good and what is to be avoided might produce a true artificial intelligence of a global scale.
--
1. If we digress a little bit with respect to the ability of humans to work rationally, it might help. This is wiht respect to the nature of tribes and thus an inquiry into rationalism. IF we look at the nature of modernity, we see that there had been an insistence of consistency being the measure of goodness. If we see abolitionism rose in the eighteenth centure in Europe and in fact caused a civil war in the nineteenth century US. The reason was that the concepts of libertarianism suffered from a crisis of consistency. The same inability to live with rational inconsistencies pushed the west continuing its liberatarian rationalist experiment in the past three hundred years to permit assertion of civil rights in the 1960s US and permitting non white immigration the next decade.
2. But if rationalism and education is capable of creating consistency in the real world among real people is quesitonable. People anecdotally, drawing from their long tribal past are more comfortable with stories than rational constructs. Hence, people drawn by stories and by complex ways in which music and culture string to their core intelligence, are able to cope better. In fact despite their rational convictions, people had fallen off the road, inadvertantly to create world wars, holocaust and imperialism, which they later came to regret. Thus rationalism and modernity creates such puritanical assertions that are impractiacal and often lead to collapses and tragedies.
3.Thus much of the yearnings of the present age might be a rooting to the tribal past. The broad strokes of rejection of modernity in the past century center upon this insistence of puritanical rationality or single source of truth, that becomes vested upon an authority and its inherent violence. Thus, a better policy framework would be to address the human units as story abiding tribes, rather than rational units. Libertarian assertions might be made with respect to freedom from coercion and a basic social security net, as with Nordic communities. At present, the irrationality of humans are handled by the mechanism of the market, while the rational faculties by the construct of law. Hence, we look at ways in which the appeal to fundamental or core intelligence could be sustained in a modernistic setup. But what is important from an inquiry into intelligence, is the concept of the essential irrational part to the intelligence, which would require to be balanced by proper political policy , as much as possible. It could more deeply be attempted by reflection on the tribal past and contemplating nature which could somehow flow up as consensual policy dynamics, rather than a visceral hate of rationality.
4. It should also be noted that the objectives sought in terms of stability and peace as well as engaging the individual free will, is difficult of sustenance. Every system is subject to entropy. Even if the best mechanisms are incorporated, they would fall apart and would need other mechanisms (as human rational understanding of roles) to be upkept and so on recursively. Hence, there is no escape from recursion than by growth. But the question is whether growth could be attempted without recoure to evolution, but by deploying capital and coupling it with epistemological intelligence in order to protect the human status quo of tribal living.
5. Actually rationalism might be expecting too much out of people. People are much more comfortable with mythologies. Eventhough some arrangements might look to be oppressive from a rationalist view point, traditional societies have their way of balancing things out, without explicit intervention on rational basis. But rationalism might itself not be rejected for the reason of being able to model complex real world systems linearly and be capable of being worked upon in a manner which could be linearly allocated and scaled. But the limitations of rationalism in predicting non linear systems is to be recognized. Many modern states as we have mentioned shy away from constructive legislation and instead stick to defining the limits. To complement this public policy move, markets ought to be blunted of their omnipotence in representing tribe irrationality, in order to prescribe a more stable and contended society.
--
1.Now that we have painted a picture, where we see that rationality as being reductive and the extension of intelligence into areas which are non linear as being important in obtaining an explanation of the state of affairs. We might as well go towards saying that some kind of our own interpretation of the master slave dialectic to be existant between the core intelligence and the rational intelligence, the latter being the slave. We might see in this ontology, that the slave is in the command of the master, evaluating and computing solutions and proposing them back to the master, whose normative and aesthetic point of view is intractable to the slave. 
2. The question is how often the slave refers back to the master. We had seen that in problems typically involving combinatorial methods, the human mind is able to build algorithms, perhaps by a combination of the principal methods of problem solving that of reduction, summarization (statistical) and simulation (stochastic). But it might as well involve a frequent referral to what is good or aesthetic in between decisions, to lend the substance to heuristics. 
3. Again it is ambivalent of the nature of the master. The master might be the ultimate historical product of four billions of years of evolution with no reductive undercurrent running. It might be that the slave is perplexed because it is presuming a finitude to its methods. It could not appreciate the infinitude of the evolution of the master, being the complete product of a fabric that is utterly random and historic. It always tends towards theorizing and looking for some cyclical drum that spins out the patterns on the brocade, perhaps this is the pitfall of the rational faculty. But empirically, the claim of rationality is not entirely misplaced, we see the universe is either of finite size or of finite age, we see so many mechanical backdrops to apparantly interesting phenomena and so on. 
4. Hence, the master slave dialectic might explain the persistant need for both to make sense of each other (Ferrier). Therefore the master looks up to the slave to attempt to makes sense of the situation. Hence, it might happen that pure slavery might not be sustainable and the slave might grow to influence the master in some ways. This body mind dialectic (rather than the confusing master slave dialectic sugestive of Hegel) is important of consideration to make sense of intelligence.
5. But it might be too farfetched to annotate historicity to everything about life. We see that in the germ cell, there is a possibility infact to hold the evolutionary tree still. The totepotent germ cell might develop into a full formed organism, in much different circumstances than the originator. Thus, it does permit an interruption of the flow, where we see that it is possible to codify the body (or the master) itself. But the presence of the code itself might not suggest tractability. No non trivial computing could be predicted by mere inspection of the code (as proven by Turing and as discussed by Langton). Hence, it might still be that while time is not a parameter to the starting function of the entire system, the subseuent paralellism and the time delays in execution of procedures referring to relative loci, might generate complexity. One ought to remember that the simulation is being played out on the supreme computer, the universe itself.
6. Hence, we do see that at times, there does look like that there is no body nor mind, because of mechanical implications into apparantly historical phenomena and the way simple mechanistic contrapctions could have computations intractable even with every atom of the universe being used as memory. The trick lies with the exponent. But practically, one might make sense that recognizing the rational faculty as being instrumental and distinct from a deeper self allows for a reflection of the nature of the self. This is ultimately speculative and reasoning about the self, could be sure of one thing only, its incompleteness that is. Hence, being aware of the incompleteness (and not arguing it to having been constructed from a purposely reductive set of definitions), might help with a possible will of mankind, that is to reason about the world and not be modified by external forces, without a reason, or more importantly against ones desire or will. Hence, the supremacy of free will or liberty is closely connected to the method of reason. This supremacy questions the forces of universe and allows the self to bifurcate them to be good and bad and not simply natural. Hence, freedom empowers picking up a path irrespective of the circumstances.
7. The present direction of computing might be interpreted from this normative premise (which makes our work somewhat less rigourous). One sees that computing is being developed to consider questions using methods of noise filteration and simulation as direct appeals to core intelligence, without requiring of epistemological explanation. In that we see that computing to be inherent non deterministic (see Turing who showed that the pathways of computing devices might involve time, as in a dynamical system). Since, the Turing machine (as against the FSM) involves extension temporally, waiting for events (which arises from paralellism orchestrated in the physical world), it itself is non deterministic. 
8. Whatever human actions in history had been, they had been but thinly veiled fight of evolution. There have been fight over ideas and religions all as means to establish the fitness of genes. This is the inverted reasoning in the extended phenotype. In this case, we see that there is no paradigmatic shift between the way computers normally work as Turing machines and their enhancement in machine learning systems. Computers have always appealed to the core intelligence, being non deterministic and had fulfilled the wishes of the master, without the need for elaborate structured argument. Thus, putting in a computer network, allows negotiation based on 'wishes' and not intellectual reflections of stoic duty. Thus, it furthers the case of evolution like none other. It improves the fitness of the species, thereby generating unprecedented surplus as capital that flows into barricading off the natural forces. 
9. We had also argued that the innate sense of good and bad has a historicity unparalelled. But we might also argue that goodness is borne of the concept of the norm. A normal distribution is directed radially, the center indicates goodness and all peripheries imperfections and suboptimalities that might be evil (which might be seen as temptation and sloth representing the edges of the curve). Thus, for a beach feeding creature, being too near the waterline (as a high risk strategy) or too far away from it (as a low payoff strategy) might both be simply variations of badness. A radial reasoning allows to locate goodness, from peer experience and with limted data without the need for a complete history.
10. Thus, we might argue that given that goodness and badness become available to the computational representative of human progress, an embodiment of such computational programs into physical systems, with physical consequences, allows a simulation to directly leverage the universal computer. Hitherto, the computer had been an abstract machine capable of producing abstract decisions. But if we look in terms of networks, they are paralell, working wiht the universal computer and having the norm for reference. A delination of physical beings is what is required for the physical strategies to play out. Hence, robotics might be a real and plausible course. The future computer might embody a set of constructional tools to build algorithms as a flow, along with notions of goodness and badness from the norm and a physical form to actually simulate in paralellism. This combination might allow the progression of primitive organisms, which might go feral, due to the absence of complete cost effective control strategies, due to the multiplicity of form and its heterogenity.
11. In the course of evolution, it had always been that life attempts to build stuff which is delicate and crowing glory, at a given point. Say the use of eukaryotes or that of multicellularity. These were at the point of their emergence, be too adventurous and delicate strategies. But at the right circumstances, the payoffs might be exponential. Likewise this might be a way to allow for a portability of the physical components of the being, which might cause an explosion.
12. Computers as we see them are already accessing actuators along with markets to attempt to recreate paralellism required for emergence. They cooperate using direct generation of events rather than a system of rules as with formal actors. These events might give rise at its earliest form encoding to caches. A demarcated computational agent might encode certain events (of peer computers) , such as say casting a shadow might discourage another agent from emerging from its hiding, for fear of being seized up and destroyed. Thus complex interactions might develop. We might need a formalism to construct this. For instance, let a system tune to a certain subset of events and a game playout in the real world. The outcomes are likely to be dramatic. These caches might in fact be compressed in some far future to create more deterministic heirarchies and more standard adaptations.
13. The evolution of the whole might proceed by cacheing and tuning to peer and environmental events. It would then need a simulation of the world itself in its physical being (now that it has a physical body) and play out in a field experimenting and searching. It might happen that the organism is constructed from a single coherent piece rather than multicellular. This compromises germ lines and reflexivity of the environment on pristine key value copies assumed to endure for a generation against short term shocks might be absent. Instead, the system might evolve smoothing and regenerating its strategic key values quickly.
14. The absence of the destruction of the body, might allow for more fluidic and quicker evolution, but it also reduces the drive to create subjective optimization, for which if the body is at stake, the drive is induced. Hence, a system that can immaterially replicate (as against the genetic materialism of DNA), can preserve its core code and add local strategies, in a multicellular like way and attempt to follow the norm, while being responsive to local adaptations. A consolidation of local adaptations might give rise to emergence of species. It might be a ghost in the box scenario. Some kind of marshalling is required to produce germ cells in order to avoid short term oscillaitons.





A formalism of emergent computational behaviour.


===
One might see that non abstract intelligence, easily overcomes the problems with paralellism. That is to say to know about a certain outcome, one simply lets it play out. This has no energy advantage, but being able to afford consequences by expendable agents might provide a solution.










===
Refer: Emergence of dialectic
Turing Machines are more powerful than FSM  and they are self proclaimed symbol processing machines and hence signal adapters. They do not do work, it is well known. They are non physical machines (cite HorÃ¡kovÃ¡, Klemen 20th century conception of machine)
Institute of Computer 
Attention needs to be given to the distinction between the way computers work as a network, thereby asserting their wholeness and buffering behaviour and in the way in which they work as discrete game playing agents thereby simulating reality or giving rise to emergent phenomena.
Much of the preference for computation based solutions reflect a rejection of authority. Computation solves problem by complex self regulation, rather than using structured truths. We represent that authority and truth structures are good, because humans are capable of moderation through appeal to reason and morals. A rejection of this philanthropic view, leads to a reliance on complex self regulation by search and negotiation on adhoc basis. This had been shown to be leading to emergent behaviour, strucutures and dialectic.

A Developmental interpretation of Computing
1. Modernity started to have to much to do with libertarianism as much as generation of surplus. There was seen an emancipatory goal in attempting to generate surplus from nature as well as to make sure that a person in general has choice that is maximized. The method of approach to generate surplus from nature involved development of rational framework which can spot events in nature, allowing for the construction of machines which could perfect phenomena that generate surplus.
2. Rationality is necessarily incremental, in that older concepts give rise to new concepts. Likewise machines are capable of building over one another and incrementally generate surplus. The surplus is then appropriated by the mechanism of governance. Governance involves in its canonical form running a registry of properties and promises. The promises are best represented in the distributed medium of the currency. The use of a mechanism to make sure promises are met, allows for committing what is material into the idealistic domain. The actual machines that are created by rational actors by their cleverness is submitted to common wealth in exchange of promises, which are always in flux. Thus, finance is the otherside of mechanization. It is an endeavour of modernity to seperate work from life in general. Work is proposed to be done as a rational process, which in turn is expected to produce incremental surplus capable of supporting idealist domians (such as politics) and therefrom the spiritual domain.
3. It is needed of the political side to be able to provide single source of truths as an authoritative source. The authority is reserved by monopolizing violence and settlement of questions by coercion and deliberation. The promises by the monetary system and the registries provide singular truths. The presence allows people to exchange material factors of production for such ideas (as money and state). But there is in coherence to mapping from the material dimension, an expectation for monetary sphere to constantly expand. This necessity of the money to obtain a positive feedback or generate a surplus is the idea of capital.
4. In the rational schema, as we have seen libertarianism holds high sway. Hence, capital expresses the ultimate free choice of the individual to do what he wants with his invention (includng to hoard and thus hold it exclusively, without necessity of explanations). The capital is deployed into the mechanical innovations, which rely on rational methods. Thus the loop runs from the machine world, onto state authority, onto monetarism and capital. The markets blunt freewill, but in many cases capital involves the ability to carry out things irrespective of the wish of the market. The market provides statistical feedback to the captial. The western libertarian concept of development where freewill and surplus are available, is obtained by this dyad of capital and rationalist machines.
5. However singular truths are not sufficient to sustain humanity. The government also knows this and hence truths that are continuous, as bands of prices (as a distribution of prices about a mean price) is determined in the market. The market is hence a non authoritative determinant of truths as a statistical distribution. These truths also guide the application of capital. Therefore it is possible that norms could guide free willed decisions. 
6. Ideally the government running the sources of singular truths, by coercion and by boundary preservation allows it to appropriate surplus and it could thence go ahead with public spending for the development of the society, based on prerogatives of planning on the best courses of application of capital. But private capital, as advised by market prices go into the construction of further factors of production. This necessity for finance expansion pushes the employed capital to search for new rational fixes. As might be seen from history, reductive solutions frequently have low marginal returns. Hence, capital pushes for transnational flow and that had been the engine of imperialism.
7. The government would hence have to take the side of the capitalist and allow for their lobbying in order that it could ensure a financial expansion. It would have to assist the transnational flow of capital, sometimes by chartered companies as with the imperialist period. This causes a divergence of the role of the government. In other words, the position of governance, becomes destabilized by this need to rely on private capital, as being captialist or acting in a reactionary mode. Thus, wild oscillations might arise. Some governments might think the push of the capitalist as being unpopular and side with the majority and revert to public spending being the primary mode of operation or resort to fascist agenda, attempting to redeem the individual from the grind of the mechanical, monetarist, captialist, militarist diplomacy, that has inflationary tendencies and irregular oscillations (due to the reflexive anticipation between incremental mechanical advantage). 
8. This in turn leads to violence. Thus, the postmodernist reaction involves a rejection of centralized authority, which work on dominant narratives like that of capital or the need for protecting the common man against capitalist choice. It would hence necessarily have to reject rationalism, finance in order to stay consistent. The twin problems of state violence and the bigotry involved in stating the benefits of reductive modelling, while modernity does not count side effects and waste was the wind in the sails of the postmodernist movement. The postmodernist movement hence takes a toll on the power of capital, of the state and of the development of powerful rational models that could be made into engineering feats. 
9. Thus the attack is essentially directed at reductive modelling. The concept of the machine as being a heirarchial rationally explainable concept wiht a positive feedback hence came to be revisited. The new machine is event based, which works not on rational concept that is preserved and sealed by capital (as working capital of large corporations operating logistic networks), but as highly adaptive, responsive machines (as with ola). This adaptive machine is hence the abstract machine, that had materialized as the concrete machine. 
10. The primary instrument of control of humans to the event driven machines is by feedback. That is to say, the actions of the machines is responded by humans by events, which in turn shape the cacheing behaviour of machines to act to certain events in certain manner. 
11. While this transition is in the making, humanity also suffers a crisis of conscience and identity. The human consciousness involves an ability to reflect on the entire schema of rational engineering giving rise to incrementally productive machines, being balanced by finance and accumulated into capital subject to free will, moderated by the norms of the market to guide actions. The ability to reflect allows humans to see the runaway complex in the way of working of the world, as distinct from the human desire for perfection of free will. This ability to reflect on the mistakes allows for the construction of an unchanging concept of the self, by expanding the consiousness to ways in which people can construct things which do not exist in free form. The causal connectedness discredits the originality of any observation. Thus, we might say that machines, capital, state are all connected and thus the self which is capable of observing it critically is more consistent than these systems.
12. We might see that while modernity was successful in seperating work from general living, the complexity arising from the incrementally productive machine, capital and its pressure on the state, giving rise to a paralellism arising from an oscillatory state policy on capital, due to the need to artificially prop up capital (or condemn it) away from the forces of the market (which is quite vulnerable to tampering) and the real world solutions generating surplus. This complexity due to ambivalence of state policy arising from the vagaries of rational machines and the transparency concerns of the market itself, leads to a frustration with the complexity. Over time the complexity becomes so high that the individual in the cultural sphere away from the work sphere is primarily spending time in deciding on how to dispose off his own capital. There is no direct link to rational material world of the factors of production, nor of the free form market. Thus, whatever culture is left off in arts suffers a crisis.
13. This artistic crisis actually triggered postmodernist expansion. The postmodernist point is to attempt to dethrone the authoritarian single source of truth from the government or its coalition with the capitalist, which in turn is dependent on the rational mechanistic performance and instead attempts to work with the market itself. The computing machines generate and assimilate events from the market, without a need for central authority and it seems to be close to perfection. But the side effect of a loss of visibility over what is constructed by human agents and how they mesh together to create the reality, causes a crisis of conscious expansion. In the market, the human consciousness could not see large scale phenomena, which it could attempt to control and experiment with. All it sees is the local context and this leads to a fragmentation of the consciousness. Culture is the ability to reflect on the history and the absence of history is a crisis too. The old schema of seperation of work from living is also passe,leading to action having no idealist representation in an abstract sense. Thus, computers become the most concrete machines.
14. This crisis of identity, drives humans to attempt to construct local identities around local contexts. These local identities might have an effect on the feedback of the computing systems. The computing systems become exposed to control by feedback among its own peers (which now have divergent feedback providing humans  and are hence much different and competing). Thus there is a loss of control by feedback from the human end. The peer to peer feedback gives rise to an emergent consciousness, which is often seen as the â€˜goal misalignmentâ€™ problem in AI literature. The two consciousnessess are at this moment in history undergoing a dramatic clash. The former is the consciousness which needs reason around constructs in order to make sense of the world, in terms of why it has not been rewarded ( a conscious that expands based on knowledge) or why there is pain and the conscious  of emergent peer to peer control among computers. 
15. Prescriptively, one might see that identity might be dynamically managed by recursive construction (after all recursive construction as with fibonacci generation algorithms had been successful in tackling dynamical problems).
A Developmental interpretation of Computing-2

Here we attempt to frame computing as an abstract machine that attempts to provide solutions without explanations. The superfluity of explanations and the rational method is attempted of presentation here as a fleeting construct of four hundred years of modernity. 

Defining Development
Development is seen as the ability to increase surplus in any frame of reference, be it a family or a nation. The other dimension to development is how the surplus is applied. That is to say, once the surplus is available, it ought to be applied to create a barrier that seperates the randomness of nature from the self. A perfect insulation is not possible but in such an utopia, a given individual is able to do such actions that have no explanations, as manifestations of his ultimate sovereignity and free will. 
But clean seperation from nature is not possible. The internal entropy to the system causes emergent problems. The ability to defer internal entropy from building up involves growth. That is to say, as the internal entropy builds up within the system, where there arises disharmony and random movements of libertarian individuals in the society, leading to the weakening of the fortifications, one would need to rely on absolute incremental order being generated in order that the inertia of the internal structures are overcome. The net energy flow inside the system allows for reformatting the system dynamically so that entropy is reduced. If the growth can be sufficiently subtle as to render it an oscillation about a mean position as a method of stability is theoretically tenable.
Interpretation of Collective Growth
As growth materializes, there is more surplus available and this available surplus is required of appropriation to the private sphere. This would need that the private sphere involves zero sum games of cultural celebration and spiritual reflection. The seperation of the public sphere from the private sphere requires polity to safeguard the private rights of the citizen. For this to be set in motion and sustained, there are costs involved. Hence, the state collects portions of the surplus arising from the employment of rational tools in solving problems of nature in order to set a system where an authoritative version of the truth could be delivered. 
Hence, the state could not be sustained without surplus and the rational enterprise is under stress to produce a surplus, rather than simply zero sum costs. There are aspirational estimations made in deriving cumulative mechanical advantage, which works most of the time, leading to an incremental surplus. The surplus thus put into the hand of the individual after paying taxes, then becomes capital, with the potential to create further capital. This capital is looped to the potential to create more cumulative machines. This loop of finance and mechanical advantage leads to a spiralling out of surplus. This surplus could be applied in the public sphere to create commonwealth. 
This commonwealth of resources, which allows for public infrastructure, fortifications and expansionist or defensive campaigns arises from the ability of governments to dematerialize physical machines into promises that flow. These promises are validated from the central registry of authoritative truth. The flow of promises allows for public sphere discussion over generating surplus, which happens in market transactions, wherein statistical truths are delivered (as price bands with distributions). This use of rational mechanical advantage at the level of a society causes policy dilemma. At the same time, the society as a whole is seeking mechanical advantage. Normally, it should be the knowledge that flows across individuals who may then utilize an invention and replicate it, to create surplus available for their own disposal in their private sphere. The flow of state sponsored promises in exchange of the material factor of production, causes the dilemma wherein the state becomes responsible not just of maintaining the law, but of herding this new found group behaviour.
The market becomes rife with rigging. Likewise the flow of capital across borders wants to involve state diplomacy and military might. It might also further reforms using the state as an agent to consolidate factors of production to reap mechanical advantage (such as bulk acquisition of land). The state hence becomes deeply inspired by the capitalist. Policy is however polar, in the sense as one set moves to support the market and the capital, another side attempts to relieve the pipleline of generation of surplus, which attempts to push the state rather than think of genuine rational mechanisms. The misuse of state machinery and the stateâ€™s helplessness in order to sustain the flow into its exchequer arise from the limitations of the rational method to consistently generate mechanical advantage. This is an area of unpredictability. The need to smoothen the inconsistencies of the rational method, involves the state playing the role of the stabilizer, obliging here and when the cartel grows too strong dispersing it.Hence, the root cause might be traced to the problem of the method of reductive modelling. 
Hence, the problems of development in the modern state involves destabilized state actors, transnational violence, imperfect markets and the usurpation of the private sphere into dedicating time in deciding on the real world feedback of rational productive techniques (by participating in the market as consumerists). 
Why the reductive modelling fails:
Reductive model suffers when non linearity is involved, which arises from paralellism in the real world. The market is a good agent to distribute the determination of the fitness of a solution in paralell mode, so that emergent wisdom might be approximated to the fitness of the solution in the real world. However, the market is a much smaller computational system, when compared to the world at large. The market is subject of fallacies that could set expectations by mass delusions. It might be nudged and influenced, even by the government and cartels. The market being imperfect reductive models are evaluated in an unstable manner. 
The more fundamental reason for the failure of the reductive model is the inherent risk in increasing completeness at the cost of reducing consistency. Thus, the system itself acquires paralell dynamics and starts working imperfectly. There arise fluctuations to its mechanical advantage (more precisely due to the environmental dynamics). A sufficiently powerful simulation does not exist and abstract reasoning is inherently problematic. The reliance on markets is also imperfect. Attempts to stabilize the reductive model paradigm leads not only to state destabilization, but also destabilization of the environment (by waste disposal/unaccounted exploitation) and of the people unprotected by state machinery (either due to policy or due to flaws in implementation of the state machinery). It also furthers cross border expansion and violence. 
The linear modelling involves construction of a serialized model of the universe. The events that are thrown into contrast and cherrypicked and assembled into machines capable of work. There are other events working in paralell, which alter the course, but it is incapable of being incorporated into any language, due to the linear nature of reasoning and expression. Hence, the only solution to reductive modelling is to absorb the risks therein by the capitalist himself. The propogation of the instability makes it complex for everybody. But it is difficult to estimate the failure probability of a rational fix in order that a control of the absorption of risks.
The Control Problem
Control of the betting in reductive engineering while maintaining sufficient buffer is a hard problem. So far the feedback from the system is too strong that influences the control mechanisms themselves. A wide berth leads to excessive rewarding of failed cases and a narrow berth leads to propogation of the shock. It needs a dynamic solution. A static wall around the workings of rational systems, requires setting up a fixed outcome from it like some taxes or royalty, which is very low. This might cause a growth in the governed system to the extent that governance becomes impossible. Dynamic smoothing by state mechanisms, natural dumps and markets have all yielded imperfect results. 
The development of the machine which controls the rational machines themselves and buffers all user feedback signals without revealing the global context is achieved by the invention of the computer. The computer puts each capitalist in a local context and he optimizes to immeidate problems.
Stabilization by Ballistics
The control problem arises mainly because of there being the need for scale in many cases in order to realize the benefits of rational machinery. There is a steadily increasing compounding of machines in order to achieve results. This is like a setup of unstable heterogenous blocks, forever adjusting and correcting their positions, sending shockwave across the substrate. In fact finance was introduced as a way to stabilize this setting. Finance market allowed for highly divisible and fluidic counterpart to the machine based ecosystem. But over time, heterogenity started to emerge in finance itself, with Banks and other institutions chunking up money.There are instruments and currencies that self referentially stabilize and influence the finance market. This is highly resented by the Von Mises Institute. 
Computing approaches the problem of ballistics from the other side, it slices the assets in time and space into tiny chunks, so that they can move about ballistically, hopefully working well with the financial and governmental stabilizers. But finance also started out this way. The agents consolidated slowly driven by certain behaviours. The agents who are supposed to optimize locally with respect to their reward seeking behaviour hold on to positions, anticipating gains or minimizing losses and ignore stimuli. On the other hand, on the moral front, where they are supposed to look in the long term, they attempt to optimize locally. Thus, we may say that much of the amoral agents tend to consolidate.
Amoral Agents consolidating behaviour:
The consolidating by amoral agents are however under control in pragmatic cases by the bureaucratic agents, which might be streatched to include the judiciary. The bureaucrats do not appeal to mythological objectivity, but derive a sense of justice from legal prescriptions which are logically consistent. They themselves are stoic agents, rather than holy men. The second type of control in pragmatic society exists by means of popular appeals to moral outrages. This arises where political agents see a problem and instability and attempt to generate public voice in stabilizing the setting by applying a strong intensive force.
At this point, we might think of computing to be dedicated to stabilizing the setup by providing a ballistic proxy. It might hence not be able to defy control by legal and moral outrages, eventhough it might consolidate as much as Banks did in the past. Hence, there is likely to be a continuing instability to the setting, but they may still be under control by fixing ownerships and such methods usually applied to emergent institutions. 
Role of Computers
Apart from displacing capital and consequent need for authority and hence moral reflection, computers however might land the people with a situation where they have no choice to be â€˜goodâ€™, even if they wish to be heroes. This itself might prompt a moral outrage, but it is difficult to anticipate how it might be formed. All events that are registered to a computer, results in the computer attempting to hairsplit some resource allocation in real time. The simulation worth of computers to project and hence predict the outcome of certain paralell processes are limited owing to the exponential wall of combinatorics. Hence computers themselves might provide little by way of control and explanation by acting as rational agents. They might be quircky and use stochastic methods.
A More speculative role of computers
But computers as much as they are controllers which work by providing feedforward controls to systems in the nature of resources, they are also controllers of natural processes themselves. Illustratively, if we see that a system of drones are allocated in deweeding and pest removal from a heterogenous field, the system is effectively controlling the field. This is unlike financial control, in that the computer can directly signal actuators (by logistical reasoning). This kind of control, directly has the capacity to displace rational, explainable fixes of reductive nature, thereby shaking the very foundation of the problem of instability. This kind of control that simultaneously smoothens the extraction of orderliness form the universe as well as the buffering it to heterogenous systems (such as societies). 
Thus the abstract problem of smoothing natural systems in such a way that a heterogenous evaluator of noise is synchronized with the oscillations of the natural extraction and the distribution is radically made possible by computing. Hence it might be a paradigmatic shift in the control strategy of humans.
Rigour to the approach:
The problem itself is one of an universal controller. The problem of universal control entails certain paradoxes. If a minimal tapping is done of the benefits,there arise a dilemma where the system being controlled reacts by arming itself or if over exploitation is done, it collpases. There is an elusive optimum with respect to control of such systems. Due to this reason, it is difficult to make a formal expression of such problems. There is always a presumption of the reader being in the context and an appeal is made to his higher senses, by a choice of words, which are intended to resonate. The concepts of morals and the distributed nature of the problem itself, that is we do not have a single port to plug into and control. There are many areas where tiny instabilities arise and are controlled at many discrete points. These distributed problems hence permits only appeals to aesthetics rather than appeal to reason and hence formal expressions might be absent in our discourse.
Note
We are presuming feed forward control only, while thinking of ballistics in finance. The finance adjustments might have feedback on the diligence one puts into rational innovation and its representative accuracy. A highly granulated ballistic might avoid feedbacks.


The Concrete Problem of Control 
An automated system of discrete control strategy selection.

In applied medical decision making, we see that there are discrete regimen which are control strategies applied to specific sets of situations. The threshold of switching between regimen involve decision making by professional heuristics. We see that the threshold might be predicted to a certain extent by the utilization of Markov chains (smoothed by a second order markov chain). 
We treat this problem theoretically that if one could provide an automation of the selection of control strategy by defining recursively multilevel markov chain, so that the threshold for switching might be evaluated despite time heterogenity. It would however need an ordered set of strategies ascending in terms of risks or costs.
Part 2
In this specific case we see that there is to be given sufficient importance to heuristics. We might say of knowledge certain things. Within a domain, knowledge is obtained by induction, wherein the general case is obtained from the specific and by deduction wherein general rules are applied to specific cases. A domain normally has an inverted tree structure, where in pertinant types are classified. Some of these types are repetitive across paralell branches but they might themselves have specific denotations. Say we consider the human stature to be a domain, the specific types that underlie is an inquiry into height and then into weight. We see that the combination of tall and light might be denoted as lanky, while the combination of short and heavy be denoted as squat and so on. Likewise, the leaves of the tree ends with specific individuals under each of these categories. There is no ambivalence as to fuzziness in each of these sets. But that does not hold true when we attempt to relate domains. Say, another domain such as heart health is presented and a similar classificaiton is made with respect to endurance strength, throughput, peripheral blood pressure and these are combined differently in different domain terms to result in multiple sets which end in leaves. Now, there exist relationships among these leaves, which have membership to both sets. But the relationship of a specific percentile of people from a specific subpopulation in domain A to another subpopulation in domain B provides clues as to how the domains are related at the general level. Statistical modelling does this considering random draws from each of the domains and based on assumption of one of the domain measure being a quantile, one derives a density function.
The domain model as we might see relies on specific typing, wherein we recognize the presence of definite classifications and lack of fuzziness. Whereas mapping specific entities between domains presents us with opportunities to understanding specific risks. The statistical distribution might provide clues here by skewness, but it might also incorporate cancelling out effects, leading to a normal distribution. The interdomain links signify causal relationships, wherein the mechanisms might not be apparent, for otherwise, it would have been that the domains be combined. Say like the domains of stature and athletic ability, we might be able to mechanically related specific stature traits to specific sports. In case of domains without direct mechanical connectedness such as stature and heart health, one sees that relating subdomains more granularly might help deduce correlations at levels higher than the individual. The interest of knowledge is in the generalization. The relationship between general categories is less random than that occurs in an individual. The way other than by means of randomness by which relationship could be established is by mechanical causality or in case of mental phenomena by signaling (such as height signifying social stature thereby strengthening prior expectations (biases)). 
Among non negotiating agents, the relationship is one of connectedness (being suspended in free space without connectedness would mean Brownian motion). The connectedness of a squat stature might be distinguished between obese and lean muscle squat stature. In general the occurance of certain combination presents a paralell way to equal levels of fitness in evolution, which again confounds search for causality rather than randomness as the method. But if we move out of biology and sociology, we might see that the association between occurance of certain chemicals in certain environments, might be traced to the distribution of the generators of such chemicals from subtyping in both domains (of occurance of minerals A and B perhaps). Thus, we see that B is a modified form of A due to certain interactions with the environment. Thus, it results in subtyping relationships by itself, creating new domains between the two original domains. Typing itself signifies discrete variation of a continuous property of the general concept. Discrete variations are according to theory of causality caused by external agents. The other type of relationship might be continuous variations like an oscillation. Hence, type systems within a domain signfies causal forces at work. Conceptual relationships between entities are hence as discrete occurance of a qualitative feature and different heirarchial combination of features to produce discrete heirarchial types. The domain hence does not signify causality, but only discrete occurance (as being the materialization of sufficient conditions under the overarching required condition). However relationship between domains among conceptual entities might lead to new domain models of intermediate types, or it might signify a mechanical causality, where one property arises being caused by another directly. Where indirect causation is involved, confounded by various environmental variables, one has to go by either statistical distributions of cross domain models. Say we arrive at a cross domain model of hearthealth and stature, we start with classification between obese and lean and then among the lean we further classify on height. Petite stature might signify a preexisting heart condition which would then give rise to this new classification that uses properties from both domains to produce no causal relationship, but only a qualitative occurance of specific types circumscribed by more general types. 
Part 3
In this regard, if we look 











An Abstract Problem of Control
A statement that control system that synchronizes two heterogenous systems of asynchrnous stability as being an ultimate control system. This might be applied to the problem of control of physical systems (say fields) and of smoothing the divergent goals of a more intelligent system (say a village). A control system that is able to smartly allocate resources to the field and also appropriate the surplus and circulate it among the villagers by evaluation in fair games is an ultimate control system. 
We also need to evaluate if such control systems be nested. That is could there be a hierarchy of controllers running between two subject systems.
The Possibility of Justice
I was concerned if justice could be reached by purely logical means in the early part of my career, which attracted me from a career in law into one of information technology. If a logically consistent system would mediate between conflicting interest, would it be possible for justice to be delivered. For instance, if a factory could be constituted as a separate profit center in accordance with Activity Based consting, would the workmen receive better wages, was the question. This was intended to thwart the heuristics associated with the rendering of justice, where law would have to interpreted and deliberated before a conclusion is made. Nothing flows directly from the law, in fact without contextual interpretation, which ultimately involves heuristics and hence apriori knowledge in the nature of a moral framework. There have been attempts to make ethics a logical system since antiquity. Ashoka had tried this and many from the yogic tradition. Descartes was a pioneer here. But in every case, people ran into the mind body problem. That is to say, the why of the desirability of moving away from a morality based system to a logic based system has to do with the mind body problem.
A moral mind, with high degree of freedom, is essentially tied to the body, which is effiminated by the cravings and the attraction to pleasure, received through the senses. Hence, pure idealism is difficult of attainment due to this problem, often refered to be the tragedy of the human condition. Even if one is to make a bold attempt to train and suppress the senses in pursuit of unbiased and beautiful knowledge, the body has a trick here. A greater than normal idealism, fosters a sense of conceit, wherein the individual starts pampering one of the most powerful of sensual pleasures, one of moral superiority, which prompts him to exercise ostensible authority and seek further respect and comfort, feeding onto a pipleine of greed, which then propogates through a network of followers, creating a heirarchy. 
If we look at the case of physical systems
Developemnt
Industrial Development â€“ East and West

The idea of industrial development relies heavily on the concepts of abstraction. In the west, we might see that since classical times, there is an interest to describe things, analytically and in terms of abstract pure forms. All the physical world is explained in the west as practical adaptations or corrupt forms of pure forms, such as a curve, line or point. These could be in pure geometry seen to slide and slip to be capable of being seen as the pure forms of the basic machines â€“ levers, pulleys and inclined plane. The machine itself is mainly the frame with fixed points around which lines and curves interact in most cases. Thereafter philosophers developed notions of dynamic systems, where the result of the past served as the start point of the next cycle (involving the exponent) and even when it comes to chaotic movements like Brownian motions, the west was there with a thoery. Thus there were theories of mechanical systems, dynamic systems and chaotic systems all pointing to an orderliness in the construction of the universe. A look at the conservation laws, the absolute points such as 0K ,the exponent and the originality of linear and angular momenta, all supports the speculation that the maker had made it by simple instruments with regular constraints. Thus, theorems are those that exclude arbitrariness. There are no endless sources of energy or self propelling machines. These are the expositoins of modern sciences from the west. It is then sought to be encoded into the way humans construct their  lives as well, using regular cycles and geometric frames to generate things of value, such as a spinning mill producing elaborate drapery from elementery movements. There was a strong reliance on symbols, which might be referential or appeals (as with infinity) in the composition of knowledge.
The initial root of the idea of perfection, of perfect forms, persuade humans to push towards perfection of their actions, without spillage by corrupt redundant exchanges and actions. The frame of the machine was supplemented with frame of theorems in law, social conduct, therey excluding arbitrariness in all actions of value to the society. It is seen as the way of collaborating with nature, as nature itself is seen as an orderly machines and in doing so in more refined ways, humans we might say are more developed than other intelligent creatures. As for the private sphere of the person, who having emulated the perfection of the Maker in the creative activity, modernity leaves it to the liberty of the individual. Thus, the value production and its incrmental perfection is aimed to be an end in itself. The body is seen as a burden in many cases to the pursuit of perfect knowledge. But there were romantics who appealed to the rememberance of the presence of human soul and of the Maker and their essential freedom from theory. Scientific progress comments nothing on this as it is with the point of why, we do things. If one attempts a holistic interpretation, one might interpret it as  an extension of the pursuit of normative actions (going by historic order) or even as a state of wakefulness that had been usurped by the bodily corruptions (as with Platonic past state of perfect goodness and knowledge). Hence, it is ambiguous territory to inquire in the nature of industrial development and there is no way to know of the basic nature of the soul, which might be seen as projecting the idealistic activity, while being embodies. 
The east on the other hand, produced little by way of opportunity to inquire into the orderliness of things. The multiplicity of life forms in the tropics, the quickness in which buildings, people, artifacts (as books and tools) are churned in the humid tropics, the vagaries of the weather and of diseases is crushing of the ego. One sees not the orderliness to the existence, but a chaos. Eastern culture is ambivalent, distributed (even religions like Hinduism are highly distributed, eluding simplistic codification). Nevertheless, there had been interest in attempting to codify the prevalent norms as dharma, which could serve as referential points in the stabilization of the system in the short run, rather than absolute truths. The buddhist middle path emphasizes this. Postmodernist philosophy of the west is greatly influenced by the eastern tradition. It might be despiriting to see how the ideas themselves are greatly influenced by the geographies and what nature to show humans, signifying how deeply the human form is mired in the circumstances.
Even the west, the repeated frustrations over the discovery of non euclidean states, of quantum mechanics and similar theories which decried a complex mixup of continuity and discreteness to the nature of the universe, eventhough was negligible in practical modelling, shook the philosophical roots of the west drastically. The moral right of the west over the development program was lost and they conceded as is the universal apriori law of morality and respect (cite Kant). The world was started to be seen to be capable of being approached in relativistic terms, without notions of central and universal principles. Each thing existed in relation to others and evolved dynamically perturbed by inexplicable forces. The notions of stability, as practical way of securing knowledge about the universe rose to prominence. Theories of relative equilibria, like game theories and of mathematical treatment of stability and control was strong.  
The course of history was also one of outrage on modernity. Modernity seemed to perpetrate injustice. There was a reliance on technical concepts as an agent of discrimination and exploitation, which resulted in war, violence, famines, pogroms and environmental destruction. As said, often times, the technical notions of modernity was used as an instrument of oppression, such as for instance the inability of native groups to write using alphabets is seen as a fundamental case of underdevelopment, which it is the burden to improve. The system as with many natural system is seen as a corruption of natural pure forms, which if possible could be worked upon or be eliminated and displaced by more pliable systems. Even such systems so improved are to be kept under control of the enlightened and pure forms of knowledge bearers. This naturally infused moral legitimacy to imperialism, environmental destruction as burning down forests and slavery. 
The primacy given to idealism in western philosophy was in fact phenomenal. It appears that the explanation for human imperfection rose from the inability to constitute a authoritarian entity that could dictate the efficient use of resources, as alluded in the tragedy of commons. If each one is to know that everyother person would be fair, then humans might tend to be fair. This is capable of achievement only if coercive power is vested onto an authority, which survails the population and exacts a penalty that is disproportionate to the wrong. This is a technique of control, where signals achieve primacy over interventions. The desire for being controlled might itself arise from a desire for equality and justice. Everyperson would be willing to divest his personal rights in order to secure the ability to be able to pick between peaceful retirement and active participation in securing the seats of power in a drive to self actualize. Hence, the desire for equality is powerful as well, which legitimizes the case for centralized control. However, control by means of negative reinforcement had in recent days given way to positive reinforcements, where everyone is uniformly penalized as with taxes and benefits are given to the enterprising who go beyond their bare duties. This seems to be working for most cases of non criminal behaviour. The desire for coercive control and participative control rests on the view on natural truths. 
In all of idealism, it is very common that the idealism being a clear stream that it is is appropriated by agents giving in to their baser drives. The body after all is inseperable from the mind and natural heirarchies and even undesirable conditions such as inflation, uneployment which tend to arrive at an attractor value despite being regulated all point to the tendency of natural truths to subvert those that are idealized in their pure form. Communism is an interesting experiment to put forth the idea of development into rectifying the influence of peer pressure to corrupt. Thus, by planning ahead and allocating resources scientifically, it was seen that people might be liberated from their peer pressure and thus be able to self actualize, thereby building great strucutres, innovating in domains (see White revolution) without expectation of baser comforts. Thus, the origin of corruption was postulated to lie, not in the mind body problem but in the problem of population growth, where uncertainty exists over allocation of resources due to variable ability to expand ones gene pool and reserves. If these were to be mitigated, by communalizing the factors of production and subjugated to central planning, the result might be an invigorating liberation from the necessity to corrupt and people finding their true spirit to create and redeem themselves. But again, the natural tendencies took control creating heiarchies, evoking mythological status to bureaucratic positions, thereby leading to instability in the system, that would cast its spectre, long after it is gone.
This had been the case with many pursuits of humanistic glory, even in seeking godliness, humans seek their own glory. The statues and monuments that praise such glory of liberation were trodden down to dust as history marched on, dramatically seen in the 90s collapse of the eastern block. Coercion as being desirable, rather than arising from contracts was found to be unromantic and the postmodern concepts of development reached new highs in the 90s. The foundations were laid in the early fifties, with the Truman speech on development. The west post the war, adapted a softer stance of projecting its idealistic metanarratives, thereby exerting leading influence on the rest of the world. This resulted in two warring camps â€“ one suggesting a negative reinforcement driven feedforward control, termed the east block and the west which had as its strategy to incentivize mostly by loans and impetus to generated desired outcome and relying on a feedback driven control model. The latter was seen to be more corresponding with the natural tendencies of humans thereby being more pragmatic in implementation, while still being idealized with respect to development. The feedback correction in the case of the east block was very strong and unforgiving of human fallibilities. In the western model, the feedforward control, which involved planning up, created opportunities for captializing of past good deeds and use it as a platform, not to self regulate, but to propogate arbitrariness, in the sense of business expansion and cultural manipulation.
Thus, we might even state the irreconcilable dichotomy of liberty and equality in terms of control. Both of these are social constructs and legitimizes control by a delegated, albeit independent agency. In the equality hypothesis, everyone is aware of the ultimate stable state of human societies and only the really corrupted are not aware of this and would need coercive correction. In the libertarian hypothesis, one is only conscious of his individuality and freedom, the question of his being equal to another does not arise in this situation. If it were to emerge in the natural course of conduct of being free, subject to emergent or natural constraints, the libertarian grants it to be just so. Thus, libertarianism is also associated with romanticism in its acceptance of constraints none other than natural. If one is to nevertheless have a state, the point of allowance would likely be towards coercive taxation and a dynamic, positive feedback driven posthoc control, with no planning. However, an incentive is not random, but advertised, as much as desirable behaviour is generated, the good arising therefrom is ascribed exclusive rights. There is no theoretical way to charge specially for state services for common services, as a result of which while a bulk of state costs go towards protecting the wealthier, sustaining markets and infrastructure therefor, the wealth generated therefrom is assigned exclusive ownership, wherein the owner has a right to exclude from productive use (to simply let it idle or even destroy it). They would need incentivization by inflation in order to be convinced to invest their exclusive wealth into the system. The inequality created by an ostensibly indifferent concept of liberty might be difficult of explanation. It is most likely caused by the established systems of service which provides an implicit feedforward plan set before the general population to follow the money. The wealth is seen as the indicators of the right path and the capital that goes into enterprises sets the plans and standards for action. Thus, a biased feedforward plan emerges from the system, much like the emergent feedback plan of the east block that consigned numerous people to the gulags. This is apart from the great friction these two ideologies caused between them.
Therefore, in a post ideological world no hard stances were taken, which resembles the west in many respects and might even mean a tacit feedforward and a strong feedback driven control mechanism as being legitimate. A genuine decoupling from ideologies such as with the dharma bums, might be difficult of attainment. It would simply imply a certain laziness to control, where there is a great reliance on the natural tendencies, rather than conscious attempts to build. In the east, as said chivalrous enterprise, to conquer and to propandaize is limited by what meets the eyes and senses, overpowering it with the ability of nature to destroy orderliness and leads one to take a more passive approach. It appears there is a critical mass to the move from pragmatism to idealism and that critical mass in premodern east was not accomplished. Hence, the east was pragmatic, free from the idea of central cognizance of control. As said, both east and west are rooted conceptually in the idea of central control. If control is an ontological question as in the east, the strategy does not matter.
The eastern idea of the individual is more in continuum with the environment and the group. The languages are much less definitive in denoting things, but rather in practice often rely on curious tonal constructs (adukkuthodars in tamil) to denote phenomena and states. Thus information is not portable immediately from the context. Likewise, the whole idea of power, control and planning is seen to be illusory in many philosophies dealing with fate and karma. Thus, the east and west took up what they saw to be the nature of the world as discerned by theis senses, not because of sensorial refinement, but rather by climatological and environmental conditions (echoeing Jared Diamond here). The very act of a networked society favoured the lesser determination of facts and caused it to evolve more as a network of distributed concerns and controls. 
As for the crucial question, as to which is the right representation of nature, one cannot help get speculate. Ferrier says that it is important to be true to the method than the truth at the end of it (Philosophy might be wrong, but it ought to be reasoned). The question is essentially one of method versus the end. In the west, the method reigns supreme, even if the end is undesirable, from what one premises, the consequence ought to follow irrespective of favour. The east does not care so much for the method as the result. It says that there are just too many variables to be capable of being modelled in a logical way. It strongly criticizes pure reason (akin Kant) in dealing with the world. In Kantian philosophy, the apriori moral knowledge is powerful influencer, which is akin to the eastern conception. The universal accessibility of ethics with no central authority, allows for stability in a natural manner. The influence of the east on the west was powerfully felt post the fifties. But as discussed above, it also served as a platform for a strengthening of libertarian ideals of the western market democracy in a never before fashion. The indeterminacy of plans and heirarchies as per eastern philosophy when hybridized with the east bloc west bloc dichotomy of western control based model, favoured the west bloc and produced a universalization of the market economy in neoliberalism. 
The idea of development in this point was an evangelization of the free market, democratization of institutions, liberation by education among others. It is curious to see that democracy was a western product, while the west was attaching so much importance to singular truths. Thus, as much as there is an embrace of the method in discovering facts in science, mathematics and from records in natural history, there was an explicit disempowerment of central truths in policy and power. As much as the rational faculty ascended as the king, the human king ought to step down. Thus it does not recognize as much compartments of truth between the world of humans and that of nature and appointed the bureaucracy as the facilitator of actions. But it did not end there, elections and popular mandate was incorporated into the system as an inelegant fix, as a way of appealing to apriori distributed knowledge. Thus, as traditional authority made way for the rational authority, it was also followed by the more intractable charistmatic vesting of authority in the west. 
The dualism of the west was strongly philosophical. We might see that philosophy is self referentially extended about a null space. In the west, this sphere of emergent truths was provided for rationally. Reason could appeal to imperceptable entities through symbols (such as infinity) and hence it is possible to frame within a rational framework, the highly variable outcome of mass opinion and construct over it. Thus, the west used a dualistic extended philosophy to reconcile its simultaneous interest over methods and results. The west thus celebrates the possibility to philsophize along any axis. One might promote romanticism and it would be a legitimate idea in the system that is completely idealistic and indifferent to rewards . We might say that rationality is a narrow term to describe its philosophical ideal plane. In this we place the reference to what could not be known (Agnoilogy), such as null spaces and infinity might be  the basis of ontology, rather than epistemology (to contradict Ferrier).
In the east, everything is grounded to the reality and context. The philosophy of the west is rooted on symmetry. Somewhere it is said that imagination is the true mark of intelligence. There is a lot of construction (followed by a postmodernist deconstruction) of things that do not exist in reality in western philosophy. Every construct has its negation, thus signifying a zero sum plane (where it might exist irrespective of rewards, being conservative of energy). In the east, there is still construction, but of disparate objects that have mythological significance, such as that of varna and jati. It has elements rooted in an open energy system, where mythological truths consign a certain direction of actions, which inevitably involve energy flow. In the western construction of imperialism, the logic was to balance exploitation of resources and markets as a way of modernizing the natives. It was idealistically placed as zero sum, though a definitive energy flow existed, much to the repentance of the west, when invoked by people who argued their way to freedom, such as Gandhi. But in a mythologically oriented system, the definition of concepts do not have negations. They have natural adversaries, such as varnasharama dharma and bhakthi, but these are centered on a internal consistency of moral rightousness rather than referring to the other critically. There have infact been theoretical discourse in the east as between theological schools of dvaitha and advaitha. But the hollowness of such debates had been much criticized in eastern culture. These had often been the preoccupation of the wealthy regions of India, such as the ganga yamuna doab, the cauvery delta and the godavari valley. What was the general pattern was the prevalence of mythologies which do not refer to others critically, much like tribal lores. Everyone knows of the existence of conflicting propositions by the others, but remain committed to a narrative. That is to say, the pluralism was qualitative, rather than orthogonal or antagonistic as could be construed in western philosophy. 
But we might say, that philosophy starts as a necessary appendage to plentifulness and different cultures or populations had experienced different levels of philosophical development at different points in history, as the circumstances permit. Thus, it might not be inaccurate to postulate that the east and west might lie in a continuum, in which in the present epoch in history west holds a superior access to philosophy. In each philosophy, there arises ways to appeal to the unknowable followed by an ontological construction which is capable of epistemologically appreciated (it is moot here if ontology preceeds epistemology, but we believe so). All civilizations had embarked on this trajectory with enough resource surplus. Hence, it might not be too much to say, that had the critical mass that triggered imperialism not reached in the west, these cycles of philosophical sophistication might proceed inedefinitely.
This created inter-tribal spaces which were not nurtered and people lived in their mythologically embedded reality. This kind of a setting relies on homogenity for stability. But as the west marched on, the clash with highly constructed civilizations, which were able to generate sustained assaults of grand scale on the pragmatic population, caused instability. Suddenly, alternate power centers arose, weaponry and business arrangements where sophisticated and this lead to a feeling of inadequacy coupled with an identity crisis in the east, both of which were capitalized by western hegemony. When the moral inadequacy of the west was slowly rising after the world war 2, the east had still not recovered from the crisis of inadequacy. The development around the objective criteria of philosophical development might be a more general scale, rather than what could be supervised by a western system already in the crisis of confidence on the strength of their philosophical frameworks, thereby cascading a slide onto an uncontrolled embrace of a chimerical deliverance where the fetters had moved to the system itself beyond the reflective abilities of humans. The state of the west as it stands might be more hollow and be entangled in the nature of things, of a massive complex of capitalism and postmodernity,that it might not be able to provide a sound advice at this point. Hence, development as a concept ought to be objectified and capable of being done so. It requires the cultivation of abilities to produce philosophies and arts and a reasonable surplus in the nation of interest.
Path Functions

On the possibility of using path functions instead of state functions. This is particularly intended for appliction to incomplete information scenarios â€“ such as medical prognosis. Where a state inspite of being designated such (say a diagnosis of tuberculosis) is qualitatively heterogeneous. It defeats the central tenet of dynamic theory that a systems present state should convey all information concerning the system. Where the past comes back to influence the state (such as with initial conditions of the host), we have a dilemma. Then the entire path matters rather than the state. We have already attempted to recursively smooth the trajectories by using multilevel markov processes. Here we approch the problem theoretically, in that where there are multiple axes, we attempt to find if a joint probability distribution could encode meaningful signals, which emerge when the distributions are joined. 
In mathematics, we are interested in linear or planar truths. Linear truths involve additivity, such as vectors along a path, coupled  with trigonmetric functions to obtain planar truths. Where products are used algebraically, it would mean an explosion of dimensions, only those quantities which could not be qualitatively reconciled would interact by mutual scaling and the result is to be seen physically to reside in multiple dimensions (such as F=ma for instance). Vectors being multiplied are rare in physical world (radius on angular momentum is one example). Such multiplication means that these vectors are acting on each other as if from different dimensions and their result lies in a new dimension. 
When there is a power function, we talk of a growing system, which quickly multiplies. The exponent is the limiting case of such multiplicative system. That is to say, if a quantile is raised to a power, it is multiplied by itself iteratively by some number which is the power. As the number grows, the rate at which it grows is a linear multiple of the exponential growth. Hence, growing functions are recusrive and the recursion irrespective of the number of times it is called, is as if 
That being said on the nature of multiplication, In probability theory, when we talk of joint probability distribution, we talk of orthogonal dimensionality of the distributions coming together. We speculate in this theoretical search that there are some slices of the distribution where the informative parts of the distribution are joined together. That is to say among normalized dimension, we might say the area about the mean is uninformative. Outliers beyond say one standard distribution, might form a shape (visualized in a polar or paralell projection), in which case, we might be able to identify certain desparate elements of a set. 
Part 2
If joint probability distributions were to show interesting values, then it ought to signify the existence of multiple unique equilibra to the system. Let us assume a system being described in a system of coordinates projected in polar projection. Among the axes which are normalized, the line joining the mid point or the mean is the natural equilibrium of the system. However there are certain haphazardly joined lines that connect outliers in each dimension in a specific pattern and these are more frequent as a group than they ought to be. These signify alternate equlibria of stability of the system. For instance if we look at the way we link tallness, poverty, alcoholism combining at extreme levels but producing moderate anamolies in the anamoly axis, and this is a stable equilibrium, we might be finding a genetic discontinuity (or a distinct subpopulation) which is highly alcohol tolerant. This examination of alternate equlibiurm which are emergent over multiple axes, help in the direction of biopsychosocial model of illness (cite The Biopsychosocial Model 25 Years Later: Principles, Practice, and Scientific Inquiry).
Thus, we might inquire on the theory of emergence of multiple stability equilibria of systems when described over multiple axes.
On the question of neoteny/geekism as Development

There are two entities appealable only â€“ null and infitinty. These signify nothin and everything. All that is inbetween are engineering actions and they sometimes approximate these ideals as such as in derviatives - dx. We might denote this as [ -inf ->0 -> inf ].
We might talk of human endavour to working in the domains of the home, idealism and materialism (these domains had been dealt with in an ancient discourse on ethics - thirukural). Home is about a caring for people, involving satisfaction and contentment. Idealism is to find solutions in the ideal plane. But in the ideal plane for each arbitrary truth a there exists a negation to a ( âˆ€Aâˆƒ!A ) . That is to say, the idealistic system is self referential. It does not apply to real world problems. It is closed on itself and does not do any real work. The infinitude and nullity of theory itself might suggest that the world itself might be devoid of any fundamentally interesting characteristics. But this is realized only in the idealistic points. In the infinite past everything was zero sum and so it is in the infitite future (Just collate interesting behaviour over large individuals and over long time and we would see randomness emerge). Likewise in the imperceptably small instant nothing exists (Plankhs constant suggests non existence of such infinite smallness) . Empirically we see for a large enough interval , we see that things do not converge towards pure randomness. Pure randomness signify zero sums closer to theory, but it sonly over large intervals. For a finite intervals, it is empiricallly such that for every A there does not exist its negation. 
Thus empiricism might be seen as a measure of heterogenity in a finite interval. Finite intervals are thus qualitatively diffferne form the null and fhe infiity. These could only be created by a product with zero or infinity (but zero isa number in algbra, sometimes seen loiclaly inconsistent). The paralell lines might be a seen as converging towards the infintidues (in lenear reasonging that is , refer cirte the mind is flat).

There ought to bhave been some initial conditoins or â€˜hand of Godâ€™ that introduced the heterogenity that makes it worthwhile to do science, while infact it deals with finite imperfect concepts.
3. Thus, materialism is not to be discredited. We only seek to propose a dualism of home and idealism with the materialism being an instrumental concept. But even such hybrid idealism had been catastropic in the past.
4. In pure materialsm many a good soul eexisted such as the nerdy game playing man child like typical fictional characters (say Sly in scorpion). These are milleniels, urbane cool people, who sun idealsim and see there is no need for moral outrage (home-people related intelligence) nor ideals (as with geometry). They see the world can take care of itself (self stabilizing) unless some one comes around talking idealistic stuff. Hence just taking baby steps (such as doing the next right thing rather than making overarching plans) is seen as sufficient. It is not discredited here too â€“ it works and if is even theoretically consistent. 
5. Hence all that is left in philosophy is to appreciate that nothing is bad except dogmas. One ought not believe that  home plus idealism is always better than materialsm. Only that materialsim ought not rise as a dogma. Here the end seems to be persistent repetitive humility. This humbling of mankind is hymns to the glory of God. Philospohy allows for this reflection and glorification and hence might be an epitome of development.
6. We  ought to honorably mention the role of pacifist millenials who by cirtue of their finitidue based approach see that all actions are simply game steps. They see that they do the next best thing not towards some vague ideal or as a way to set right some outrage (or extreme suffering) but only as a game move, in their own little world. 















The mendicant leisure class and the secret service






























a

































Emergent and Constructed Intelligence
We have earlier seen that intelligence is highly embodied. If we are to think of the various things that constitute our consciousness, we would see that apart from the rational faculties and sensorial information, there are a lot of innnate feelings, urges and barely perceptible motivations in our system. This had been alluded to by Kant in his concept of apriori knowledge and Descartes in the concept of the soul. In this case, we see that the universe might be an unfolding of history, wherein there are so interesting and random occurances as to deny the existence of free will and intelligent actions. The mind, we might say is inseparable from the body. But as we get more and more glimpses of the universe, we also see that in the interval of finitude that we have grown to be capable of observation over years of intelligent actions, we see that there is some pattern to the unfolding of history. If the universe were to be an unfolding of history, why is there patterns to it. What is more interesting is that these pattern indicate the use of repeatable processes, which evidence the use of instruments by the organizer of this universe. And what is even more interesting is that, we see imperfections to the application of these instruments. It is as if God is running a gigiantic printing press, putting together all these marvellous creations, by simple overlaid patterns of CYMK dyes and patterns and the impressions they leave, actually bleed. There is no perfection to the printing press, in the same way as it would be had it been put together humans. The orbit of mercury, the quantum states all indicate the imperfection. Likewise, our inabiliy to make sense of the world in a perfect form, also arises from the inherent inability of the world to fall into any pattern, however abstract it may be, nevertheless showing some patterns. The incompleteness of mathematics as a system was vindicative here.
But if we observe that intelligent life exhibits property of growth, of gathering up things and organizing them. We see automobiles, buildings and roads all put together by the intelligent form of life that we are. Similar efforts have been made upon nests and burrows by other creatures. Given that all intelligent action arises from an observation of what is called a phenomenon, which itself we speculate to be an intelligent action of some superior being, we might say that intelligence is nothing but a projection of the past upon the future. Thus, all intelligence arises from being able to observe the works of a great maker, who had already exhibited intelligence and project these patterns on the creation of things within our intelligence. All physical laws derive from observation or an introspection of our natural intuition (which itself might be a form of empiricism â€“ so sorry theorists). Afterall mathematics and logic rely on our ability to sense asymmetry and orderedness from an intuitive aesthetic perspective. The intelligence thus projected upon the future, allows us to rise to the level or the creator, or does it. The creator here, given that he is toiling with imperfect tools, but with admirable intelligence, with no apparent purpose (other than seeking fulfillment of some visceral motive, planted in him by some superior Creator) might then, perhaps be assigned a name of an angel. Likewise, there might be an infinite stacking of angels who seek out to a mighty Creator, who defies all understanding and references, other than perhaps an appeal through symbols (as with infinity). One might look at the story â€“ â€œI don't know, Timmy, being God is a big responsibilityâ€ published in Bostromâ€™s website. 
This being so, it puts us humans with the possibility of actually using intelligence to create intelligent life, for intelligent might be recursive and arises from application of imperfect instruments to perfect concepts on random subjects. This, however again begs the question of why, as to why would humans consider elevation to angels, a dramatic goal of development. But the angels do not seem to be greatly served for all their exertions, the beings are hardly conscious of them, let alone exult them. Perhaps they do it by virtue of a deep phenomenon that pervades the universe, perhaps the Maker had set in motion such a thing, in his whim, we would never know. But what we might be witnessing in artificial intelligence, is an irrational drive to realize the maximizaiton of our intelligence, which would then be the emulation of the intelligence of the superior angelic being. 
This point of view also contradicts the emergent theory of intelligence and of the mind. There might actually be a intelligent exertion in putting together finite components and the complex phenomenon might however emerge therefrom. We might even surmise that life is not a miracle as much as we give credit to it. If one is to dance in a hall and sees guests coming in. One might convince and pride himself as being the spirit of the party. But sooner than later, one would be likely discover someone else taking over. The delusion of being singular and unique is shattered much too often in the world. There is only left a particular quircky randomness to claim ones identity for.Otherwise, it is just a matter of observing something before being convinced of the mediocrity of oneself. Hence, it might be that some universal spirit would start animating things, once we put together a certain intelligent construct. Likewise, as much as human embodied intelligent is continuum from the past, it couldbe projected  upon the future as well, in which case we would be arguing for the case where the soul is inseperable from God and God had in fact projected it upon us, to carry things forward, in the principles of advaitha. 
PS
But it is also not possible to dismiss the emergent nature of intelligence. There does exist a continuum from unicellularity to multicellularity, to social intelligence (say amoeba, portuguese man o war, fishes, mammals). We might look up the work of Extavour to characterize the persistence of game dynamics even in tightly integrated multicellular creatures, thereby creating intelligence as an emergent function of simpler machines. (cite Extavour). There might be premature experiments in eusociality in insects, but which had since been superseded by behavioural approaches such as nesting and rearing young. But it might as well be that it is not the only route to intelligence. In that intelligent construction of intelligent machines might be plausible, we ought also to realize, that it is the imperfections of the construct that causes it to become intelligent and endeavour to perfection (or completeness or more emotionally fulfillment). Thus intelligent might be the reside of imperfection, that arises like a ghost to attempt to perfect the setting. No, wonder Wiener (cite) was postulating intelligence as a negative feedback. 
Despite, multiple theoretic views of intelligence, the question remains if we could consciously construct intelligent creations, given that there is evidence of such happenings in the past, by angelic actors (in the physical plane). Whereas biologically, there is suggestion only of emergent intelligence. We might see the whole as a way the imperfections or chaos in the initial configuration of the physical universe combining to create an orderliness that it had been missing, to produce what might be a simulation within it of more perfect intellgence. 
Thus we might clearly distinguish between physical and biological intelligence. We might say that biological intelligence is a feedback loop that arise from the imperfection of physical intelligence. This control loop is probably not orchestrated, but is a part of a self stabilizing system. It might just be â€˜keeping up with the timeâ€™ concept of stability, a dynamic stability. Thus, at a dramatic stroke of a developing feedback mechanism, it might happen, that humans might be able to engineer something that is as big as a solar system or a galaxy. Here they might alter the physical universe to the ends of greater orderliness (thereby stabilizing the initial project). It might not have biological connotations at all, it might in fact break away from the concept of life itself. The mind might at last be liberated and it might actually work on problems as big as a galaxy. But if we question the human endeavour to introspect this, and label this process, one sees something of a contradiction. It shows an awareness to seek the bodily root of this process, the question of why after all, we do this. At some point, humans might succumb to the constrains of their emotions and never attempt the liberation and it might be a limiting case of the feedback, of stability itself. Or perhaps it might involve an altered universe, towards greater stability, with fundamental designs having been altered in which the feedback might start in genesis. This afterall points to an anthropic view, where the fundamental dilemma of the universe echo in our mind. Hence, all that remains is a dilemma and we have satisfied ourselves dragging ourselves to the edge of the cliff to take a peer and Camus was wise.






The rise of nerd clubs

We have seen that there is presented an eternal dilemma at the end of this search. For a dilemma, the solution lies not in static or final decision, but in the nature of context sensitive decisions, which actually serve in a manner that is like righting the boat. That is to say, given the dilemma between choosing a biophilia (cite) based intelligence and an eternal and more pure intelligence, the choice is to be attempted contextually and given our natural tendency for symmetry (with  minor imperfections) we seek to balance the situation where one side of the dilemma is systematically more supported for, in an arbitrary manner. Thus, it is for the preservation of the dilemma that we work, atleast in the public domain. In fact the symmetry might itself be inherited from the natural world, where a great deal of symmetry exists except for a few (electric unipoles, entropy, the problem alluded in subatomic physics). 
We believe that it is important to preserve our choice in the presentation of systematic progression in a particular arbitrary direction, that goes against the human attunement to higher order intelligence. We have repeatedly alluded to the case where we might have passed by some golden age in the mid of the past century (in our older books like Computerized Societies). There used to be problems in nature, which was restraining of humanity, which lacked the appeal for the aesthetic and hence was the subject of much engineering approach, rather than evolutionary or biological approach to intelligence sometime since the enlightenment. But it would be a mistake to think that intelect had developed in the recent past of mankind. A tribesman say 3000 years ago or the ancient greeks have substantially the same level of intellectual ability as modern people. They lacked the agumentation of modern instruments and the construction of the theories which we have access to now. Thus, when faced with the problems in nature, humans attempted to build reservoirs and ploughs to make it better for mankind. The dynamics of the struggle with nature destabilized the human society caused it to oscillate wildly as it sought new equilibria. Settled life created opportunties for raiders and exploration created opportunity for crafty businessmen. Thus, the pursuit of truth, the singular source of absolute goodness had prevailed in history since time immemorial. The various local customs and mythologies are appeal to this universal truth rather than local truths. The speculation of the existence of the truth and not many truths has been well documented in greek philosophy and one can see its asian counterparts in the ancient traditions of Buddhism and in the philosophy of advaitha. This pursuit of universal truths is an attempt to incorporate the intelligence of the angels in human action and thus redeem themselves from the human condition of embodiment. This would hence need a decoupling of wealth and rewards from the pursuit of objective knowledge, as in the traditional academic campus. But the pursuit of intellectual awakening still has to reconcile with the plurality of approaches, since even the angels seem to be imperfectly grasping the truths. Hence the academic pursuit of truths have been dogmatic and had always placed in high regard the intractability of the absolute truths to which only an appeal could be made. This is reflected in the spirit of romanticism, ritualism and celebration in academic campus along with the principles of decoupling from rewards and material preoccupations. 
Thus pure theoretical pursuit and romantic inclination are an unlikely combination for those that pursue intellectual pure truth (and not merely biological truths). Thus, the dichotomy of biological dynamic truths might be fundamentally different from the mechanical truths. The former is pursued by the computers (trivially established of their dynamism in their capacity of being represented by lambda recursion). The latter is more mechanistic and involves development about axioms (such as the angels use with constants like exponent, pi etc). The dilemma of these truths is pervasive, say when Camus said that suicide is the only philosophical problem, because it suggests that the essence preceeds existence. Kant was suggesting that theoretical constructs are closed on themselves and they derive from  a small core of apriori truths projected about a null space in all directions. The development of such theoretical pure concepts, is like a perfect setsquare. Only with the instrument of theory could we percieve phenomena or interesting heterogenity in finite intervals. The persistence of this dilemma implies that we ought to take a stance where we either follow the current or attempt to appeal to some higher order aesthetic balance and it might be a very important decision that we make.
This adherence to intellectual truths that is detatched from the embodied intelligence would anyway involve a satisfaction of the body, but the question of which preceeds which remains. In one side, we may think of the pursuit of objective knowledge is an extension of our embodied intelligence and in another we may see that the body merely anchors us to a historic past, now rendered obsolete, in that we have our eyes  opened to a greater and grander intelligence. The latter is the school of posthumanism, where there is a drive to detatch humans from their bodily existence and emulate them in what is called the cyberspace. But this would mean a conclusion of the dilemma rather than preservation of it. If the dilemma is to be preserved in a given context and point in history, the subscription to the universal higher order truth would require the absence of dogmatism. This is a subtle concept, in that the proponent is prevented from appealing to higher order truths explicitly. He would then have to appeal to higher order truths with humility and ought not attempt to find fault with and undo history. He would then be left with a position where he does appeal to pure solutions, even in the face of extremely tough problems, such as say terraforming planets, rather than extending his biological capacities by dynamic devices. This would be an appeal to intellectual fulfillment, like some game, rather than material enrichment of a hedonic nature. The proponents would most likely then be virtually angelic in that they attempt to enrich in accordance with their telos (as Aristotle said) and pursue the game of finding intellectual solutions to every complex problems. They would also engage in differing in their ideas due to their recognition of imperfection inherent to such pursuit, where they appeal to nature by contemplation rather than analysis. This would generate a conflict, but this conflict would be pure in that there is a sportiveness and a principle of the better man wins, in proving certain points by ardurous experiment or by theoretical exercise. It may even extend to duels and battles to prove points, but it would be governed by the code of chivalry and the constructivism of it, rather than being driven by the phenotypes. Thus it is neither an idealists den, nor some frolicky escapade of nerds. It is rather like ancient orders of knights, pursuing pure principles (not just restricted to local kings, mythologies and national interests) but universal truths, to which these local concepts appeal to. It would also involve development of rituals and local mythologies which appeal to these concepts and the preservation of the spirit of art and contemplation. The key is remember the telos and not be swayed by temptation.
In the past such idealist conceptions like the leisure clubs of England were the origin of exploration and a pursuit of nationalist glory. The glory in a pure romantic conception was an appeal to the glory of God. But it is often made crude by appealing them to material and reward based concepts like race, locality, a king or a mythology, in such a manner that it would lend itself to authority over people, rather than knowledge. The pursuit of such knowledge might result in the ability to control natural elements in its stable state, so that people might form into natural heirarchies, in a manner as how cooperation emerges and stabilizes the population (literally in our times, the population count had stabilized, by emergent equality, people no longer care about having many offsprings). The solution to natural problems result in emergence of authority and control in the society. But the pendulum had swinged away from the sweet spot, with the rise of postmodernity. That is to say, the development of ubiquituous computation, results in dynamic development much like evolutionary development, thereby by the entities of markets, of enforced normality, result in control by self implicated means, much like the natural forces. 
Hence, the development of a social setting, where the problems at hand, is handled by means of intellectual pursuits, by experimentation and adversarial search, by romantic contemplation and by suboptimally using the resources (in an epicurean fashion, in such a way as represented by the campus, with extended resources not being harvested at haste) is a good way under the circumstances to appeal to the universal truth, again as a civil endeavour, among people. It would thus, extend the individual concepts of pain and pleasure and mitigate it by dispelling loneliness, by mitigating solutions to natural problems, by social absorption of the outputs as well as allowing for joint puruit of philosophical consolation. Without a social setting, humans find themselves unable to account for the imperfection of nature. Hence, it would be a legitimate attempt to create such social groups as the contextual missing piece. Loneliness also arises from the nature of finite life. The information in genetic nature is conveyed by other species, but the critical realizaiton (perhaps as a critical mass phenomenon, of the awareness of non bioloigcal order, or a revelation perhaps) could only be sustained by civilization.
The primary intellectual activity of such campus groups would likely be ones of control and the pursuit of theory of control. That is to say, as much as the social group attempts to disrupt the pursuit of market based intelligence, the discrediting of apriori goals and feedfoward controls even in the most conservative institutions (let along corporations), would involve development of a sound theory of control. This theory which when applied to individual situations, would reveal their phenomenological disposition and thus, capable of being neutralized in the sense, that the heterogenity in the finite interval is absorbed in a theoretical frame of conservative neutralization, which is the essence of control, to be able to theoretically reason over things and thus be able to bring about a neutralization at a desired speed. Thus, control would mean from a political standpoint to identify the diverse factionism, which essentially mena nothing theoretically and thus, applying suitable incentives and sanctions to bring stability. Likewise in natural systems such stability could be achieved by controlling chaotic swings and interactions. The end of control is to build platforms for further engineering development by exploration.

Technicalities of Control

Control is a way of neutralizing in some domain where control is no feasible. Thus, it would require the existence of a domain which could not be absolutely controlled and hence would need an instrument to control it. That is to say, if we have it that wind currents could not be controlled, we can control the evacuation of power supply from windmill networks and schedule maintenance of these mills. This could be attained only if the windmills could be capable of quantification. That is to say, if we have a theoretical framework, about a null space, then the windmills could be quantified in terms of distance from the null space. This could hence be capable of being stated as equations. It also allows construction of closed systems, where the sum is zero, while different parts could be brought together. But as said, the motive of control is being able to neutralize energy volatility, much like converting energy is the point of a machine. The control system does not do work, it merely utilizes the volatile source of energy, appropriate portions of it as reserves and uses it to neutralize the volatile system. For this it requires a medium of disciplined system which can identify and actuate actions based on the control policy. That is to say, there is a need for a definite phenomenological entity on which stabilization is normatively preferred, say A. The second thing is a mechanistic theory on the definitions of the parts of a control system, which can convert the signals from A into quantitative entities, a system B. This control system would be able to do work on A by logically mapping the signals from the system by quantitative mapping a phenomenon a1 as a scalar variant of a counter phenomenon b1 and if b1 could be imposed by B upon A, then the system might be expected to be stabilized. That would require the identification of a mapping between points a1 and b1, which would presume that a model of the system being controlled exists.
Thus, a linear model might be constructed of the system A as system B and it could be analyzed with linear techniques to attain stability. Where the system A is difficult of being modelled linearly, due to its dynamic nature (in that it evolves to the control signals) or because only incomplete information is available about it, we require more advanced mathematics. A system of which some information is known via a set of equations, might be of any shape subject to the well known constraints, in which case we can work to maximize or minimize it to an objective function. 
In case, where the system is described as a logical tree (as a partially ordered set) which has fields that could be mapped to various quantiles which are strictly ordered, then the system could be analyzed using computational techniques. The system is computable only if it is a tree, capable of being recursively constructed by a lambda, which would be able to populate the leaves using quantiles from a strictly ordered set. Normally functions are said to be computable. A function is actually a blackbox system with a function, which reserves the right to conceal its structure. If a system is presented for inspection, if there are points capable of determination in the system and these points are connected by means of paths which are constrained by rules then the complexity is less. If there is less known about possible evolution of the system, where the points are capable of development in time along any possible pathways, then the systems complexity increases. That is to say, complexity of the system arises due to its dynamic behaviour, or randomness in adapting states. The randomness itself due to paralellism, or inability to locate a single algorithm that works the prime mover of the system. The complexity increases exponentially, where there are multiple algorithms working together (or does it, consider additivity of waveforms). 

On Why computers might get banned.
So far our critique of computing rested on metaphysical grounds. We seek to make it more mathematical by this exercise. 
The Turing machine model of the computer is capable of searching trees, which is a very powerful feature. But is logically flawed, in the sense that trees are only partially ordered and the question as to why one would want ot seek qualitative and quantiative search targets mixed up is lingering. It seems like an arbitrary action. It could also be seen as an instrument which can introspect and control its own state by recursive reference outside of its immediate cursor. No machine, physical or abstract could do it. But it is in practice an excellent control model. If we take Ola for example, it dynamically stabilizes the system that is normally tumultous between cabs and passengers. It would not be inaccurate to predict that it has a feature for searching trees. It first looks for cabs in the main roads before searching the side streets. This is relevant and works only because drivers choose to park the vehicles on the main roads while waiting for pickups, because they foresee it to be  a better way to get pickups. Thus, we see that there is a lot of reflexivity in the control system.
The computing power is able to emulate human behavoiur as in the behaviour of a recursively goal directed agent. This kind of machine (animals) which are reursive goal oriented arose as a consequence  of application of pure concepts to the physical world by the agels. The resultant turbulence due to imperfections gave rise to the need for dampeners or stabilizers, which worked on th ebasis of recursive goal directedness, which makes a very good loop, while being self stabilizing within itself. Hence, the control problem involving two systems A and B as aforesaid was working well because B is able to self stabilize and could hence form a good control system over A without high computational costs.
We are also posed with the question as to whether mathematics is a complete system enough to critique the logical flaws of computing. Computing emulating human behaviour is classified as biological intelligence, while mathematics is classified as pure intellect. Mathematics in fact has more inconsistencies and arbitrariness to its techniques, such as the infinitisemal, asymptotes or even an embrace of contraditions in the dirac function. But we think these are techniques which exist to quickly make us see the point, while there does exist more provable methods to reach the same ends. Hence mathematics might be a consistent system, but an incomplete one to some extent. But it ofcourse follows that it is a matter of degree in the inconsistencies between mathematics and computing. The selection of certain degree as being good threshold is arbitrary. But that is the essence of the dilemma. There does exist an arbitrary cutoff where the imperfections of a higher intelligence ceases to be the residue and becomes the driving force of a biological feedback loop. 
Thus for a ny ssytem B in order to stabilize a system A, when it comes to reflexivity (contextual choice of techniques), there exists a threshold beyond which A and B coevolve and this arbitrary point seperates pure machines from dynamical machines. The culmination of the feedback is to be able to alter the pure machines in fundamental ways (such as what humans endeavour to do â€“ to be able to engineer something within higher physical laws that could somehow make things perfect at a stellar or galactic level). Thus, the question is that whether the dilemma exists as to whether to prefer a choice to culminate the feedback or carry it forwards by engineering biological systems, which present a case for transhumanism. That is to say, transhumanism thinks that there is a need for further engineering biological systems, which would in fact rise as the progenitors of the evolutionary feedback, to the extinction of the existing human race (or deprecation), before the intellectual attempt is made. The transhumanist philosophy thinks that with the existing embodiment the endeavour is unachievable, eventhough they agree on the goal. The continuity of consciousnes between the present form and the future one is highly debated however.
The continuing of evolutionary pathway, might lead to instability is what is postulated here. It might lead to a highly divergent equilibrium (refer our old take on why the singularity would split â€“ cite). In the savannah phenomenon or more generally in evolution, there had been repeated empirical evidence to the effect that defection arises whenever a system is highly stabilized in order to carry out adversarial game like stabilization at a higher level. Thus, the solution to stability might be construction of imaginary games which could simulate higher order conflict rather than allow them to unconsciously enact them.
That is to say, if there is a need to address the mind body problem, there is needed of demonstration of a will to enact the adverserial search as an important stabilizer by being able to retain control, as one controls an experiment. It is important to be able to address the stability of the biological feedback, which always attemtps to get ahead, before attempting intellectual exploration. Therefore, it seems that there is a need to attain an universal consiousness of this cause, either to be enacted as state policies or popular cultural influences so as to attain the end of holding control, while also being to enact plural experiments. This brings us to the case of the Nerds.
Part 3
In our much earlier work on Computerized societies, we addressed that there might exist distinct populations which confront perpetually. We had discussed one of the population to be the â€˜common manâ€™. Now in the light of more information and deeper metaphysics, we might develop this system to a greater accuracy. The common man might have been the nerd, the geek who is romantically disposed and at the same time unwilling to accept unexplained development. He was said to be civil and clear and straightforward in his conduct. He exhibits an apriori allegience to morality and only participates in struggles of power, where the rules are well set and the criteria for success and its measurement well appointed. He might chivalrously come back to grant the failed one a chance in a next battle. We have also talked of a more primitive people, who had acquired power and determine it to be an extension of brutish strength which had accrued to them by genetic dispositions, which might itself have been powered by the flow of history. This subscription of the randomness of power and advantage, allows them to claim legitimacy to any act that as far as the ends are clear. They do not distinguish between well appointed battles and backstabbing. The nerd subscribes to the idea of seperating intellectual advantage from physical or skill based ones. They  thik that it is civil process to decide on the games first and them step into them. The brute thinks that intelligence is an extension of embodied power and hence, they get out into the fight on sight. In this situation, the position of the queue cutter is most interesting. He seems to thrive on the ambiguity of these labels. They so much are pragmatic that they assign themselves the label (as a queue abider or a brute) according to the situation. This refutation of idealism and in fact considering it as a springboard for brute force is what makes the situation peculiar. The brute does not want games to begin with and the destruction of the dialectic, is what presents as an important problem of control. 
Hence, there came to be designed in history, a multilevel and distributed system which relied on checks and balances and heirarchies to retain control. That is to say, as the nerd and the brute could not agree on the dilemma of mind body problem, there arose a new form of brute who could take advantage of the engagement in dilemma and use it as a means to achieve his goals. This could only be stopped, if the nerd and the brute while they were fighting, could assign a part of their allies to carryforward the dialectic and dilemma to higher resource bases, defining authority about papers and intellectual systems to constrain the brute from out of bounds actions, while allowing him to assert superiority within game boundaries. This balance was however constantly endangered by the questioners of the essence of the dialectic itself and this leads to an inelegant, adhoc system of control that is evolutionary and dynamic. 
At some arbitrary point, we said that this kind of pragmtatic brute, who does not recognize control itself grew so numerous that they started to become capable of institutionalization of their rejection of well appointed games themselves. The accompanying pain and bloodshed was avoided, because the softbrute, as we call this category was able to port the struggle, to a different environment. The point to recount is that both the brute and the nerd had morality. They were contending upon whether a game is necessary or not, the brute withheld attack till such question is settled. They were contesting the merit of their apriori truths, one of brute force and one of interpersing struggles with rest and retirement. The soft brute has no apriori truths and relies entirely on feedback to guide his actions. This it was predicted might give rise to a complex, wherein the softbrute is strengthened as a synthesis of the dialectic between the brute and the nerd. The softbrute carries the struggle to a different extent, to the point where both the nerd and the brute run into identity trouble, due to inability to define each other by contradiction. The softbrute however, as he ports the struggle to machines which are recursively goal directed, in terms of preservation of self and growth, gives away little by way of control. The software system that could navigate trees and act upon events, could not be proven, only be tested. Hence, entities that hold them in ownership are as much controlled by them as they control them. The new model becomes a fine grained network of highly reactive agents. This web of stability it is said offers to stabilize the dialectic of the brutes and the nerds. 
The outcome of the complex was offered to be that there would arise a mainstream, which is so out of control and evolutionary, that there is promoted an interest that does not arise from a will of people. This propogation is predicted to lead into a control system that is so effective, that it is able to define goals within itself, or in a closed manner. The system B is able to stabilize A because the goals exist within B itself and has nothing in terms of  an agent C who expects the stability of A as a way to produce surplus for his domain. (The agent C is humanity). That is to say, the question of control of A becomes so well formed, or well posed that there is no dilemma in it at all and it could be settled within B as a closed problem. This is only possible if B is able to set up a suitable adverserial determination platform as to determine the dilemma at higher frequencies than A. But it is doubtful if C would be able to interpret the dilemmas of B that is expected to overflow it at some point (the self referential truths within B being as much synthetic as it would have been in A). This is because B might not have an access to moral codes that involve the apriori concepts like pain and happiness that form the basis of the dilemma for arbitration by C. There is no guarantee that there would be a simple correspondence between the two. Likewise, there is no proof that more elaborate games and plurality might not capable of being orchestrated within A itself by C, so as to allow for B to sustain as a governing system that could continue to serve C. 
This would mean, that there is required of more zero sum games within A, that are actually nearly zero sum and not purely so. This conflict could be produced only by setting up countercultures or nerd clubs is the argument advanced in the work (computerized societies cite). The setting up of this plurality and spirit of challenge, is likely to generate enough plurality in the system as to permit derivation of pure completely objective pursuers of truths, in the form of a campus. They might be insulated from the concerns of biological survival, by the chivalrous proxy wars or games, with very small rewards in the open system sense. The development of such mythologies might be plural and capable of being constructed recursively so as to be capable of producing a self stabilizing system that is denoted by coarser grained pursuit of truths. Thus, it is presented that the construction of ever increasing recursive orders by conscious effort would allow stabilizing of a high powered but finite force of evolution and allow a progress that is defined by intellectual attainment that is disembodied, but still rooted in the embodiment of the human being.
Before making the case for the counterculture, the work visited the notions of philosophies of the east and west, each making a case of pessimism and optimism over the human condition. Each presented pathways of redemption through surrender to God and by sheer will respectively. This is presented as an example of a dilemma which is constructed in accordance to the specific circumstances presented in history. This dilemma it is presented to be capable of being trivially resolved in the human psyche owing to the presence of the idea of love, of sacrifice and morality, all of which might embrace contradictions while remaining consistent. This allows for each population to make contradictory cases while meaning the same thing. This same thing is presented as the basis of the struggle of the nerd.
The nerd is hence presented with the point that he could attempt to redeem the world, to keep the idea of consciousness and struggle intact, by rising by peaceful means, rather than letting the brute do it, or worse to simply let the softbrute walk over,as is shown of the possibility in postmodernity. Thus, the idea of a counterculture is presented in good detail. The idea of an inherent loss of boundaries owing to entropy as this dilemma is being treated in the counter culture is addressed as well as the problems of mischief and conflict. This arises from not being able to do disservice to the embodied intelligence. It is again argued that the problem is technical and though the force of evolution is great, it is finite (as is evidenced by the stabilization of populations). It is argued with two technical solutions, one dealing with the use of computing systems as record keepers alone, as automatons and not turing machines (much like cobol which does not allow an unbounded while loop). The other one was with regard to money and the localization thereof, given the ability of the indpendent areas to remain stable in their own way, through international trade over multiple currencies. This might allow for local stabilization, which might propogate as global stabilization. 
Thus, it had advocated the localizaiton of stabilization programs and their presentation of ever fewer problems for higher levels (till at the topmost level, say the nation state or what is seen as the modern state, the mandates are very few and intuitively self explanatory). This it is discussed allows for a tight seal on evolution, while humanity might work to exercise their approach to liberation. The work is however very cautious on this technical fix as being one among the many possible and stressing that it is the effort to make the pitch on behalf of an intellectual and countercultural discussion that counts, rather than rushing with final solutions.

Redemption by Pop Culture and Words of Identity

In those cultures where there exist a variety of defintions of phenomena, in which case, there might exist words for specific attributes of a person, it propounds a sense of cultural recognition of what might have been alienating and different ordinarily. This had been the case of the nerd or the conservative or the liberal democrat. The political, sports, intellectual beliefs, economic beliefs, religious beliefs all impact the cultural setting giving rise to a variety of identities, which might have been difficult of comprehension within ones own culture. Thus, pop culture had been liberating form many persecuted and sidelined within their own cultures. It might also have been not even a persecution, but a vague sense of exclusion from the norms. The assignment of such terms as geeks in pouplar culture might have been a balm to the pain of exclusion in ones village. Thus, pop cultures and cities had been drawing many oddballs and they had been successful in settling down more at ease in the new and more integrated environment. However, this also lends them to come under the engines of integarative, neoliberal economics strongly, allowing for the branding and packaging of cultural elements. Hence, the cities as much as they are cultural centers, might be detatched from being economic centers too. For, otherwise, it would be difficult to sustain an open cultural posture. The intense conflict between the efficiency and context sensitivity of survival is what had prompted persecution in the villages in the first place. Porting a superficially acceptible and almost flag grouping in the city, while also defining them to be demographic bases for marketing, makes things sticky. It might end up with generating a personal sense of exclusion that well veneered thereby making protest and expression difficult. Hence, we had argued that pop culture while being liberating ought to be at the hub of more primitive villages which might afford the hubs through support by real production. Even the modern city is run by villages as hubs for services that are not strictly extractive or generative, but merely deliberative. The explicit declaration of the fact that cities are only cultural melting pots, like some fair grounds in the ancient worlds might help in stabilizing the villages without creating self referential problems within the auspices of the city gate, leaving little to the village (now system A) to generate dilemmas to its residents or appointed lords (the system C). 
The development of pop media had been an outcome of the pop culture, allowing the development of ideas that seems liberating, while itself the subject of signalling, nudging and thus choice engineering to the ends of some businessmen, themselves held captive in the swirl of the finance markets and state regulations and lobbying. This pop media while being great in their outreach due to fast and cheap communication through international press and all the way to satellite relays and the internet, however was thus growing more plasticy and superficial as being orchestrated by overlords who do not know what they are looking for. The age of TRP and feedback set things to greater disillusionment. But pop culture in itself was strongly liberating and ought to be upkept, from the hinterlands, rather than being the center of action themselves. This would mean that popular cultures developed in melting pots like say new york city, ought to be financed from the artistic supporters from distant lands rather than being administered by an industry in intself looking out to charter advertisers who would use the culture to send subtle signals of control. What might have been the platform of liberation is inadvertantly turned onto a platform of control, one that is controlled by the emergent structures of the human nature itself, rather than some individual. Naturally, outcasts who are not convinced of their inferior skillsets are the ones who emigrate from villages (or less culturally open nations) to greener pastures. Hence, while  being culturally vibrant, they are also intellectually more coherent and present a good counterweight to the conservative way of operation. The villages might consult them over difficult natural problems as well as mitigating conflict. The result might be a development in a manner, in which the village might be proud as well, in being able to reflect better on its future course, or to present its conservative credentials without the need to oppress (letting people choose between the conservatism and a liberal city or even western migratory plan).
A strong village culture, which 











a
