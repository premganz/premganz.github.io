Curiosity: A theory of the intelligent organization
Metaphysics of a new method to science augmented with techniques of dissipation and control
Our (Mostly) Simple universe
A discourse on Critical Realism as a Philosophy of Science and its application to Automato Theory 

Introduction

Introduction 3
After much soul searching, it is found that I have independently developed a philosophy of science, much popular in the the scene in the eighties and the nineties. This is the critical realistic school of scientific philosophy. Karl Popper was an important proponent of this view point of science, but it took full shape in the works of Roy Bhaskar. The emancipatory nature of science and its essential embodiment in the social life of humans that is dualistically split between the search of objective truths and authentic art, as discussed in our theory has much in commmon with Roy Bhaskar’s treatises. From defining our philosophical leanings and moving therefrom this work becomes a work on unembodied intelligence or automata. In our discussion with embodiment and the nature of intelligence as trancedental and at other places we seem to have definite dicusssion on the bodily nature of intelligent decisions. It becomes therefore an intricate critical work considering both stand points logically and philosophically rather than a monotonous exposition. 

From an applied stance, our work would throw itself to critically examine the modern probabilistic network conception of artificial intelligence or automata from a stand point of the definition of what a scientific truth is. This is intended to dispell the general notion of artificial intelligence being a science. We have even attempted to slap the label of pseudoscience to it. But on both counts, we are still not fully successful but we see some interesting discourse to it. Secondly, if we are to ‘name’ artificial intelligence as pseudoscience and perhaps excite policy and culural respose accordingly, it might still be existent as good as it ever was. Therefore, it becomes important to develop the science, particularly the theory of automata to be able to describe it as a phenomenon. These two become the core objectives of this book.

War and Peace
Our idea of peace seems to be distinct from the postmodernist idea of peace. In postmodernism, peace seems to be a central piece and it is a mainstream adaptation of a subculture around peace, brotherhood and generally ‘chilling’ it out. We see that this tendency to seek peace as something that is already there and would only need getting out of the uptight mindset is what we see to be problematic, iconized in the postmodernist symbol of peace. The idea of peace, in a dualist sense is more of the outcome of a work, it is not the default state of affairs. One would need to be vigilant, single out the evil and vanquish it. One needs to work hard at the forge, amid the heat to make from vulgar ore, fine useful tools. This piece of work is what brings out the peace in a spiritual and domestic sense. One could accomplish peace only by setting out in a war against injustice. Hence, being uptight, even though old fashioned might allow for a better state of mind. The universalization and trivialization of peace is dangerous and capable of cultivating a mindset of the joker, whose tagline is why so serious.
On the other hand, the crusader mindset gone wrong is also visible all around the world. But it is questionable if we would want is a struggle at the metaphysical level, among crusaders and the chillers. It would be better that the fight be done at the level of justice rather than empty philosophy. It is important that one does not attempt to resolve philosophical problems materially, it is bad and wasteful. Good philosophy tries to keep itself to the realm of imagination and never finding use. But one should not consider the philosophers efforts to be futile. Only on meditating on already solved problems for profound insights  can one build upon the system of knowledge. Thus, upon seeing the straight line movement of light, one might philosophize in a seemingly useless way and create a system of mechanics which could help guide satellites into space. Philosophy provides the basis on which extensions could be made and arguments could be held over physical realizations. For instance, even the everyday currency note, is an instrument that points to the abstract entity of the money, as an instrument of the nature of promissory note. Money itself is immaterial and purely theoretical. The proper construction of things based on such theory, rather than flowing simply from usage, would help a system to be controlled via appeals to the intellect. 
We need to recognize the fact that in many theologies, the notion of such dualism of good versus evil is not strongly supported. Christianity speaks of forgiving people for their sins. It is attributed to Jesus that he appeaed as – forgive them father, they know not what they do. In multiple places, it could be seen that hate ought not be directed at people, because people are intrinsically good and sins arise or evil arise from people being taken possession by a complex, a system on which they have no control. It might be a well formed idea or it might be a ‘condition’. This notion of evil, promotes peace and emphasizes love. The problem is seen as distinct from the agency, as a system in itself, which needs to be destroyed. Thus, slavery might have been a problem, arising from vague notions and finding itself into a system of otherwise decent people. On the other hand, it might be that some idea has been promoted by the authority of an autocrat, such as the holocaust and it is sustained by helpless people but the initial push of the power makes the origin of the complex traceable. These complexes could be diffused by an appeal to reason the individuals who back it by physical force. This had been the method of Gandhi. Thus, one might say that love is sufficient to realize peace, but it might not be enough is what is proposed by us. There are in fact conditions which would need to be diffused by intellectual exertion. Thus, it might be a materialist conception of the world, where work needs to be done in order to make way for peace. But it is not synthetic or adverse to the soul, it is infact a natural endowment of the soul to reason out and solve problems, as Descartes claimed. 
Therefore, we attempt here to frame the need for an expert effort, an apriori model of control so as to invest in the effort to single out evil, as in conditions and not in people and use the work so invested to diffuse conditions and elevate the soul. If humans are to rely entirely on aposterori control, loving and desiring peace, ultimately only to find that they do need some apriori controls and such controls emerging without their will, would be self defeating. The universe has a natural balance to apriori and aposteriori control and it shall be our endeavour to tune well to the universal balance and harmonize with it, in order to survive in our present form. Hence, problem solving becomes an imperative exercise. It would need the identification, formulation and neutralizaiton of problems as an intellectual endeavour before these problems present themselves materially. This might be a general solution to the problem, which had in fact presented itself materially in a small sampled way. Thus, abstract problem solving is what we advocate here. It should also be noted that problem solving is not simply goal directed materially, but redemptive. It ought to solve the problems in such a way that post the problem, the equilibrium is conserved. It is not to solve the problem of insect bites by burning down the forest, because that would dispose the system to an unstable state and result in further developing disposing us to think dynamically in an unframed manner. We ought to attempt to conserve the equilibrium of the system post the operation, which is the essence of scientifically consistent problem solving. The problem solving ought to deal with closed systems (even if it means to use dynamic techniques in the bounded context) and come up with methods of neutralizaiton . One might take an instance like say, ordering blocks of stones to build a wall – more or less the stone pile rearrangement leaves the system in equilibrium and it is a local dynamic problem, of type say, subset sum problem. This kind of problem solving using dynamic methods, yet capable of being theorized in a conservational schema is what would lead to an intellectual work that would likely create peace.
As we had seen, even conditions are those which become problems only where they are known. Regarding the uknowable, nothing can be done. If we move about and wreak havoc on the microbial population about, we are in fact wronging them and being unjust. But it is something that could not be helped. Some cosmologies like Jainism attempt to avoid this as well. But if something is unknowable, then humans should be open to unknown incursions into their harmony as well. The randomness of problems would then be outside of the domain of the sciences. It might however be dealt with in the domain of culture and spiritualism. 
At a tactical level, we are posed with the question as to the dealing of problems where the condition is backed by violence. Then it might be that the scientific, intellectual approach to problem solving ought to resist it by force (even Gandhi preferred violence over cowardice). Therefore, tactically we might end up with well schemed battles, laid out fair and without collateral damage to uphold the notions of there being an idea before the material. This might be the core of the idealist philosophy, that there being an ability for humans to discern truth from lie, good from bad, expressed instrumentally in rationalism but embedded in their apriori notions. The methods of science embody the instrumentality of reason, but it seeks completion in the ability to appeal to larger aesthetics and non epistemologies faculties of humans. Hence science is elegant in that it acknowledges its incompleteness, specifies and limits its methods, relies on closure attempting to leave the system in the best possible equilibrium post a fix. Science is good in that it acknowledges its instrumental value and does not seek anymore further. 
Our philosophy of dualism cosmologically and theologically boils down to the seperation of the soul and God, enshrined in the dwaitha philosophy of India. That is to say, if we should claim that there is a certain condition which is distinct from the ideal and the need for ‘work’ on the part of the people themselves to redeem themselves and reach for the highest level of good, we are in fact asserting the existance of the human condition as distinct from Godly condition and that there is needed of work and distinction between things that lack an intellectual element of control as being bad, without having to hate the proponents thereof. We might even denote the dualism as having only instrumental value and see that monism has a critical value in promoting the combined instrumentality of philosophy in attaining an abstract notion of good, which could not be epistemologically pin pointed, but in fact good particular instances of it could be made, such as being harmonious with the microbial environment, having only chivalrous conflicts, having a seperation of time between work and rest among others. Our past work on peace, love and computers greatly deals with this instrumental philosophy.
From a technical angle, we seek to strengthen sciences, which we believe could not be rebooted by the cultural stance alone. There is a need to extend it methodologically. We examine the treatment of problems as dynamic conditions and the need to control them as being posthoc to goal specification as the primary boundary to dynamic negotiation in order to generate static facts as a complementing factor to the traditional method of science. This is discussed in first order cybernetics extensively. We sample some studies here and explore its compatabiity with the method of science. Also we see that in the present state of the world, there is a prominence of the point of view of emergent stability of systems which is of a monistic flavour. Thus, distributed methodologies like agile, capitalism and mass media had taken stronghold in the era which might be called postmodernist. Dualism would need to arise as a critical cutural movement apart from presenting a rigourous technical formulation of the method. This might require the development of critical philosophy in the lines of a natural tendency to emergent prehoc control, such as with supercapitalism and present the historical inevitability of dualism. The choice presented here is whether to keep the nature of the feedforward control, conscious or not. Thus, it is in essence a claim to retain consciousness through cognition of our intents and by introspection. It might look uptight, but we extend to present it as the other side of  the free spirit in romanticism.
This brings us a complete circle to our discussion in Peace, Love and computers. We had all along dealt with the philosophy of instrumentalism. We had asserted that the fundamental pursuit of man is romantic, it is for beauty, whether be it in scientific truth, in mathematical symmetry, in art or in spiritual realm. Thus pursuit, we had said does not state that the spiritual and artistic pursuit as being the real ones and does not reject the rest to be illusory or Maya. We rather acknowledge the instrumentalism of the pursuit of ideas as being necessary to attain the ability to pursue aesthetics. Likewise the material pursuit is the instrument of pursuit of ideas. It is the experimentation using dynamic methods,( dynamism is the essence of materialism) that help determine ideas in their static forms. Likewise, the committal to philosophies with regard to such ideas such as for instance over the nature of physical systems that lead us to the beauty of that which cannot be directly and generally expressed, but only expressed using instruments in ideas and words to a certain extent. This theory of instrumentalism is well put in our above mentioned work. It also talks about the core pursuit as thus being neither linear as with idealism (of infinite progress) or instantaneous (as with materialism) but rather mystical, which is revealed to us and hence we intuitively pursue. It is a state of redemption that we talk about in this plane. Therefore, we had said that one should attempt redemption, or keep a forgotten state of redemption by instrumental means is our core argument. But we had also made it clear in our work on Engines of Postmodernity, where we detailed out on the proposition of a possible entity to pursue such higher goals. This entity, we might say, is humble with respect to the philosophy.
We might say that the philosophy arises not as a messiahnic revelation, but rather as a product of the times, where there is a growing historical twist to the use of computing in the modern times, leading to postmoderntiy. It is seen as a zeigteist in postmodernity and an attempt like any other. We had also mentioned that there cannot be a complete expression of any idea. For instance, there is always lurking in our system the Russels paradox, where we could not explain, as to whether an idea if sufficiently good ceases to be an instrument and does it become a subject of aesthetic pursuit itself. Such questions are unanswerable in our system, much like any other. Therefore, we would present is as a project and as a limited and circumscribed work to do. It is also not a detestation of what is clear and present and thus, it is material. We had also made it clear that it is better to rely on material conceptualization embodied in physical structures as buildings and spaces as well as the cumulative product of history, as in the tradition of the university to express the philosophy rather than rely entirely on the idealistic conception of languages and words. 
In our very early works on Computerized Socieites we had also discussed the notion of developing declared components that have instrumental value in realizing the real components of our life. These real or natural entities we had said is the common man and his nuclear family, the cultural unit or the tribe. It is nowadays common only to include the first one as the natural conception of man and split as the individual and then his nuclear family. The other components which we disucussed to counter balance this, which we might say as synthetic, but which is a natural culmination of an effort to find peace and fairplay are the modern state and the city. We had not discussed much on the city in that work, but we had extensively dealt with in the later works on postmodernity as the city being one of diminished identity and thus redeeming of liberty. Hence, we had formalized a philosophy of instrumentalism that could make the world more interesting and good and by good, we had meant aesthetic for most parts. 
Introduction 2
This is a treatise on a method of analysis. This sounds like a dry exercise in academic pursuit of those truths that serve no purpose than to assuage the ego of some genius on how far he could see, ignoring the practical aspects of material suffering of the downtrodden and innocent. An action that does not address the practical situation at hand, is considered wasteful and even sinful depending on the gravity of the situation at hand. Nevertheless there are so many aspects of our life where we do things that are neither useful with regard to our pleasures or alleviating justice and equity. These dissipative aspects of our living including familial affection, religious and cultural practices, holidays, defense and even war take up a great deal of our energy towards stabilization and actually much less goes towards progression. The fundamental question is whether this dissipatory effects are required in order for humanity to sustain. It is on the face of it, inefficient and wasteful zero sum games. Then comes the question if justice is sought in taming nature or in terms of attaining better stabilization. That is to say, do we in sympathizing with the condition of deprivation in fact refer to those problems that exist in nature like hunger, death and impoverishment or do we in fact seek to address those problems as hunger in the midst of plenty, unfair death and relative impoverishment below potential ability for satisfaction of certain needs. Most people keenly wanting to solve problems acknowledge that both these aspects are necessary in order for us to claim to be solving problems. 
These might even be problems which we feel subjectively, something that irritates us as arising from a certain nature of things or injustice in the system. Both these might in fact be aspects of the same root cause. Let us see how natural problems irritate us. If the sun should rise in the west (notice the symmetry), that is not a reason for despair, but if there be winter for nine months in a year, it is a slight reason for worry, but we can adapt around it culturally and physically. If there be random weather shifts, then that is a greater problem. The aspect of randomness could be smoothly extended to cover an intelligent tormentor, who puts us in trouble while either being substantially in covers to our intellect or playing on our beliefs and understanding. The former is the case of natural problems such as diseases while the latter is the case of a dictator or a bad boss. In both these cases an adaptive or inherent obscurity to the intellect of a certain domain is considered problematic. There are no inherent problems however, the universe is not asking for fixes. Not all problems are appealing either. As we had seen, it is those problems that we are motivated to look at are those that cause some kind of an outrage of morality or aesthetics. This presumes an apriori knowledge of certain truths and beauty as well as historical chance in our proximity to certain problems. That being granted to be present and to have occurred, we will inquire further as to the nature of problems. The problem is hence a dissimilarity to the intellectual apriori frame. The deviation in the representation would hence need some attempt to correct it and make it similar to a mental frame either of such object or its essence or some universal essence. There is needed work to fix a problem and problems as and when represented to the senses thus, would be considered by the intellect or perhaps the soul to be meriting of a fix. The fix is then to be conjectured either by the intellect or by the aesthetic entity, which would resolve on the issue by reflecting on things around it and within it, till such point where the problem upon deep reflection is no longer a problem. There might be problems of fantastic nature, such as endless life, which one finds it difficult to grasp ontologically, they could infact grasp it in bits and pieces and as disparate experiences, but infinity could not be grasped due to which these problems are not well posed, such as say a problem formalized as – does a one ended stick exist?. Those well posed problems, which have been promoted by the reflective faculties to be requiring of work, present to the intellect. Often times, the emotional self avoids certain problem or seeks help on certain problems rather than attempting to solve it. Both these represent the existence of an emotional sentiment of sparing pain, either of the self or seeking benevolence of empathic individuals. Likewise, the consideration of problems of others present a symmetric half to this approach. Thus, problems are attempted to be mitigated by accomodation and consolation, before being presented to the intellect. The intellect then rules out problems that are not well posed, by an analytical exercise. 
Therefrom, it is tempted to solve the problem by relying on the senses alone. Say, it recieves a quick feedback about how the solution is proceeding from its senses, by seeing how a certain behaviour is elicited, say after fencing a field, how a stock price rises after a sell etc. This approach to the problem allows for feedback as an instrument of control. It also assumes that the system under observation is not intelligently adaptive, to observe back. Construction of an unbiased observer upon the system, reduces the ability of the system to observe back, because the observer now has fewer biases to be taken advantage of. Such a construct to observe a system from the point of view of solving certain mathematical flaws to it, or rather to elicit upon it a positivistic transformation by objective operators, leaves no grievances to the system under observation. Thus, positivistic approach to problems might help in curtailing the complex of system interacting intricately and puts one side in charge, which is the human controller side. 
The scientific approach to problems, or generally the method of reason works to solve problems, by attempting a transformation of the system, which is equivalent post the transformation to the inputs plus the work done to bring the inputs together. Thus it is a chemical reaction (rather than an action) and a mathematical equation that defines the positivist approach to problems. However, feedbacks could not be excluded from the system and still present the system for the attestation of the senses. Hence, witnesses are generated by controlling the conditions of observations. The empiricism and theory combine to define the necessary and sufficient conditions to phenomena. The phenomena thus positivistically defined, becomes vulnerable to monitoring and control to normative ends. Even beyond such monitoring the dynamics of the normative side of the system exerts a great deal of influence. That is to say, on one side problem solving has to cope with adaptive dynamics of the complex domain and at the other side, the development dynamics of the solution domain. Bringing in the solution, disturbs the harmony and leads to inequality as well as feedsback to the problem domain. 
At some point, it was speculated if it still makes sense to attempt positivist fixes. After all, it is said that integral equations is still good mathematics, despite their escape from analytical methods. Thus, if a solution could be numerically determined by polling discrete value denotations (such as price bids), the overall function would still become amenable to policy manipulations (by nudges say) or formal treatment even. Thus, we have a situation, where problem solving is seen as a process without a heroic moment of outrage, a positivist solution and a normative harmonization invoking sentimental constructs and instead as one dynamic process, which could be dynamically computed. 
If the computability of problems is the criteria, then, one supposes that the problems associated with computation is inherited into the solution domain as well. The primary among these is the halting problem, where there is no guarantee that the system does not run into redundant loops while solving problems, which would then loop meaninglessly till starved of resources. The presence of such emergent dissipation in the system, might make us wonder if such a monistic approach to problem solving is tenable and if it is just an experiment which would cause the solution domain to evolve the very similar references of apriori knowledge and rational processes that we already have. Keeping the society or any person from experimentation is futile, as it happened with the Eden garden. But does it make sense for us to consider if such an experiment or temptation is driven by forces, that attempt to control humans. If such a malevolence does exist in the universe, then it makes sense to attempt to strengthen our methods to resist it by an appropriate method. This is admittedly a romantic stance of the universe, but it is a belief like any other. It could be a safe assumption and tendered with no possibility of disproof, paricularly where it is tendered as a critique. This work addresses the very specifics of the method, that could work to counter the temptation to move into a new modus operandi. The solution is flexible and dynamic to an extent, so that appealed implementers are not policy operatives but the business and the general culturally aware public. 
If this method could be advocated by business houses, they might find a meaning and branding is about meaning. If they take upon themselves to promise a soulful experience in travel or in food, they could also assure that in their solution. Modern businesses are formalized as value sharing networks and hence, the proposition is difficult of acceptance, unless the proposition is strong enough to generate a tree which is a subset of the network. If the business generates purpose, but does not label and sell it, rests its claim on values, ackowledges and has transparent heirarchies, then such an organization would likely benefit.












Older edition
This is a work that supposses that development could be overdone. The concept of development is a dynamic construct, which is open ended and hence, development happens as an intelligent action (either rationally legible or not) contrary to the general state of system response to time progression being entropic. Hence, the phrase overdevelopment seemed to be logically thorny. Whereas underdevelopment is quite well entrenched in the lexicon as a state of relative lag in the progression towards the ideal of development which is generally presumed to be infinitely perfectible. Here, we question the notion of the infinite perfectibility. The point is not that we deny the infinitude of growth of humans and their possible distribution, but when it comes to the way in which they make sense of their state or condition with respect to the ideals, there seems to be that there is a definite center, from which there might be deviations to either end. One end of the swing, namely underdevelopment is vastly discussed and debated, while the other end, one where development is taken as an end in itself and pursued to endow humanity with ever greater ability to construct ordered systems till such point, the effective finitude of human intelligence looses track and development ceases to be incapable of verification. Thus, we suppose there exists some apriori truths, such as the optimum to development. It is closely tied to our aesthetic sense.  It was first discussed in our earlist work on Computerized Societies.
In many ways it is an exploration of positive ways to state normative problems, without the pitfalls of reduction, compartmentalization to disciplines and linearization. It initially was motivated by my search beginning two decades ago, if justice could be accomplished logically. The ontology of stability, systems and the intrinsic mathematical patterns to the way problems could be formulated and resolved provided encouraging clues to the possibility that complex problems could infact be presented as problems of control, in which certain operations, behaviour, problem structure could be disucssed and manipulated objectively. However, it looks like there is lot more needed in metaphysics than it is with experimental sciences. Hence, holistic approaches, that just don’t eat their tails, that could be used as analytical tools to the problem, in order to promote technical fixes (leaving out a residue) might be a viable approach to discussing complex, normative problems. 
The recent ‘digital’ shift in the industrial organization, where problems are preferred to be solved locally and global equilibrium is attempted to be attained by governance, by nudges and other discreet approaches. This presents a paralell to the quest for justice in a political environment and possibly a very important problem in our times. The approaches discussed herein, particularly with respect to the construction of an ontology of intelligent control and the metaphysical constructs in cosmology and rational pscyhology would help to formalize problems and propose meaningful solutions. 
On the metaphysical method
We have seen that we are going to adapt a favourable position as to the metaphysical method. As of now, we live in a world dominated by the scientific method. Science produces solutions to problems posed to it. But it is also of importance to note that not all problems are solvable. There exists problems that are frustrating and sometimes beyond the reach of human perception and reasoning. That is to say, while science makes us rooted to reality, in that we do get along solving problems that matter in everyday life, we still have to deal with problems, which have no practical significance in any public forum, but nevertheless daunt us personally and spiritually. Even in the case of a well rounded personality, at the minimal, the problem of finitude and uncertainty of existence, makes us wonder what lies beyond. Thus a reflection on the reality that does not concern our everyday existence, is material at an individual plane and it would be tyrannical to suppress it by a vocal public policy and belief that it is all that problems that are ‘closed within reality’ that are material. Science itself does not forbid exploration of problems beyond its scope, nor is dogmatically bent on throning itself as the only path to knowledge. It merely ‘passes’ those questions that it could not answer as being not posed in a formal way that could be understood in the scientific method. It is scientism and not science, which makes assertions to the contrary.
We had visited upon philosophy as being mostly consolatory in the past (cite alain du button). Whatever we hinted as being the incopleteness of science, could in fact be picked up by philosophy. But philosophical reflection to the failures of science, mostly concerns itself with persuading the individual to think from a different perspective, without altering the reality. That is to say, if one is anxious about the finitude of existence and there is nothing in science to approach the problem, philosophy often shows new perspectives, such as the universality and the inevitability of finitude to console the indivdiual. This does not fundamentally alter reality nor does it pose a truth that negates all the other truths. That is to say, whatever truth value that the philosophy proposes is non exclusive, it is a truth like any other. There is no way to falsify it, nor does it attempt to negate the truth value of other perspectives. Thus, philosophy often looks like an exercise in phsychology, for which reason rational psychology was set as a major branch of Metaphysics by Wolff. We might however, explore here the possibility of a transcedental reality might have come of age, in these times, to warrant the use of metaphysical methods as technical tools to alter reality and produce falsifiable truths (in the same manner as sciences). Such an age was postulated in the early works of transdisciplinary exploration of Wiener, Rosenbluth and Bigelow. But attempts to model metaphysically around universals that are removed from the particular material concerns of the discipline, that is to say a general phenomenological approach is difficult of sustenance, because it had been in the past assumed such cult like holism, that it had gone beyond a formalism as a method, which effectively sealed progress in the field of cybernetics after the 70s.
The scientific method concerns with sensorial perception of reality in a measurable fashion. That is to say, science solves probems that could be linearized, by the methods of quantative analysis. The solutions are linear as well. In fact the entire body of causal reasoning is linear. It of course works for most parts and helps delineate between that which is well behaved and noise, preferrably white noise and a highly useful way to solve problems. But if we should look at a reality, where richer geometric forms than the line itself could be used into modelling. That is to say, is it possible to propose that instead of a linear causality, there exist forms that are extended in space and hence behaviourally attain certain variations in time, that is repeatable and intrinsic to the form, rather than the discipline. Say, if we look at the prognostic probability distribution of rare catastrophic failures, they have particular dynamic forms in n-dimensions that could be applied across disciplines (medicine or engineering). This approach is seen as a teleological approach rather than a causal one. The problem in decision theory, as between causal and evidence based, ought to be reformatted as between teleological and causal, but later on that. As it stands, even science is not completely linear. It contends with several problems, the most famous of which is entropy, which could not be linearly or causally reduced and only considered from a teleological point. It is built into science as an essential axiomatic law that could not be proven nor derived. It hence merits that an approach where such phenomena like entropy are first class citizens and the causal model is an anamoly is not entirely demerited. In fact all emergent phenomena defy science and is accounted into noise, as in meteorology. 
In fact everyone personally had wondered of an intelligence that transcends the body. There is an intelligence and order to the way the universe is crafted, as if an ellipse is preferred over other shapes for its geometric symmetry in many points, the limited building parts of the universe in elements and subatomic particles etc. This is not emergent out of the phenomenon of life. These and other aprioris make a strong case for a reality, outside of what we percieve and relate to and even Kant acknowledged this. Therefore, again it is not demerited to make reasonable speculations on the nature of such reality that lies beyond the finitude of existence and the correlation between how human intelligence comes to be so much in congruence to a more powerful apriori reasoning system. That one suggests an anthropic scheme to the universe or an image of humanity a arising from divine providence. 
A less arbitrary proposition would be to postulate the existence of intelligence, indpendent of embodiment. That is to say, humans can make sense of the orderliness of the universe, because intelligence is singular and convergant. Likewise, this extraneous intelligence exists in lesser degree in all the creatures that predate humans. The development of notions of  a general order that is beyond a linear order conjectured in science, to one such as symmetrical transforms, stability as discussed in Lyopunov models and in Lagrangian physics, statistical distributions of reality as observable in methods of Kalman filters and in more fine grained way in statistical mechanics. The notions of game equilibrium, of non equilibrium thermodynamics, nudge like approaches to stabilize paralellized systems all merit deep study to understand intrinsic patterns to the universe. This reading of patterns had arisen to prominence after the rise of electronic computing ubiquitously. It had become what is called datascience, which relies on emergent phenomenon such as from neural nets. These alter the very frame of the scientific method, due to the intelligence of the observed actors or their very high randomness, as with subatomic particles. Therefore, we might say, that the crisis to science, is likely to be better addressed by methods of holistic and metaphysical nature. 
The question arises, why we ought to do this and get started with this. Does it have to do with the notion of development, is a question that is central to this work. In that we had so far seen the application of the scientific method and resolution of problem by linear mechanics, allowing free energy in the system to drive cultural and social development, have we arrived at a point where the methods no longer satisfy the hunger for development? This in fact supposses the infinitude of development and perfection. The question is why we are compelled to seek solutions to problems that could not be solved linearly. That is to say, for instance, if we suppose to solve critical problems in medicine or perhaps more simply, a problem of navigation, such as self driving, we are infact attempting  a non linear domain. That is to say, we attempt to find general solution to a problem where an object is able to dynamically adjust its behaviour in terms of velocity and direction in order to seek a certain goal, where other similar objects attempt to share the space with the object in question. This general problem seems to indicate of certain innate patterns to the organization of the universe, that is to say, the existence of a signal field about the material reality, that could with high precision indicate the normal range of dynamics of each of these material entities, so that it is possible to solve problems without materially touching them (as with self driving contrasted with the pumping through a pipeline, both of which are navigation problems basically). Therefore, we might surmise that this indicates certain rules as to the dynamic evolution of a given system, which could be computed about these rules, rather than combinatorially distributing observation about every point in the field. These rules of symmetry, normal distribution and anamolies form the basics of intelligent perception. If these rules underlie the universe, even circumscribing the behaviour of intelligent creatures as well as non intelligent (for instance a falling branch and an errant driver both form equally formdiable computational problems), then what about the static rules such as elliptical orbits. That is to say, if dynamics is capable of estimation and underlie intelligent processes then, structural reasoning could be superseded by dynamic reasoning. It might solve more problems and create greater orderliness. This is very satisfying for life as a process. As for development as a term distinct from evolutionary progress, we mght have to think deeper. 
That is to say, the rise of dynamic control as a powerful force when compared to static modelling which could then be controlled by causal rules invocable by free will, in a discrete measured fashion needs to reckoned with. It has infact happened in history, without our premeditation and reflection, as the computing revolution. We see many problems are now generic problems in the computing domain across and away from the individual disciplines. This has hence become the course of development irrespective of our reflective stance. What is in fact in question here is to bring about a reflective dimension to the course of development, so that we have the necessary metaphysical constructs to exert control over it and place it in an equilibrium or optimum which signifies the existence of not an imperative for infinite perfection, but of a definite free willed preference. Therefore, it is important from a policy perspective to dwell upon such metaphysics. 
If we look at the very problem of liberation of the mind from the body, such as performing human like intelligent analysis on the environment, say by optical intelligence, we are deep metaphysical terrain. In fact it becomes difficult to even state it to be metaphysical, the question itself seems to posed in a transcedental sense, where the agents solving the problem are actually transforming themselves radically in a manner that would obscure their perception of an external reality and hence transcedental constructs of reality becomes mandatory. If the navigation problem,the problem of selling and buying of resources and numerous other heuristic problems become universally solved, that leaves the humans to attempt to solve those problems which are required of their embodied condition, rather than absolute demands of the intellect. That is to say, a frequent lamentation in philosophy is the human conditions where the intellect is held subservient to the needs of the body from time to time. It was postulated that the needs of the intellect is pristine and coincides with the universality of orderliness, whereas the embodied condition puts certain specific demands and needs that are whimsical and hence the notion of free will as distinct from rationality (not just causal but also lagrangian) is a valid entity in philosophy. 
The embodiment itself is however a situation that arose from constant tuning to the general principles of dynamics, rather than tending towards a static elegance of formal logic. The question hence arises that if computing and the progression therefrom is actually a success of lagrangian mechanics over classical one, or dynamics over static, time independent, context free aesthetics. The direction of such progress is challenging to pure intellect in that it ought to be kept in check. It is important to note that the dynamic intelligence actually produces the concept of self, by its very randomness and chaotic, but nevertheless incomplete. It could be completed by extension into the realm of unembodied intelligence, which would mean the existence of elegant rules that govern the workings of the universe. Of course, it is incomplete as well, but the precedence of the static model, allows for the precedence of intelligence over existence or apriori causes. Therefore, it would be an intellectually satisying preoccupation to model the dynamics of system behaviour and express it as augmentation to static arrangements. That is to say, we might be looking at the control problem of intelligent system, that is of high importance in the contemproary context. It is also important from the point of view of businesses. This is because business organizations consolidate ideas and distribute them and exist in a world, where highly distributed production and consumption is eroding their bases. In order for these entities to sustain, in a now rapidly going out of fashion, capital intensive setup, they would need to frame the developments In dynamics in timeless models, provide elements of control to their utilities. This might be the last battle of the capitalist and as of now, he still has deep pockets to properly put together a fight. It is this fight that the construction of explicit metaphysical models that could be linearly analyzed (as contrasted with evolving computing models) become useful. They also become technologies which could be invested upon. 
Revisiting the Experience Machine
We had been looking at the ways to approach the problem of problem solving. For this we had relied on the nature of problems themselves, in a generic sense. The nature of problems as being dynamical and computational allows us to gain a new perspective to problem solving. That is to say, in considering there being intrinsic patterns to the nature of problem solving, without the necessity to go into the specifics of the discipline and being able to state such patterns in a mathematical language, allows us to approach the solutions to problems in some kind of an universal frame. This looks enticing, but if we observe carefully, we are in fact solving problems, by the single measure of the possible dynamic states of the system in a given time period and acting in a way that allows us to reap a certain advantage by the knowledge. This is like a game. For instance, if we wish to solve a navigation problem (without contact or crashes in a car drive problem), we need to observe the environment, make dynamic models of the objects seen by knowing its fundamental characteristics of inertia, symmetry and randomness (much like in animating a certain world). This dynamic model allows us to attain a certain goal on the progagonist, at the expense of our being indifferent (not antogonistic) to the other actors in a a given frame. There had been two questions that arise from this situation. The first one is that if a development to this effect is an exercise in development and secondly about the reflexivity in the situation. 
The need to solve the problems which involve dynamic negotiation, which is the production of tools that are networks. For instance, a neural network is a statement that the use of non linear functions randomly wired together is indicative of the additive reduction of complex problems as stated in Fourier transforms. Likewise universal solutions as with normal distributions, Lagrangian dynamics exist. Leveraging on the general pattern, if we are to promote solving problems such as the navigation problem, we are actually solving the problems of the body. That is to narrate from the point of view of the mind-body problem, the mind is concerned with the solutions to things that are linearly enumerable, objective and singular. It is the body, where the free will resides. It comes up with volatile needs and pursuits that are obscure, even to the mind that resides in the same body. Likewise, the body solves problems by techniques that are mathematically and computably translatable, such as the vision problem. Thus, we might categorize what the subconscious does into bodily actions. The solutions to many of the unconscious problems work based on dynamic laws. We negotiate through milling crowds in the bazaar, we taste food, breathe and search for interesting things in a tour, by our mechanisms of dynamical problem solving that could be reasoned post hoc (cite). But mostly, we seem to be bound into the game of dynamic actions. Of course, there is a grain of randomness in our core, that gives rise to suppositions that are irrational and have no post hoc explanation either. 
In solving the problems that concern the body, such as movement, acquiring food, maintenance of health we are merely attempting to distribute what we need to do with our body to machines. These machines could survey the weather, cultivate and distribute food, move the food, provide us homes in a way that could satisfy our physical needs. They work on principles of universal dynamics. If one is to make arbitrary demands to them, the presumption is still that a large range of variance could be accomodated, but a strong ceiling is required. Thus, we might say that humans are attempting to be a brain in a vat. That is to say, in that humans attempting to solve all those problems that are solvable without the application of intellect and other apriori information like morality, by simple negotiation they are infact continuing along the legacy of evolution itself. The structures of the brain had evolved around the essential symmetry of the universe and its various dynamical properties. Hence, in solving these problems, they are solving problems other than the intellectual. It could be witnessed in contemproary market economics, the questions of morality are already pushed to the backdrop. Many of the questions are solvable by relying on the inherent choice structure of the numerous actors in the market as being locally stabilizing. This is in opposition to the control of resource distribution by means of central planning and morality. 
The questions that rely on apriori knowledge however refuse to be capable of computing. Therefore, all that would be left would be these actions. As we had seen that they are objective and singular, there would not be any personal styles to solving those problems. In buddhist cosmological philosophy, we see much discussion over the problems of dukha, shunya and maya. That is to say, it is said that all suffering stem from the bodily part. The human condition, whether related to by Prof. Moriarty or the Buddha himself narrate that the bodily needs anticipate things to be more favourable and suffers frustration as a result.  The maya is the principle that explains the inevitable suffering, where the dynamics of bodily information processing, inevitably relies on setting state of certain variables in the future and attempting to nudge the agencies in that direction, with varied results. What lies beyond is seen as shunya. That is to say, the intellectual mind is capable of solving problems in the universe. These problems are of an infinite quantity. If a mind should delve on such solutions, it would inevitably lead for ever, to solve these objective intellectual problem. That is anything minus infinity is still infinity and hence the intellecutal pursuit is unsatisfiable. Given that there are an infinitude of problems and solutions, any point chosen in their midst is likely to be surrounded by a perfectly random distribution of problems along any intellectual plane, making it a perfect zero or shunya. Thus, nothingness is not the absence of material, it is rather the pure randomness of material phenomena. Thus, the intellect does not suffer, it does not attempt to solve the problems and attempt to diminish infinity. In shunya, one can only contemplate and mediate. This buddha leaves to the sangha or club of enlightened monks. As for the ordinary people, he is very pragmatic and provides a solution of moderation and simple existence. This is also the principle of the pessimistic philosophers like Shoppenhaur. In their statement of avoidance of pain as being the prerogative of the body, while it sails unto the ultimate frustration, a consciousness of the ultimate randomness and pointlessness might be actually liberating. One could live life simply and avoid grand projects and narratives, according to them. 
Thinking along these lines, if we thus say, that we are attempting to solve the problems of the body, so that suffering is diminished and the intellect is liberated, we have to reckon with the problem of whether development is illusory. If we attempt solving problems by externalizing bodily dynamics, are we really solving problems is the question. Let us say, we attempt to solve the problem of driving, we are actually creating an environment where dynamical systems could play games. The problem then develops entropy, as with any physical system. If we start with automating signals and then automating the vehciles, then over time, it would happen that the vehicles would communicate among themselves in proximity in a distributed fashion and avoid the signals altogether and make smooth passes in intersections by dynamic speed adjustments. This would mean a less tractable and more distributed computing. This might go further as more and more computing happens at the periphery. We might say, this is an increase of entropy with the system. But it also been shown that cooperation emerge in games. There would likely arise local stability, where emergent rules (which replicate the signal) arise within the game and float to become the new rules. This leads us to a paradox. If we suppose all rules are objectively rooted at singular laws of the (static/apriori) universe, we should be inevitably be seeing these rules simply reemerge and replicate those that drove the signals. Then, overcoming signals by dynamic computing, would have been an illusion of development, if we are going to walk back into having new rules. Then we might say development is illusory. But it is also that in the process of actually moving without signals, we are feeling development. Thus, the state of development might be illusory, but the process is not.As we acquire more belongings and avoid redundancy we are in fact developing, here, cutting travelling time. 
The question warps to be one as to whether we prefer to develop in a way that would involve forgetting old laws and getting new laws in its place. Once we feel development, we would feel liberated from many constraints (not so much the physical constraints), but we eventually walk into them. But we might say, that the new constraints are more comfortable than the old ones. If we reflect from our cave days, we might say that we have developed. We have in fact been transferring consciousness across generations and prefer a genetic affinity and in recent times, ideological affinity to transfer the resources to sustain a replica of our conscious. So, far however, we have been decoding the static laws of the universe. 
This puts us to propose two pathways to development. The first way is one where we retain access to the static laws of the universe, have conscious ability to reflect upon the dynamic ways problems would have to be necessarily solve, that is to continue to lament on the mind body problem, while attempting to do our best. This approach might not have any theoretical limits. Humanity might choose to settle on distant planets and diversify relying on a thinly spread history, which is the tradition we have been following now and where we are still inspired by drum beats. The other tradition is to adapt a dynamic system and move beyond the existing mind body problem to a new one. This might involve a more flexible body and a transferred conscious state. It would eventually reach the same problem essentially, but we experience development in a different way. It is a scary leap, also without theoretical limits. In the former case, we are attached to our bodies while in the latter, we shift our forms. The intellect would still be there, unaffected and uninterested. The question is whether we should transcend. That looks like the law of the universe and if we should do that is a question of free will. Did, the the maker create the universe such that the intellect is an apriori truth, which we once know and endowed us with a soul to wonder and experiment or are we simply a twig in the whirling flow of the universe. The idea of the capital, vaguely relates to free will and supposition and hence the preservation of the workings of the captialist system is still a preservation of human legacy of bodily commitment, which could recursively converge on objective truths. 
We had in fact argued not in the side of the objective intellect, but on more arbitrary point, that of the historic incident of the human bodily form.That leads us to the point, that any point that matters is essentially subjective. Or in order that an action is material, it ought to be goal directed. In that the entirety of the cosmos, spreads out into pure randomness, if something is to be done, a problem is to be posed at all, it should arise from the point of a goal. This is something we have hinted at, in our earlier works as the ‘universe is not asking for fixes’ (cite). This goal specification is kernel around which the problem is posed and solutions are attempted. But we had also suggested that it is important to model solutions, not in purely dynamic format, but in static formats, in order for humans to make sense of it, or for that matter, for any intellect to make sense of it. All that is static is also linear. It could now be seen, that this paradox of dynamic fixes that need to be reconciled with static formulation is attainable in the formalism of the scientific method. The scientific method allows for goal specification, that is a hypothesis, as long as the necessary conditions are met. The theoretical framework is itself revised by a due process, arising from experiments in a dynamic setup. Hence, what we had criticized a few paragraphs above as to the scientific method, now seems to be sensible way to specify goals and solve problems dynamically while being true to the intellectual form. 
People like Wiener hence thought that in fact that dynamical problem solving is growing fast, one needs a new science to actually prescribe a theoretical framework to it. This theoretical framework, seems to be more of the interest for Wiener, rather than the dynamical implementations in engineering. It might be speculated if such a theoretical framework could be made out entirely mathematically. If such a framework of dynamical systems be indeed made out, it would still be able to describe fundamental fabric of the universe with respect to dynamical systems and thus, allow for linearization and formalization in a formal system and that means the possibility of control. The study and control of dynamical systems is a fast growing prerogative of the industry, because capitalism itself like the scientific method attempts to reconcile experimenation in individual enterprise to intellectually concievable priors or static models that hold across the universe irrespective of time and place. Thus, the philosophy of science makes it also desire such a control. The right environment is now present in capitalism and the scientific temper to approach the problem of control using mathematical and formal analytical techniques and Cybernetics might be a phoenix.













On Goal specification and the computer as an experiment.
If we start with the idea that computers solve dynamic problems, we see that there have been formal models of the computer that discuss it as being essentially recursive to strongly imply that. The Turing machine, which is the most powerful computer known, such that every computable function could be computed on a turing machine, relies heavily on recursion. This brings major implications. Relying on recursion introduces uncertainty into the solution. If a problem in formal language (or a problem with material dimensions if expressed in formal language) is to be decided by a computer, the dynamic nature of the computer causes it to decide such a problem with a grain of uncertainty. Turing proved that some problems, though formally expressible are undecidable on a computer, likewise every problem of a non trivial nature, could be either decided or the computer does not halt. Therefore, the uncertainty of halting exists over even minor problems inherently. This problem is independent of the problem of hardness and complexity. 
Thus, we see that the computer is a statistical machine only. If a formal problem is posed to the computer, it sometimes runs providing witness to the decision over the problem and sometimes it might not halt, in which case (assuming we interrupt and restart it), it might have been a decision to accept or reject the expression. Therefore, the computer paradoxically could decide well framed formulae from a formal tree only with uncertainty. Its methods of relying on recursion (as formalized in Church’s lambda recursion) or being able to go backward in the Turing machine imposes this uncertainty. If we are to look at this paradox in heuristics, we see that in science, we have it that we start with a formal knoweldge tree and rely on experimentation to provide support to a hypothesis. In computing we might see that we in fact start with a formal specification and then we design a program and run tests on it, depending on the number of test cases, we receive evidence of a certain strenght to support our hypothesis- which is replaced by the notion of a goal. Thus, to state more precisely, the formula attempted of being decided would be the goal, the actual runtime tests form the evidence and the formal language in which the formula is expressed forms the body of knowledge. Sometimes the hypothesis is to reject a certain well established point and it might happen in order to allow a scientist to revisit the existing body of knowledge. The same might be possible on a computer. Hence, what we might legitimately make of the computer, is the possibility of a broader embrace of the scientific method, into the everyday transactions of people. There are algorithms, which form formal languages, deriving from inbuilt premises which form the theory. The arbitrary formulae which are proposed therefrom are proven to be valid or not based on the runtime experiments of the computer, or Turing machine. Based on the outcome, we either version the program and move it forward or vindicate an existing thory and modify behaviour. We see this strongly reflected in software engineering practice. 
Reasoning along these lines, we may say that computers are automated proof systems, to logical trees. That brings us to the point, if such logical proofs are capable of being generated to the consistency of our normative position of – preference to human embodiment. To relate to software engineering practice, we see that many proof systems, actually involve data modifications. That is to say, if a valid expression is submitted to the system, it is accepted and then it is persisted into a data system. Data is nothing but an instatiation of formulae, there being however arbitrary values for string literals and numeric data (which might be in the formula discussed as ranges) for such instances. The program then might be actually seen to be modifying its truth values based on the cardinality of certain sets (to which particular classification of the data). Thus, post an insert operation, a formula might evaluate to be true (say, given that a user account exists, the system prompts for password), which might have otherwise evaluated to be false. Thus, the system becomes dynamic on the collective of user actions. A modification of the data, is to some extent, modification of the environmental truth values. Thus, the data is actually the data environment of the system, in which it attempts to solve problems dynamically, so that the existing version of the theory is vindicated as much as possible. Thus, the very reflexivity over the environment (the data enviornment) makes the system dynamical.
The use of dynamical systems to solve problems, or a preference to dynamical solutions to problems, brings with it the uncertainty. That is to say, we had seen that tree searches could be made only by Turing machines, because of their ability to back track searches (cite). These tree searches allow the system to be powerful enough to prove arbitrary formulae. The possibility for not halting always exists, because of the inherent confusion of the command and the data. Thus, a node could be interpreted as a command to coninue or break an outer while loop. If the node that is the data, is interpreted as command (as everything is a lambda) we bring in the possibility of not halting. Thus, the data environment might itself signal a loop to continue or break and if the rightly malicious formula is presented, somewhere the evaluating truth would never be falsified or a memory of an outer loop position is lost, leading to an infinite loop. Thus, the potential for uncertainty lies in the intepretation of data signals as source signals. The environment always confuse the system with an instantization of a rare formula, which the algorithm handles by a transition mapping that leads to an earlier point in the system state chart, leading to this confusion. There is nothing technically to prevent such arbitrary transitions and they could very well be introduced legally in a program. The problem is the language might have redundant ‘references’ or pointers to transition events. In the absence of repetitive references to the transition table, this might not be a problem. But all formal languages are referential. This, hence might have its roots at the Russels Paradox on the undecidability of whether a set is a subset of itself (which arises when presented with a set of sets). The program might be seen as consisting of a recursive nested set of multiple subsets, including the null set (which would be needed in order to perform closed intersection operations). This eventually leads to a paradox where the root undecidability problem is invoked. That would need to be inquired further. But for now, it is suffice that there is a rampant use of dynamical methods to solve problems. The versioned software are so complex that they start resembling non recursive truth tables, by building layer after layer of configuration and polymorphism, like the genes. This leads us to the position that the theory, might actually start diverging. 
In that we had seen that recursive formal language trees (recursion implies references) could be decided only with uncertainty, there is nothing in mathmatical logic to prevent their determination by human actors. It is only the problem of the automatons, or automated proof systems. The automated proof systems require constructs in dynamics (where just like in recursion, the present state decides future state, rather than some context free rule). This is because, humans keep track of their while loops. But it is easy to lead humans into an infinite loop, as with the ancient game of the maze. One might however in most cases, be reasonable while searching a tree, keeping trace of the pointers at various levels of the formal heirarchy, such as the case where a taxonomist tries to classify an animal. But it is not the memory ability of humans that sets them apart from computers. We have interrupts. That is to say, as humans search through a tree, at some point, a superior command cuts in an interrupts a possibly infinite loop. This might happen, where a person reflects upon that he had gone down a rabbit hole in solving a problem or that he simply stuck in a maze and call for help. The ultimate embodiment of humans allow for a master program that constantly controls lower level programs, to the effect that interrupts become possible. These interrupts might be reflective course correction or primordial wakeup calls such as falling asleep for instance. The presence of this interrupt relies on certain apriori truths, that govern the system, which is discernable but not epistemically accessible. In that we see that governance and control being central to this discussion, we might pose that all lower level programs other than the master program execute to goals. The goals might be specified in simple language, but the constraints are often non epistemic and difficult of specification. Hence, as Bostrom says, the central problem in artificial intelligence, might be the goal specificaiton problem (cite).
Thus, we might as well interpret each program to be capable of spawning multiple local programs, which it controls (much like an experimental evaluation) so that the controlling program could vindicate or revisit its truth system. This is cascaded backwards to reconcile with fundamental notions of good and bad and pleasure and pain. Thus, a high level functionality delivered into a system, in fact allows for low level programs to be accessed for such low level programs (say residing on a sensor, or a simple UI) to interpret environmental events to the governing system. The low level program has more precise goal specification than the higher level program and recieves interrupts from the high level program. The event driven nature of the computing process causes the system to grow dynamically, until a human actor controls and decides to version it. Thus, human control of software provided a crucible for experimentation. But much of the experimentation tunes to primordial notions of reward in humans than being morally and logically right in practice. In fact we might trace the availability of root controls and interrupts to the notion of self and that of conscious and even the Cartesian argument of a ghost in the machine. Practical software developent is rooted to market forces, leading to a dynamic strategy by humans, due to which the system might develop undecidability at an organizaiton or even a macroeconomic level. The fundamental instability could be regulated, it might be speculated, if fundamental theories exist as to stability of the system and its control. But there does exist such general theories, in fact of dynamics for centuries, such as in thermodynamics. But these physical models are very abstract and find it difficult of application to low level ontologies. Hence, we might in fact be reaching the same level of modelling, where the usefulness is lost to accuracy, by dwelling into the theory of control.
It would be better that disciplines that derive from static truths take the center stage. Static truths are the real knowledge, they are linear and causal. This is actually the only knowledge, the human mind is capable of interpreting and expressing without ambiguity. They are objective and non plural. But even physics is suffering a crisis of uncertainty. Judea Pearl (cite) wants Bayes networks to somehow enhance the linear and static knowledge. His theory of knowledge, works on the ability to express information as joint probability distributions. But the goal or apriori is required of specificaiton, in order to generate causal knowledge. In fact goal specificaiton is a partial expression of a deeply rooted human sensibility. If the bayes network that is evdence based is applied as experiments to hypothesis, then based on the measures of sensitivity and specificity, so central to Bayes formula, could be used to vindicate or reject them in favour of a null hypothesis. But it remains that the causal model is still conjectured apriori and placed external to the system. Machine learning on the other hand, proposes that the apriori knowledge is actually aposteriori. It reflects the essential equilibrium state of systems for those who work on the basis of game theory, emergent local equilibrium like Nash equilibrium which could then emerge to be goals. It is not trivial to reject this argument. In fact if we are to argue on the objectivity and singularity of static models of the world and see it perpetuated in environments where there is no life at all (such as astronomical symmetries), it leads us to think, that every system, sufficiently complex, whether living or not, converges to such universals. Thus, there might not be a directionality to time, which is primarily arrived through entropy and where order is emergent as Axelrod says and Lo! time might in fact be reversed. Hence, locally the arrow of causality reverses to provide the apriori truths aposteriori. This coupled with the mathematical revelations of such connectedness and emergence arrived at from such forms like Fourier transforms which allows for anlaysis of complex systems constructing them to have emerged from summation of simple sines and cosines, or physical notions of relativity.  This ambiguity of time, allows the construction of machine learning, to provide for its own constraints of controls ( to achieve externally specified goals), either within the particular dimensions selected as linear coeffecient generation(as with supervised learning) or where even the dimensions are let to be autoselected or a reliance on fundamental notion of reward exists (to the detriment of Kantian apriori morality and in favour of game equilibrium). 
Thus, we might say here, that goal specification, that allows for emergent order might be a viable path. In fact the point that varied additive methods such as SVM and neural nets converge towards this provides tangible evidence here. On the other hand, we might as well argue that dynamical problem solving, should in fact lead to generation of linear causal knowledge and fundamental controls including interrupts ought to exist where the system develops instability while attempting to solve problems. Thus, the generic tools of controls for instability, overheating and such notions as prognostics and diagnostics ought to exist for every dynamical system, in order that it does not lull us away from declared limits of tolerance against apriori truths. In fact the notion of stability and elasticity about a stable equilibrium are conditioned by theoretical apriori that are deeply rooted, as with thermodynamic models. These kind of deep rooted control, provide also a viable alternative to the development of theories, or epistemically expessible, causal knowledge reliably. The system in the process of vindicating theories, is by the notion of holistic governance, prevented from abducting to completely new theoretical grounds and that remains in the human domain. Thus, we might say that the theory of control and machine learning start with opposite philosophical suppositions, the former on the notion of irreversibility of time and the latter on its ergodics (cite). The former involves work in creating new mathematical formalisms and extending the existing notions in hamiltonian and Lagrangian dynamics to provide a framework of high level controls to dynamic systems. The machine learning approach, on the other hand might require work on the continuous reconcillation of emergent dynamics to existing aprioris, revising them and transforming them by the touch of the machine, rather than humans figuring out things operating at a lower frequency (cite). Thus, in fact the whole thing reduces to a philosophical starting point, from where the work needs to be done. The work in machine learning is primarily technical involving newer elementary systems like the ancient perceptrons (cite Pitts). At the right technical matrix, the system might kick in dynamically to provide human like emergent intelligence, with its values and morality. This was seen to be superintelligence in Bostrom. It might be that such emergent intelligence, while satisfying human like sentiments, might not see fundamental value in coexisting with humans. But that would again question the fundamental philosphy of what degree of change is extinction. We might say that dinosaurs survive in birds and analogously, we might believe in transhumanism. The goal of humansim or rather the absolutionism of the human state, like what Einstein believed in and the need to invent ever abstract and ever far reaching, but useful models of control as with cybernetics are poised from opposite directions in dialectic. A lot depends on one’s philosophical stance on whether God does play dice. The market economics favours aposterioris but the capitalist economy favours aprioris. Thus, they might in fact be opposites unlike how it is dealt with in popular press. Thus, a theoriest or technician based on the philosophical position he professes ought to appeal to the right resouce base in order to grow his theory.
After note: Hence, we might say ML and AI are oppositely poised. The latter attempts to build algorithms that could dynamically synchronize with the market forces to build an emergent intelligence, which would in the nature of things align with human intelligence and could coexist. AI on the other hand, is a more ancient field, pioneered by Turing, Von Neumann and Wiener. It attempts to build systems which can self replicate and explore ideas but only as experiments. The believed, mostly that knolwedge is apriori and experiments are our ways to vindicate them and remove bias and not as the empiricists only sources of knowledge. Hence, they believed that computers could help running experiments much like lab assistants and devised ever general and useful models of control. The ML side believes in the generation of a belief network and that being the basis of human epistemics while the AI side believes in the knowledge tree which is augmented and altered by experiments. The AI side did not worry much about safety, while the ML has a lot to worry about in the uncertainty of transitioning. 

Practical Approach
This is an important part of this work, which would allow the work to be absorbed and resourced in the real world. In our discussions, we have seen that the problem solving by application of static rules to dynamics sytems involve the development of a drift in the system. Even deeper, if we see the way humans make sense of the world, as meaning, we rely heavily on static constructs (as Leibnitz says, truths that hold in all possible worlds). These truths are geometrically like trees and they are analyzable rationally. Real world however, relies on paralellism and dynamics. It is possible for humans to percieve dynamic truths as well, in a non epistemological, poetic sense. 
That is to say, we present a metaphysical model of the world. The root entity of our universe is the society. This society as a metaphysical ontological entity could be a continuum from the community level all the way to a global level. We might for some time assume a group which is politically united and culturally united, which we might designate as a nation. The society, could be componentized in our metaphysical model, or metamodel as being composed of a boundary around it, faced to a generally entropic universe and being progressive atleast to the level of coping with the entropic pressures, which is not constant, but might be chaotic as well. This society metamodel hence would need intelligent procedures to solve the problem of entropy. As we said entropy is paralellized, dynamic, computationally intractable and chaotic. The means of intelligent is to compose a tree structure which is logically consistent. This formalizable, provable tree structure forms verifiable formal knowledge. This idea of knowledge, as epistemological construct allows an agent to solve problems of entropic nature has its limitation, in that it generates reflexivity. The entropy starts working around the obstacle placed to it dynamically and the solution wears off over time. 
In our metamodel, we also have it that these engineering constructs are built of episteomologically coherent scientific knowledge as adapted to dynamic situations by optimization to constraints. This engineering is carried out and as said above it has reflexive effects. The engineering construct is positivist, with no notions of goodness and hence both entropic forces and the consumptive forces are treated with equally clear boundaries. The failures of the system, in terms of unanticipated noise perpetruating beyond its boundaries, results in reflexive, adaptive consequences on either side. Hence, a railroad laid through a region, might affect both flooding paths as well as urbanization as being chaotic effects of the mechanism. These chaotic forces cause an increased normative expectation or formulation of new goals as well as a requirement to maintain (by culverts to manage flooding, say) of the physical chaotic forces. Hence, engineering would have to be maintained and the steadier it becomes, the more it becomes what is called the technical infrastructure of the society. The generation of maintenance costs arise from policy initiative. This brings us to the second component of the society, which is the political stabilizer. There exists certain entities which legislate and regulate which engineering fixes are permissible and which are not, despite the scientific coherence and engineering efficiency. The regulation of pace and permissions to the construction of engineering is done in the interest of stabilizing the chaotic normative environment. This stabilization of the chaotic physical environment is still in the hands of engineering progress. This dichotomy was described in Mueller’s sociological treatises (cite). The political policy environment however is formed of a more absolute epistemology, which is rooted in an ontology of human existence. That is to say, to be human is to be able to appreciate good and bad. This apriori truth allows humans to decide on the political policy required to stabilize the normative environment. This is described as the moral substrate by G Hardin (Cite) as well as in Kantian apriori. The existence of apriori had also been described in our earlier work (cite) on why only automatons suffer the halting problem. Real humans halt, either because they reflect back to the moral root, or because they die (cite). Thus the significant barrier between the automaton and the living form is death. 
To move further in our metamodel, the necessity for political stabilization creates the space for liberty. The third component of the society would then be the individual domain, which is libertarian, non formalizable and even spiritual. Thus, the moral root is referrable, or rememberable (in Platonic sense of recollecting a past of absolute knowledge), by contemplation of nature, art and aesthetic beauty, which form the poetic and non epistemological domain of knowledge. This allows humans to indulge in art, aesthetics, travel and compose poems which have little to do with survival and in progress. This absolute and timeless reality of human state forms the absolute substrate for the formalization of both scientific truths which are adapted into engineering and ethical truths which are adapted into justice and equity. In fact Baker (cite) considers that at its deepest root, Physics is epistemology. Thus, we might say that there might be a possible unification of the symmetric and aesthetic constructs in epistemology of ethics and of science itself and a generalization of human uniqueness to the presence of a root (on death, or more generally the finitude of memory even if one is to survive forever, forming an external overarching finite state machine) on life itself. 
In this metamodel, if we introduce the notion of computing, we are infact referring more generally to dynamic fixes to dynamic problems. That is to say, were we to prefer the use of distributed decisions to problems, rather than theoretically singular facts, we might have a dynamic, co-evolving fix, in which the concept of reflexivity, tractability, control and stabilization does not make sense. This is not unique of digital computing. If we look at the way the market operates, we are in fact looking for dynaimc fixes. The market operatives however refer to their liberal roots (free market, here means liberty), without the intermediary of the political setup and decide on the questions. Thus, a dominant market is derisive if not dismissive of both policy and science. The dominant market, would however require the individual to be mindful of rational choices as being verifiable of the operative. Hence, the operative either relies on scientific analysis which is rooted total recurisively, but in a postmodern metamodel, he relies on the partial recursive construct of the corporation. This allows achievement of operational efficiency, even without referring to moral roots. The aprioris of policy and morality are both removed from the equation in a postmodernist operational control model relying on distributed operatives and partial recursive corporations. The postmodern corporations are themselves dynamical and distributed, unlike the capitalist model, which involves material bets based on analysis. Thus, analytical truths give way to opaque, network based and dynamical truths. The role of computers is not unique in this, but it rather an evolution of how the corporation and market operates, much like the  chainsaw is an evolution of a regular saw. We might tie this to the cultural evolution involving a dismissal of metamodels. If we look at 42 being a satirical truths arising from computing, postmodernity thinks 42 is as good a truth as any other. 
The question hence is to see if this dynamic model is good or not, from a metphysical point, or rather describe the condition more precisely, as is the aim of a good exposition. The dynamic model excludes by removing the conditions of necessity of a policy and a moral setting. We will ignore the emotional dimensions thereof and explore further. If we are to understand engineering fixes deeper, we see them to be dynamic adaptations of fundamental science. Say, a railroad wheel is designed based on the equilibrium being judged on convergent series analysis on forces. If a flange is designed to be at 28mm for a 1670 mm track, we expect it to hold in a great many range of conditions. But application of the theory to practice is conceptually the same as experimentation, albeit noisy and long running. It might actually, though rarely help update the theory in fact. We visit whether it is possible to build into sciences a thoery for managing the chaotic normative environment. This we do in order to avoid the monistic dynamic fixes, which had taken to new highs following the introduction of the electronic computer. Thus, if the real world is to be considered experiments, could we formulate some behaviour seeing through the noise and record them in common ontologies in a rational tree. This was attempted in management sciences and there had been a strong relationship between the science of control with management, particularly evidenced in the works of Stafford Beer (cite, The brain of the firm). Thus, we might visit upon the idea that whether formalizing a theory on general way in which dynamic systems behave allow us to control them and treat them as experiments capable of enriching the theory. This is putting a lot of merit into the hands of science, almost to the tune of scientism in order that the social model is freed of the perils of a progressive monism. 
This begs the question of what exactly is the peril. If we look at engineering fixes to problems, we might see some interesting distinction between analog and digital. Analog systems, such as the railroad wheel, balances by oscillatory movements about a stable equilibrium, listening to continuous events. It is different from the computer, only that it is listening to physical and continuous inputs, as frictions, as heat and rigidity. The use of digital solutions on the other hand relies on discrete events, but a more important distinction is that it is materially unconstrained. That is to say, while the analog solution allows the train to pull into the station and is limited in scope to that domain, a digital solution of rail stabilization could be trivially extended to do facial recognition and hail a cab, as soon as the train pulls into the station. This displaces, the captialist cab drivers waiting at the station gates, heuristicall profiling from faces to make a suitable offer to allow the passenger to hop into their cab. The digital fix is transcedental of material dimensions, allowing it to displace the other dimensions of human existence, rooted in the ability to reflect and contemplate beauty, goodness and justice. That is, it robs humans of a good dilemma. 
The solutions provided by dynamic systems, extended digitally, which presents a particular extension to the schema of the solution due to technical reasons, we will visit later, hence lead to a situation, where there is no necessity to be restrained by policy and morals. It is however still physically restrained. Let us assume that a given action under the presented necessary conditions (which involves only the physical reality and not the moral apriori), is automatically cleansed of morally undesirable and politically undesirable consequences, then in effect, we might be living in a simulation. That brings us to the question of peril, which is an arbitrary question that whether one would prefer to live in a simulation, where pleasure and pain are immediate from our actions and does not extend to moral consequences. Thus, the sealing off of reality, in this case to a degree greater than with conventional modernist models might present a dilemma discussed in the experience machine (cite) and Bostroms simulation argument (cite). 
If we explore the case, if a person would not want to live in a simulation, then he has to live in a real world where there exist situations, where he could refer to the constant of his moral root to give meaning to phenomena. This would allow the person to detatch from the value proposition of dynamic fixes and instead introduce tractable fixes where theory is held important, again for arbitrary reasons. Thus, the need for a moral root, requires to operate in a manner where a theory is to be available for examination. This would mean finitude of dynamic phenomena being determined by theoretical computer. That is to say, the ultimate interrupt to dynamic systems only becomes possible, if there is a computational process running in our brain and our brain is programmable only by epistemological tractable data. Global controls such as these, even if not foolproof, being smudged by bias and short term interests, nevertheless seems to be the only way to do things, to which we, as an individual, is incharge, even if remotely and through a pyramidical dynamic arrangement. On the other hand, if the mechanism such as a completely free market is to self stabilize, then there is no point of control or to deliver the interrupt either to stabilize or terminate the process. This results in no way of asserting if we are running in an infinite loop, which is the characteristic of a finite simulation. This argument is distinct from our arguments in our earlier works (cite) where we refer to the possibility of an emergent orderliness. The older argument suffers from the impossibility to prove that the emergent equilibrium is divergent from the singular universal truth tree.
The practical thing therefore, is again not technical and not dynamic. It involves the construction of science, the promotion of stable policies of cultural vibrancy to lend substrate and contemplation to ultimate truths. Therefore, if a technical adaption of the truth is made, it is essentially the development of a theoretical framework of control of dynamic systems. It might involve a policy promotion and appeal to reservoirs of surplus, such as grants and capitalists to allow for further examination of the metaphysically speculative model. This is hence a cultural move, suggesting of a subjective notion of redemption and is zeigteist and limited to the current historical circumstance. Hence, one way to practically adapt this metaphysical view is to develop theoretical frames which can supplement scientific truth finding by being able to denote real world happenings as stable experiments and thus enabling the development of scientific knowledge of fine grained epistemology. Thus we might designate that given we supplement the scientific method with the theory of observation of experiments from noisy systems, we might be able to scale science indefintely. Thus, we might have overcome the problem of wicked systems and the universe might itself become ultimately tractable (amazing simplicity of the universe, cite). A schema of progress along these lines, as we have hinted, might call for cultural divergence as a voluntary way (knowing the possibility of all life to reconcile over aprioris) and to spread intergalactically. The tools of science could be honed further to produce an universal instrument to sustain our ‘way of life’ and permitting of cultural experimentation (which is zero sum) and thus we approach a cosmological model bordering on scientism.
Tactically, the preference to analog engineering arises from the point, that restraining engineering to a dimension which could be catalogued and controlled by our senses would avoid extensive dynamism to the fix. Alternately, we might need observational tools that present the informaiton dynamic model to our sensorial faculties, of which rational faculty is one. It is also for this reason that we represent the need to build analytical models of dynamic phenomena.
Part2 – The notion of Work.
If we look at this proposition where it is impossible to tell about the question of simulation itself, we are in tricky terrain due to our presumption of a sustainable infinite loop. In fact entropy demands otherwise. In our earlier work, we had relied on evolutionary empiricism, to suggest that the system would develop new equilibrium dynamics due to emergent entities and ‘control’ would emerge. But that does not deny the possibility of a relatively sophisticated simulation. We might invoke the classic example of multicellularity as a simulation. Civilization likewise, might be seen as a simulation, where the real pain is simulated in a more sophisticated environment. The progression of this to the degree of multicellularity is hence a plausible situation and the desirability of specific threshold is arbitrary. This being the case, we visit upon the question of whether there exists an urgent need to move away from simulation, while in fact it is a matter of degree. 
The discipline of cybernetics placed great emphasis on the notion of control. This would mean, that there is a need for a science, in a manner that is not from the point of view of dynamic addition of value, but rather in an apriori sense, of control. The importance of an objective examiner who can look at the way systems evolve to defy control, which is the hallmark of being sucked into a simulation, was felt as a cultural move to counter the threat of an ageing disciplinarily compartmentalized science faced with a world leaning heavily on dynamics to get things done. But unfortunately, cybernetics itself lost its principal objective by including the observer into the system in second order cybernetics, which spelt its end. Hence, it is basically a cultural question, if a certain degree of simulation is warranted. It is more of a need for pain, of torment (even Descartes was hinting on the torment of being skeptical of the special status of humans, rather than a mass of physical elements as being the driver to feel alive – “I doubt therefore I am”). It is the ideas of pain, torment and dying that allows us to feel alive and actually be. Therefore, it becomes imperative for humans to resist moving into a state of simulated reality, where the real pain is simulated to various degrees in environments. The strength of the boundaries of the simulation is presumed stronger than humans being at the perimeter. It again arises the question of degree, wherein, a progeny of humans who arise from the conglomeration of humans is desirable. If we do allow our progeny to defy our intellectual access, it might as well be that transhumansim is merited. Hence, again the question is open and arbitrary. There might only be cultural propositions to the effect. We had elsewhere seen that the question could not be avoided or resolved by debate or analysis. The only way it could be handled is by action, by cultural or countercultural recipes. 
This question, we visited in our earlier essay on Musk (cite). We might propose cultural experimentation, with the possibility of torment as a recipe to allow humans from descending into the simulation. The second and distinct way is to formalize a theory of control so that we might be able to better sense the real world happenings and adapt them as experiments in order to feed to the existing body of science, making it more robust. We might as well combine both the approaches. The point here is that there exists ways in which dynamic systems could defy control. There is needed of ‘work’ to actually encode the dynamical system in a manner that people can make sense of it, to their root context. If humans choose to externalize the problem, by further tuning on science, or they attempt to handle the problem by cultural differentiation in which case they might rely on consolation by philosophy and local stabilization, not by invoking scientific methods, but again by philosophical or mythological reflection is not material in doing the ‘work’. Thus, it becomes imperative to define ‘work’ here. Work would mean anything done to the detriment of immediate gains and directly defies Lagrangian Pathways to stability. Whether the work is done by encoding or experimentation by cultural divergence is not material. This might formalize the arbitrariness of the decision to split culturally or to improve scientific prowess by devising general mathematical models into the notion of essential work in a Lagrangian frame, where there is no arbitrariness. 

Part 3-On the formalization of dynamics, on Medium
This is an attempt to describe the general principles of dynamics and the way in which it could in fact reconcile with static models. Let us take the case of multicellularity. Multiple cells might stick together and evolve based on historical points of divergences to form a simple colony as with Poriferae. Let us take the case of a rooted colonial organism which has a central cavity. The organism is able to dynamically progress, because once a certain prey is found into the cavity, the cells dynamically work together, mostly by copying where one of the cells start secreting enzymes that digest and the others fall in tune. Thus, the presence of a medium is important for any dynamic development (we might call this a field in mathematical models). The body cavity provides the fluid medium for the cells to interact. The blood stream, enzymes and chemical signals had allowed for the development of dynamic development of systems, into specializing and cooperation all relying on the balance between self interest and constraint by norms. Even at the population level, all actions in dynamic progression rely on highly rarified media, it involves air mediated pheromone distribution for instance, visual signals rely on photon particles in order to allow for cooperation. Thus, essentially the need for a medium is important for the development of dynamic systems. The rarer it is the better. 
In this view, particularly with respect to multicellularity we look at whether a ganglionic system of neurons form a medium. It is important that while the dynamic approach based on norms and the pain of alienation upon defection form the fundamental principle of organization of dynamic systems, including basic multicellularity, it is difficult to coordinate about such a system. Coordination involves a serialization of operations in a strictly controlled manner. That is to say, if the neck cells of the porifera would need to perform a swallowing reflex, involving first ring of cells constricting strictly followed by the second ring and then the third ring, there is no way in dynamics to attain this behaviour. It ought to be coordinated through an electrical signal network, rather than a fluidic medium. The electrical discrete, digital medium is also dynamic and computational, but it allows for specific switches to actuate certain other circuit paths in sequence. Once a switch is thrown, a feedback ought to ensue upon constriction of the first ring and this event is processed by the statemachine arrangement to transition to a desired flow. The notion of ‘feedback’ and events is central to electrical circuit based discrete computing. Thus, discrete computing relies on feedbacks and switches that operate in a delayed fashion in order to work. It is just another way of cooperation as the aetheral medium encountered in analog dynamics involving fluids and actual mechanical contacts (which might be seen as electromagnetic field interferance). The use of discrete circuits, rely on notions of feedback in order to transition through pathways and hence is liable to infinite loops, till such point entropy destroys it (for perpetual motion machines could not exist). Therefore, discrete circuits also involve dynamic development in ways that is not distinct from regular circuits, but have low entropy due to the ability to ascertain the originator of the feedback, instead of a random first actor in a game like situation involving broadcast of a message. This allows for central coordination by a computational entity. The surrender of controls to central computing, might have arisen by a dynamic principle of coercion. Thus, if a nerve embeds itself into muscular tisse, the defectors are punished by the majority by destruction due to mechanistic distortion. Thus, neuronal coordination allows for systems to dynamically develop in a way different from medium based propogation, because the signals are point to point from a central computer that is set in motion by an incoming sensorial event. This point to point signal is responded back by the reciever, achieved by coercion persumably in the simulated environment. The muscular tissue unlike in analog development misses on the aspect of choice of response to the trigger and the ability to analyze it locally. The neuronal actuation is in relativistic sense more absolute decision made and the system ought to respond back (unlike in other cases of broadcasted chemical signals, where an individual cell can hide behind a group and thus cut queues). The mechanization of the cell (enforced again by the popularity of local response?) allows for better central coordination, or rather cooperation among groups of cells over a central consciousness. Thus the tissues of the hand exert to defend an attack to the face, because pain receptors elicit a coordinated response from the nerve network. The network becomes aware of certain goals, such as avoiding pain and accordingly executes a program over a digital network. The programs are loaded from memory by a master program which listens continuously. This is possibly a manifestation from the behaviour of the single cell, which allows for reproductive transmission of the behaviour, which then takes us back the presence of an apriori master program.
If we look in terms of mathematical fields, we might consider the presence of functions in a field. Functions map from one number line to another. They may be arithmatic like coefficient based scaling, adding and composing of functions themselves. They might be mappings from the line to a curve in the case of logarithmic functions and from lines to circles in the case of trigonometric functions. These functions could be combined algebraically by addition, composition and equaitons could be made to point these functions to values on the number line. This algebraic function has roots, where all the additive values of sequential combination of functions converge to a null value. That is to say, where the function reduces to nothingness at its root values of the variables. If multiple equations are involved, we formalize the system as a system of equations in matrix algebra. Transformations in the nature of translation (like coefficients) or rotation by application of trigonometric functions are defined for matrices as well. The rules for matrix productization require that the specification of transformation specifies completely (hence the rule on columns of first matrix to match the rows in second matrix). This, being our fundamental notion of functions, we see that geometric expressions of funcitons require a field to be specified. In that the functions could assume spatially extended forms, it is possible to say that this spatial extension happens in a finite time, howsoever small. Therefore, it is possible to know the location of a point at a given time. The ability to know a future point of a function, just by knowing its form, expressed in a formula or equation makes the function a true function in Eulerean sense. These functions could be differentiated to different depths. Thus calculus is a study of pseudodynamic functions rather than real dynamic functions. This is because the field is assumed to be smooth and unpolluted. If one has a system of equations which permits multiple solutions, then for the system to evolve in a certain direction, there is needed of impurities in the field, or atleast some the variables need to be initialized to a certain values as equalities or inequalities. Therefore, a system truly evolves from an indeterminate state on the first occurance of certain initialized values that influence the form. Thus, we might in fact view the entire dynamic process of development of multicellularity onto complex societies as being mediated in an impure field, but essentially remaining functions. 
While it is disconcerting to understand that humans are in fact products of history and its dynamic, it does seem factual. The ability of such dynamics to resemble a mathematical field, albeit with some impurities, allows for a possibility to bridge onto the pure forms of mathematical knowledge. Therefore, eventhough humans and socieites are products of dynamics, there is nothing to prevent them from contemplating the essential order. But the impurity had forever seperated us from the pure forms, which allows us to contemplate the pure forms, by such appeallate symbols like infinity and zero. We do not have a consistent system to verify or epistemologically make sense of such pure forms. But our historic roots, has some apriori reference to the initial conditions, perhaps. 
Clarification:From our foregoing discussion, we might as well say that digital systems are really not as bad as it was initially discussed in this essay. Digital control, depends on specific feedback from systems, even if it involves regulation of railroad speed by sensing curves and acting on algorithms. In such situations, the feedback is important and the algorithm proceeds. Its extension to cab hailing we foresaw is due to the adaptation of methods of analog control onto digital control. This problem actually arises from interaction through a network medium, rather than working to synchronous polling for events as in a tree search. The computer is likely to extend its functions to hail a cab, apart from stabilizing the train, only where it interacts with cross domain systems. It somehow floats a message onto the cyberspace and does not wait for events to come back. The registration of the message to some queue manager or distribution manager is sufficient for the system to treat is as a ‘delivered’ event and halt. Thereupon, a tree search ensues to find the best actor on the message, or it is pushed to some sink and the distribution manager polls again for incoming messages. This element of continuity to digital system is what is problematic. A system that interacts with analog agents and is contained within the same machine is still a machine. But a network is a network and even if the medium is electronic, it approaches the analog medium. Hence, it is the analogization of the digital field that is disconcerting and not otherwise. 
Therefore a formulation on practical terms might be to 
Formalize the notion of essential work that needs to be done, in a way that adequately keeps the internal energy of the stable system of humanity so that simulation in a computational layer is avoided. This might need the notions of stability, metaphysical components, cybernetic theory for allowing science to be combined with the theories of stability and controls. The formalization and use of what is called datascience models to convert real world happenings to experiments. Formalization of internal dynamics of cultural divergence might be helpful too. The essential work might be formalized in the generation of internal energy to avoid the further development of the system. That is to say, how to keep a system from developing in a dynamic way and instead facilitate it to either remain stable, by dissipation of energy in cultural actions or by doing essential work in encoding the knowledge in the system (which is redundant too and exists only from a humanistic stance). The development of the future to prefer detatchment of the network by copying into discrete components. That is to avoid the fate of multicellularity. 
This formalization of a model that could describe the tendency of networks to develop might be approached from the formalization of the notion of simulation (by nested infinite loops), of dissipation in the interest of stability, of the use of static solutions in order to complete the dissipative picture of humanity. There is also a need for formalization of the ‘work’ that needs to be done to keep entropy at bay and avoiding overreaction (a notion of universal control) in order to stabilize the system. Thus, it is a formal model to describe as to how a system can do just adequate work in order to self stabilize itself and to progress by complete detatchment, rather than growth in a network. One also needs to examine the possibility of a potentially infinite perfection of scientific knowledge, without having to rely on dynamic methods. If that be the case, it is possible to host a very large conventional civilization and grow linearly without loosing fundamental characteristics of stability. Hence, the problems of how to scale without running into developmental dynamics. One way is science and the other is cultural experimentation. We propose a paper to examine the stand point of Elon Musk in the near term about Martian emigrations.


Todo – research projects in hnd
To devise a theory of the matrix, where computer networks are approximated to fields and diffusion concepts are applied. To devise an algorithm, non experimental of finding equiibrium clusters and pathways in topologies ot be applied to medicine. To rigourously refute simulation argument. To develop a theory of phychology around graphs. To metaphysically formulate the necessity of martian ascend. To work with formalizing notions of suffering with Helsinki.

The Method
From what we have been discussing, much of our approach is metaphysical. In fact we would here state our current projects as being:
Wwe have been looking at problems concerning simulation, suffering and other survivability questions of mankind as a whole. This second problem might be provided under the head of Developmentalism. We have postulated the ontology on the entity of Society, which is common in Sociology. The functional analysis of the components of the society is also the realm of metric based economics. Thus, the second part of the practical quest might fall under the realm of Developmental Theory. This might also include the questions related to  address the question of cultural divergence, as with Martian colonization. This also involved an analysis of the emergent behaviour behaviour  in computational networks, such as topological and control theoretic explanations of the inherent behaviour of systems, across domains. In an applied domain, we have been mulling the application of such understanding in prognostic and diagnostic problems in dynamic systems. This might be about devising ways in which systems with complex dynamics such as biological processes could be controlled. Thus, we might say, our work is divided into Developmental thoery and complex systems dynamics. These together could be clumped under the domain of rational cosmology, as categorized from the general Metaphysical body of knowledge by Wolff. Our other concern regarding the nature of cognition, its distortion by chemical receptors, of the mind body problem as underlying the questions of progress and redemption, concerns with the metaphysical domain of rational psychology. These both are metaphysical in approach. 
To State 
To work on the problem concerning finding trajectories of diseases, in terms of a topological field containing state spaces, in which oscillations over medians are definable (state of relative or absolute homeostasis) or as function spaces, which involve a traversal of state spaces that is reversible or irreversible and much frequent than random movements in a field as with random walk stock prices. Thus, we define the field as having inherent qualities of stickiness and low resistence function bands which resemble a curve. These are not clusters as defined in machine intelligence, but rather a set of functions and state spaces that could be derived in a generic way from an iterative observation of the behaviour of a system in terms of parameters or dimensions.  Thus, we formalize the ability to derive functions of recursive nature from iterative truth tables working on generic methods of topological analysis.
The second problem is concerning development and its sustainabiliy, in terms of avoiding a marginal decline in normative satisfaction as well as avoiding the case of putting adaptive pressure on the problem domain, which may or may not be live, but of any arbitrary complexity. The use of positivist fixes is hence postulated of requiring suitable developmental approaches, in order to attain stability.
Thirdly, we look at the problem of cognition of reality and ability to define them over mathemetical formalisms in graph and field theory.  This cognitive theory might help us formalize the nature of emergent intelligence and help develop a theory of diffusion over a sparse field (which might be a graph lke network) in order to make sense of computing networks. This, we might call as the matrix theory of computational networks. The matrix is regarded as the dispersal medium (much like water) in dynamic life like systems.
In terms of metaphysics, we see that working with objects that do not have physical or material counterparts. The objects in metaphysics are ideological and abstract. In such a premise, we might state that mathematics in fact presents a metaphysical discipline. Removing all the material components, what is left of metaphysics is often the method and for which reason, mathematics is often described as a method. Hence, our work on metaphysics is more concerning the methods of problem solving. If we say, we solve a problem in science, we are actually giving the necessary and sufficient conditions for phenomena. Thus, it might be expressed as a bidirectional implication in logic or an equivalence in mathematics. The formalization of phenomena as equations provided very powerful methods to solving problems in the past and the results have made history. We wish to extend this methods, motivated by its shortcomings in the recent developments in history. That is to say, there have been frequent allusions to wicked problems that defy scientific analysis and an incremental application of iterative, paralellized and dynamic methods to solve problems (such FEM, partial differential equations, agile methodology etc, non halting continuously evolving systems etc). The application of this method and this new metaphysical proposition is to counter this development. 
To be more precise, if scientific truths are static, independent of time and space, but leave behind an imprint on the dynamic systems in which it operates upon as well as an adaptive claim on the developmental system, it attempts to service, there is a problem of insufficiency of the scientific method. The adaptive system comes back with new problems to decrease the marginal utility of technological formulations based on science and the developmental system destabilizes, craving more development leading to a vicious cycle of instability. But, there cannot be universal claims for harmonization without the detriment to the utilitarian nature of the narrative. Therefore, it is important to generate a system of working, much like how motivated by humanism, renaissance opened up the gates for the scientific method. Renaissance, valued the global nature of human reason, as capable of agreeing over common and objective things. This is in opposition to the then prevalent basis of mythological truth systems. The universal apriori nature of human rationalism was seen as an alternate to the mythology based truth system, which relied on selective few custodians and interpretors of the mythology. Thus, the universalization of human reasoning was the basis of construction of objective approaches to problems. This did not preclude the ability to humans to contemplate and reflect on things beyond rationalism, even Descartes was a devout catholic. Thus, the scheme of the scientific method attempts to generalize problems in terms of their components and representing them in ways they could be analyzed rationally as well as empirically verified. This marked a significant point of departure for humans in their solving of problems. It culminated in modernism, but was followed by several crises of conscience since the turn of the last century. As we mentioned above, it was the problem of an adaptive problem domain and an infinitely progressive solution domain. The idea of harmonization is normative and not in the metaphysical realm of science. Cybernetics attempted to reconcile this divide by formalizing harmonization as control and allowing an arbitrary elite observer the privilage to control both the problem and service domains, by imperative methods by insights gained analytically over the domains. This knowledge was supplemented to the scientific solutions to allow for a longer lived scientific problem solving. That is to say, scientific solutions hitherto given instantaneously and without regard to the dynamic aftermaths in the system (side effects) came to be conjectured in the medium term, as regarding such problems of control and stability as parts of the viable solution. The precision to which the control is to be exerted and the duration of regarding the validity of the solution remained arbitrary upon the motivated controller. In second order cybernetics, the motivated controller was removed from the picture and the whole metaphysical basis of cybernetics witnessed a dramatic collpase.
What followed the crises arising from the aftermath of having to reckon with a scientific method that did not regard its side effects, iconized in the atom bomb, that caused a  moral crisis to Einstein, there were several ways that holistic approaches were attempted. There were experiments in reality and culture in the aftermath as well, some motivated by LSD though. But the waves of neoliberalization and computational progress, mass media weakened the quest for the new science of Wiener. Instead, there was a shift to solve problems dynamically, without regard to epistemological reconillation and arbitrary boundaries. This has its problems. The shift to a monistic extreme, either in terms of planned economics or as laissez faire is difficult of conception. There is required a dualism in order to make sense epistemologically. The postmodernist, monist, dynamic approaches to problem solving presented crises to intellect and eliticism in many societies and on the permise of there being a sufficient motivation to consider alternates, that we present this method. 
This method has a metaphysics that hits an arbitrary middle ground. It is no more arbitrary than conventional science though, in its dualism of empiricism and theory as well as its commitment to humanism. We take the solutions to problems, or the method of problem solving to include a meditation on its stability in time. The dynamics of the problem and solution domain ought to be taken into account to an arbitrary limit before deciding on the solution. This involves conception along two important directions. The first one is the importance of dissipative structures in stability. In fact the extraction and dispersion of precious metals such as gold, serves as an excellent stabilizer of the society with no real value. Likewise one might consdier wars, defense spends, spends on cultural assertion and rituals as being stabilizers in the society. That is why when Elon Musk desired to propose a Martian colonization, his idea was to stabilize by divergence. Afterall, nothing stabilizes as a good war. Our list above, is in the reverse order of ethical desirability though and ordered by efficacy. The second dimension is the dissipation by encoding to scientific recursive truths away from iterative finds. For instance, an iterative solution could trivially resolve a Fibonacci series, while it is taxing on recursive systems. But an adaptation to a recursive container atleast in theory is required in order to encode it into a formalism. It might appear that encoding the solution in a humanly accessible format, is completely value free with regard to the sustenance and growth of the system, but it forms an important dissipative action. Thus, we rely on dissipative mechanisms to assure the stability of the system. But encoding is also a way to put a mystical controller on the system. That is to say, in spite of rejection of mythology in modernism, it in fact relies on mystical controllers in the nature of human apriori notions of justice, aesthetics and emotional sentiments as effective stabilizers. It allows for a supply of interrupts so that the whole does not run into an infinite loop of computing. 
This metaphysics to our approach summarizes the basis of an extension of a new science from the existing body. Now, coming back to the question of arbitrariness of the position of the controller in this method, we revert back our discussion to second order cybernetics. The question that was considered was whether a system having developed a certain degree of complexity, be able to introspect itself. That is to say, will some arbitrary part of the system become so developed as to have a frame of observation over the rest of the system. This might be expressed as self awareness or autopoesis (as in the gaia hypothesis). Once the system so becomes introspective, the next thing it does is to devise a method of control of the rest of the system. In fact humans attempt to control their body and not completely in charge. We have repeatedly seen the emergence of recursive controls in iterative systems as in multicellularity. Therefore, we might say, that a failure to build a science of an adaptive stance, as above, might lead humanity to a situation where the dynamics prompts an autopoesis and the result of which humans might descend to being kept in domestication, by more intelligent minds. In that minds are separable from bodies (as seen in extra life intelligence as with planetary systems), these minds might control the whole, leading to the extinction not of humans, but of human bodies or their being held in blissfully ignorant servitude as in multicellularity. Any rejection of the desriability of this state of affairs is essentially rooted in belief and culture, because transhumanism is no more externalizing than progenition. But, if one should take a stance there and desire for control, then one needs to fix arbitrary standards of what is development in order to work on dissipative mechanisms and epistemological tools to stabilize it there. In this regard, it does look that a divergence of the civilization as an ultimate progenition might help preserve the species, rather than evolving further on. Thus, this represents the struggle between species, as to preserve their base versus evolution. As for the question of how powerful such intelligent controllers of simulation, we might speculate that they might be as good as they have memories and computational resources. In fact we now tend to a theory that universe is made up of information quantiles, hence even such a past could not be ruled out. But our metaphysics axiomatically believes that there is no such a past, due to our theories of rational theology. Hence, we approach the problem as according to our cosmological approach of developing dissipative, encoding and progenition structures. 
To continue with the discussion, we might see that the extension of the scientific method to cover local problems. An extension of the scientific approach to cover local stability issues, helps both in solution angles as well as in generating a cultural dissipation mechanism. Secondly, it is possible that such localized science, can help in generating a finer grained scientific algorithmic body. Let us say, in examining the local dynamics of cretans in terms of their diet to estimate their longevity, we might either develop a science where we consider the cretan as the first class entity, rather than the human as the lowest level entity and proceed therefrom, thereby producing useful local models reinforcing cultural sovereignity. On the other hand,it might trigger a formalization of hypothesis on certain anlaytical components in order to produce a finer body of theory. This could happen if we could incorporate real world observations as controlled experiments.

The rational psychological dimension- Of labour and love

In that we have in fact dedicated a deep inquiry into the way we think of problems existing in reality and our solutions to them,more particularly dealing with the notion of method rather than the end in result. This, we argue that a goal driven problem solving by dynamic methods regresses in the long run, while a methodical approach to problems allows us better control and insight into where the progress is heading, that is not loosing the context of our human existential cognitive complex, or more simply our bodies.  But thinking from the view of the mind is not any less important. We might in fact be subjectively overrating the notion of problems and solutions, after all. That is to say, if we should look at the state of the world as it is now, we see that many of the improvements in life, knowledge, organization and health come woefully underappreciated. A lot of focus is on things that are designated problems which need to be approaches and solved, either by methods or dynamics. But it is a question of the mind of the observer, to know if the problem really exists or how it is quantified. 
Even in the worst polluted, crime laden city, with a drudgerous and dead end work life and uncertainty in our responsibilities and health, one could not deny that human and Gods grace is filling and rises beautifully, demonstrating so much a miracle in its mundaneness. This has been often the sentiment invoked by artistic reflection in movies such as ‘A winters tale’ or the short story, ‘the terribly, tragicaly sad man’. There is no denial of the rarity of life on earth and the infintiely precious blissfulness of the moment, even amidst the raging chaos, pain, war and subjugation. Therefore, one could very well see that the notion of problems is pretty overrated in our world. This is often been the theme of postmodernity, in its imposition of pervasive nature of our need to troubleshoot and remove problems day in and day out. This is seen as not stopping to smell the roses and simply solving problems all the time. This is particularly flagrant in modernity, where most often solutions are circular and zero sum (such as printing telephone directories as a career pursuit). But postmodernity had gone to question if ‘people are in fact fighting somebody’s war’ in the seventies which deeply offended Veterans and a general rejection of all -isms as being too pervasive and presuming of people’s preoccupaitons. But this was a theme in Max Nordou’s ‘Deliverance’ as well and the very phrase could be seen in it. Thus, it throws into relief if it makes sense to see problems and put work onto it and not simply live the moment, in innocence and bliss.
Therein lies one of the deepest problems of the mind, perhaps it is a matter of perspective than problem. This is on the edge of rational psychology to explain and make sense of. It is more accurate to say that it lies on the edge of rational philosophy, where it looks like the whole point of rationalism is knowing when to stop analyzing. If we look at the question if there exists pervasive narratives in the world, one which we should be preoccupied with and make it as a lifetime project or even as labels, such as a nationalist or a communist, the inquiry is interesting. There is a tough necessity for people to actually redeem themselves or at least emancipate others. This was the compelling need of people who are at the helm of development and was chastized by the authors of Development dictionary as a egotismal behaviour in promoting development as a tool of emancipatory self serving ego, of the white saviour narrative type, so to say. For, them under-development does not exist. It is simply a forcing of an insecurity over not having a purpose or cause by a developed population in trying to impute certain level of change on a purportedly under developed population. There are practical complications with the implementation of development actions, but here we address the question if the question of development is valid in principle. It is for the reason of having work to do, solving problems or emancipating that the world appears in modernity to be controrted, anxiety inducing and cluelessness outside of the work. It is not even a crisis of godlessness, it is one of demonlessness. The absence of having bad things to solve is disconcerting. One is forced to tilt at windmills to overcome this vacuum. IT is a fact that romanticism could not survive in the absence of a good war, a belief, an untiring warrior bent on noble truths. But one had seen the drabness and banality of existence, reflected as far back in Don Quixote or even in the song of Beowulf in western culture. 
There are needed eternal notions of good and evil, black and white and a noble quest in order to appreciate the romantic backdrop of a war won, a work well done. Even a sunset seems to have been earned in this perspective and one can sit and enjoy it to have been granted from hard labour or even if not one had worked for it, it is an appreciation of the grace of almighty to have provided these things as a divine gift. Thus, in order to be romantically moved, one needs to appreciate Godliness to world, work hard and hence be able to tune into miracles, as arising not from work. Thus, the element of work, virtue and a struggle is important to know what is a miracle and thus reflect romantically. These might spun out as heoric adventures where at decisive moment the good triumps over evil, by the hand of God. Therefore it becomes important to define our war, not simply battles as a pervasive problem in the world, on which one needs to put in hard work.  This is then the question of existential philosophy versus essential philosophy. In the former, there is either a singular truism or so many diverse narratives, that no dichotomy is sensible. There are just great range of shades of grey, which does not have any sense in sorting into goodness and badness. Therefore, in existential philosophy, burning desires, heroism and sagas have no meaning. It  is just an amoral market oriented world, such as the movie quote – ‘sucker for simulated hospitality’ in the movie up in the air. In essentialism, we see that there does exist pervasive battles between good and evil and one needs to be religious and ritualistic, honorable and brave in order to make sense of the world, or to actually experience the world. Many ancient orthodox culture narrate this need in order to experience the world and of the power of the mind to slowly turn down the experiential value of actions to the stage of samadhi, or alternately a KIA moment as being a voluntary exercise as the ultimate blow to the nature of things. 
The question is if these polarities could be reconciled. It turns out, they could be, at least from the point of view we hold and attempt to promote. If we see that there exists a problem in the world, as distinct from the perpetrators of goodness and evilness, then one is liberated from having to make a choice between existence and the essence. Jesus is quoted to have said – ‘forgive them father, they know not what they do’. Likewise, there are pervasive ‘conditions’ like the underdeveloped condition, the amoral condition, the apathy condition or more narrowly the indian condition or the African condition, which involve certain unaesthetic settings, that make the world less beautiful, while not directly assignable to villains who operate the conspiracy. These kind of conditions, in fact are pervasive and there is infact a pervasive need to fight a battle of redemption, against these conditions, which leaves us in tune for romantic contemplation. Thus, the duality of conditions and a singular puported pursuit of truth, might be a truism, but such a truism might itself be split as a plurality of conditions and a plural critical pursuit of truths, that hook into specific conditions and attempt to diffuse them. We might then think of a plurality of a finite sort, as a rightful expression of dualism or dualism itself as a way of experiencing life. The fight hence, at this point of existence, a valorous and critical inquiry into how far the force of greyness has reached and shine the light through it. It might manifest as a simultaneous acceptance of pointlessness  in our dynamic pursuit of local truth and a truism of a singular direction of unfolding of history, without the possibility of alternate histories attainable by prayers and hard work. 
Therefore, the struggle is one that is instantized and materialized at a given point in history, in reflection of the force of dualistic goodness. This is seen as having arisen in a golden age and remembered from time to time. What bridges the gap between fighting against evil and not making it as an all consuming desire is the idea of love. Thus, in fact postmodernity might be interpreted not as a dismissal of all narratives, but only of pervasive narratives. Those narratives, that are definitely limited in scope and it might involve a struggle to pursue justice in murky environments as honorable and not reductable in terms of rewards. What it rejects is the assignment of lables around such work day life pursuits, as becoming all consuming, because it gives that there is to an exist a truism of love, which could not be denied. Thus, one might take sides and fight a war legitimately and still not hate the infantry ranged on the other side. It might be a battle against conditions and not people. The battle itself might be just a necessary action, rather than all defining. It might involve beliefs and chivalrous fights which might lead to death. But living is not the whole point, in a sense. It is rather to love and labour. In labouring for ones ideas, one hopes and goes to a battle and in vindicating ones battle, one gains a romantic contemplation. If one should fall in fact, it does not leave him of the comfort of a work well done, of having put up a tough fight while not pinning ones existence to hatred. The existence of grace and contemplation might hence exist without labour. Labour is merely an instrument to help realize the grace. That fall might in fact be deliverance.  
PS: On explanatory parsimony, linguistics could be seen leading in many cultures, pointing to the structure of their understanding of concepts. Say about denotations of words such as banality which may not have direct words in more relaxed languages like tamil. The semantic richness in hairsplitting concepts in some languages promote an environment of over analysis while some language quickly renegade to non epistemic contemplation of concepts. Thus, an adaptation of linguistic balance that could righly reflect upon the stance on duality, inspired by the critical stance on the prevasive conditions at the moment in time and history would be important to attaining these goals, or rather to define a substrate for intentional actions towards these goals.


Elaboration of the method
First we will attempt to discuss solutions as equilibrium positions. In this approach, we might see that we are discussing methods in which we attempt to combine the scientific theoretical approach with the dynamic approaches. To elaborate, if we attempt to define science, we see that it is empiricism wherein we attempt to fit obervations into some axiomatic theoretical structure (cite Mendelson on what is an axiomatic theory). Thus, the requirement of axioms in a theory, manifests as disciplinary compartmentalization in the practice of science. In dealing with experiments and obtaining confirmations from them, we would need to start with hypotheses in the theory first. This works well in reality, because science attempts to explain phenomena and phenomena are in fact organized in some way. The essential organizaiton of phenomena could be modelled into scientific theory and applied models could be derived from them. The fundamental basis of application of logical methods to science relies on the fact that scientific transformations are akin to mathematical transformations. Just that there is no normative basis to mathematical transformations or application of functions to objects, science by its conservation laws, has no preferred direction or shape or transformation. Any given transformation could be effected, as long as the conservation laws are honoured and scientific phenomena could be treated as mathematical ones. Pure mathematical problems however deal with phenomena that are analytical, smooth and defined over a smooth field. These imperatives allow for modelling of phenomena such as classical mechanics and  electromagnetism in mathematical notations. But in the real world, the problems are noisy, in that the definition of axioms to a discipline causes the model to perpetually have some degree of noise to it, leading to the instability of the solution. 
Thus, if we see that a solution as a search for equilibrium, we might say, that we are in our dealings with natural phenomena, attempt to apply arbitrary mathematical transformations to do things that are in science non bothersome to the universe, or the global stability. But local stability is anyway disturbed .If a dam be put in the path of the river, as long as the conservation laws on gravitation and on pressure is honored, science has no preference between either path A of the river or path B of the river. But the immediate ecosystem of the river course destabilizes. This instability is local, but it adapts to regress the positivist fix to the problem to one of normative indifference. Thus, we might say, we started out with seeking a windfall from a specifc configuration of the universe, but the windfall is presumably a pursuit of justice subjectively. This subjective pursuit destabilizes the ecosystem, leading to compensatory adaptations to the extent that the windfall looses its value over time. The same is true with respect to the other forms of stability, such as social stability and psychological stability. There are local disturbances caused by positivistic interventions. We had proposed to integrate these local concerns to the fix in order to rate the fix, pinning on an arbitrary belief that the extension of the scientific method to such concerns is warranted of a separate theory in itself, which we deal here. 
Now, in that we have to deal with dynamic systems and we have access to positivist, symmetric constructs in mathematics and logic, we would need to look at how these two interact. If we look at any dynamic system, it tends to differentiate. That is to say, it propogates as a tree, diversifying in time. There is only an origin of speices and hardly any merger of species. The tree however is subjected to environmental disturbances of acyclical nature, which causes the extinction of some branches of the tree. If we consider both these together, we might postulate that the propogation of dynamic systems in time, when cut short partially by near random events, we would have it that from a certain functional dimension the tree becomes a more general graph- a network. That is to say, we might safely postulate that the cardinality of the set of essential functions is less than instantizations thereof (cite ergodic theory). Thus, even if an organism witnessed at a particular moment in history is absolutely unique, the same could not be asserted with respect to specific functions that the organism embodies. Thus, it had been convergence over functionality in a dynamic system, like the taxonomic tree. An apex predator is a function, which upon extinction of one specie, such as the saber tooth tiger came to be filled in by other species like the brown bear. Thus, the interaction of dynamic processes with sporadic uncertainty creates a network. The initial postulation of a divergent tree runs counter to the entropic law of homogenization of variety in a system, but we have to contend with the contradiciton as being local and supported empirically, in that even entropy is only an empirical law.
Thus, in such a situation, between disciplined sciences involving organized pheomena (such as rains and snowfall grouped as precipitation could be explained causally by some wind direction change) and the pure mathematical system encompassing static phenomena, we have a wide rift to explain theoretically. Here is the role of discrete mathematics. There had been tremendous development of this field to be able to model the interesting dynamic phenomena which propogate in time and form a network as it is worked upon by the forces of nature, has high degree of paralellism and self organization behaviour, has patterns of chaos and attractors and so on. The discrete mathematical field covers various reasoning processes involving methods that are not strictly analytical. This covers integral functions (to which analytical expressions are not known), statistics (which deals with discrete events though adapted to continual distributions and analytical expressions for computational efficacy), chains and networks over probability such as Markov Chains and Bayes networks, methods of simulation like partial differential equations and finite element method, underdetermined systems like systems of equations defined in matrices. Even modern notion of functions involves statement as arbitrary mappings between sets, rather than requiring an Eulerian continuity.  The use of computation to numerically analyze the system at hand, by techniques of optimization, delay difference equations, symmetry analysis by application of modern algebraic techniques and even stability analysis by convergence and divergence might be seen as the application of numeric methods to problem solving. These also covers techniques of simulation and combinatorial search. The area of numeric mathematics lies between the pure logical approaches, involving axiomatic theories in experimental sciences and the continuity and purity in mathematics. Thus the techniques of discrete mathematics, which might even be said as applied mathematics allows to treat problems which fall in between these forms. The models built using these techniques are however not analytically tractable, say by simply substituting values to algebraic equations. They could only be analyzed locally, say, we need to find shortest path distances between points A and B in a network, we might have an answer, likewise we can optimize to specific objective functions by convex smoothing and not get  a global solution for optimization problems. 
The development of these techniques hence provides some kind of analytical insight into process of dynamic and distributed nature such as biological and epidemological process. Thus, in terms of engineering, we might say, that engineering is a search for local solutions, or rather local equilibrium while policy is a search for global equilibrium. The nature of an innate pressure to seek solutions hence could be tied to the desire for justice and equilibrium and a preference of symmetry. The symmetry itself is supported in mathematics and the laws of conservation in the sciences. Therefore, it is natural to seek a symmetric rearrangement of historical systems to attain greater sense of equilibrium with respect to physical systems as well as interpersonal systems. Thus, in terms of stability theory, we might say that the muellerian duality of policy and engineering could be converged to indicate the pursuit of different degrees of stability. In our earlier work (cite) we had indicated that it might be an axiomatic premise that symbolically expressible knowledge is both necessary and sufficient for all business knowledge. To elaborate, we might say that business should concern itself with the search for unique positive solutions that could promote equilibrium in a system, thereby generating free energy. The profits ought to be the diffference between the search cost and the surplus. Thus business is seen as a fundamentally engineering endeavour, seeking local solutions by application of specific transformations selected from a set of arbitrary positive transformations. But in the modern period, the expansion of business around distributed settings has lead to the prerogative of corporations to be seekers of global stability akin to policy institutions, by governance and branding techniques. This is wrought by the limitations of theoretic methods to encompass real life engineering. The greater augmentation of numeric methods to engineering might hence restore the role of business as engineering entities. Even if it involves simulation methods they could be seen as organized researchers rather than policy oriented institutions.  
When it comes to techniques, we might see that it is important to understand positivist transformations from local stability points of view. Say, a certain column of a bridge is analyzed for forces acting at various points in the nature of shear, torsion and compression. The geometry of the column as a bounded structure (in that air has such low viscosity to be ignored), helps us use the FEM, by defining multiple combinaitons of these forces and their axes of operations to numerically test the stability. The test oriented approach is integral to Computer Aided design and precision engineering could be achieved in many areas due to these techniques. We consider if analytical solutions could be extended to other environments such as biological, if we can augment them with numeric techniques. As we had seen, a lot depends on the nature of the geometry of the system. Geometry of the builiding is assessed by the senses, while that of complex systems are done by indirect measurements and reverse computing.  Considering the finitude of essential functions to instances, we might be able to construct an ergodically limited network by following the progression of dynamic system, thereby giving rise to a finite network that represents the behaviour of dynamic and rich systems. The internal osciallations (like homeostasis), the sub functions that operate within the network (as progressive conditions like ageing) are material to the introduction of solutions into the network. Thus, if the network is treated as the geometry of the system, then we might use mechanistic notions of elasticity and proximity to determine the future stable states of the system pursuant to introduction of certain nodes that are rigid (such as a receptor blocker, say). These reinforcement to the system by blocking oscillaitons or boundaries, might help predict the system behaviour in time. If these numeric models could evalute positive functions, we move to determine with greater certainty certain analytical solutions, as we have some kind of a pre-experiment from the topology of the problem and solution domains. 
Thus, we might say, it would help in the promotion and sustenance of the analytical methods, rather than purely numeric and dynamic methods, that rely heavily on local equilibrium.Thus, if a business is to rely exclusively on local equilibrium (with analytical formalization at all), it would dispose the system to chaotic dynamics and the loss of epistemic handles. There would be an overwhelming move towards policy and subjectivity leading to reconcillations on the basis of intersubjectivity and self stabilizing paradigms. This might make the analytical method irrelevant and progress intractable. 
PS
What we are dealing here has a greater emphasis on methods and metaphysics than the technology of achieving a model of dynamic coefficients to static solutions. Most often, normatively motivated notions of fairness and duty are enough to solve the problems heuristically and in fact a lot of people solve these problems this way. Thus, we might say, even if we formulate our position to seek notions of good and bad, of strong epistemics to support a romantic fodder, people had been solving this all the time heuristically. Perhaps motivated by our extreme scepticism we attempt to formalize this solution. 
Scientific method and applied dynamics- A crisis of empiricism
We had been looking at ways in which the scientific method could be strengthened by a revisit of the method and metaphysics involving dynamic techniques. This leads us to an examination of dynamic methods in particular. We had seen that science reasons about the particular, for which reason it relies on axiomatic theories. The presence of axioms make the disciplines qualitatively distinct from each other, leading to an examination of particulars as the preoccupation of science. On the other hand, general things could be discussed mathematically. Mathematics concerns itself with the general layout of the universe in an abstract sense. While science in fact uses mathematical methods for analysis, it uses it only as an analytical tool, leveraging on the belief of a fundamental mathematical fabric to the universe. Science strictly concerns itself with facts and their logical ordering. Even where mathematical techniques of analysis are used, the concern is with respect to pure phenomena such as electromagnetism or classical mechanics. One might deduce that the time taken for a ball to roll down a frictionless inclined plane decrease with the increase in the angle of inclination, by the use of mathematics that allow us to deduce the component of the force acting downward and acceleration as being capable of integrated into the velocity. But in order to confirm this fact, science would need us to perform experiments with balls and inclined planes, making allowances to friction and other disturbances. This is to ensure that there are no specific situations in which the formula would not work, perhaps the relation stops at a cetain threshold of gravitational pull, for instance, or that there are more fantanstically, listeners in the ether that establish this correlation from time to time. We never know. This is what the ultimate scepticism of science and such a necessity for sensorial observation had lead to numerous interesting scientific truths. Thus science compels that both the mind and body is in the same page, that is to say, we ought to see what we deduce. 
In those cases where the situation concerns itself with stable systems, mathematical techniques could readily be employed, such as when conservational laws are in action, we can postulate the existence of heat sources and sinks and formalize the Carnot engine, say. But where the ultimate aim is to observe an equilibrium which is local, there are specific challenges. That is to say, where we attempt to study phenomena in general, which does not fall under any of the discipines directly, we would need to use the analytical techniques from discrete mathematics which straddle between logic and mathematics. The availability of the analytical tool, allows us to build models that can deduce local truths, like shortest paths. The ability to represent a high dimensional space structurally, so that certain physical conservation laws such as that of energy (as applied in Lagrangian mechanics) could be applied and from there we might deduce the possibility of positive transformations or engineering, allows us to work directly with phenomena without the need for axiomatic theories. 
Let us first examine the nature of solutions, we seek in such situations. Let us consider a dense forest, where there is ravine carved by a river that forms a barrier between two tribes exchanging goods. A person that attempts to cross expends a lot of waste heat, trekking downhill and climbing on the other bank. Let us assume, a clever tribesman sees amongst all the trees and debris, a fallen tree trunk in good condition. He gets men to pivot it about a point to lay across the ravine, allowing people to walk across. Now, this bridge saves energy by orders of magnitude, than what is required to pull the tree trunk across the ravine. This, generates dynamics where the waste heat goes down, trade flourishes until some point where both tribes generate waste heat either as ritualistic frolic or by arming themselves in warfare. But progress is not futile, in that between these points in history a lot of orderliness had been created by the thriving trade. So, let us presume that progress is not futile and it could be achieved by bridging apparent gaps in the efficient organizaiton of a certain phenomenolgical space. Here, we are ignoring one important component to the work done. In case if the tribesmen are blind and deaf, the extent of work it would take to search the forest floor for useful bridges and the error correction that is needed in tactile communication to ask tribesmen to pull a painfully discovered bridge tree trunk, would involve an energy that is much less profitable. The optical faculty, on which much of our observation relies, actually does all the computation for us, making observation so easy that the inefficiencies could be trivially located. 
If we move this analogy to a domain involving non spatial dimensions, we might see that it is possible to apply apriori truths like energy conservation, inertia and symmetry to derive the topology of the problem space of high dimensionsional phenomena, of which we have observational ability only to indirect clues. Thus, if we use applied dynamics to infer from data samples from a problem space, we might be doing what our optical faculty does resulting in an exposure of blatant inefficiencies which could be trivially corrected in order to stabilize the system. In fact much of what is empiricism and experimentaiton relies on optical observation and measurements. An experiment has an arbitrary starting point, but the end point is always an equilibrium, beyond which there is nothing interesting to observe. Where we apply discrete mathematical models to a problem space, we might be able to deduce local truths, but the verification side of it is problematic. Hence, to see, as is a requisite for the scientific method is difficult while dealing with phenomenological studies. That is to say, if we look for local equilibrium from an analysis, we would be able to deduce it from the model, but to observe it we would need to make sure that we have control over the system. That is to say, if we one is to observe a locally optimal path and if one somehow attains the material dimension to the problem say a biological system, to perform an experiment, the time lapse to setup and run the experiment would make the local truth obsolete. This is because, we seek fine grained truths from complex high dimensional models. In general, whenever we consider the phenomenological science, which is now called data science, the method is to deduce local truths and directly apply it in an open loop, rather than experiment with it. Thus, the suffix of science to datascience itself is disputable. The human approach to problem solving was never purely dynamic. It is in fact mystical, in that it attempts to promote truths that are universal and global, rather than to clinch local advantage or selfish advantages. Thus, over time we might expect the tree bridges to evolve as infrastructure and this attempt to define global truths forms the basis of the narrative of human progress and justice. 
If we are to investigate, if the whole method of applied dynamics serves only as instrumentation, much like the microscope or reagents in chemistry to indicate the state of the system to a granular level that is available for visual detection, we might see some preservation of the scientific method. If the whole dynamic science allows the system to be interpreted or as in popular literature, we go for data visualization, then science is better served. The possibility of visualizing globally a highly dynamic system at a scale where apparent flaws in efficiency could be made out is however questionable. We know in statistical analysis we seek very local truths, such as a normal distribution in a linear quantile selected from among a million other dimensions. The whole high dimensional material might be a regressive and random cloud after all. If we ontologically postulate the existence of a macroscopic material structural pattern to the system in virtual space upon high frequency observation (which might be the same as high resolution observation), we might have a possibility to state the problem domain in formal well posed scientific problems, approachable by analytical methods. Thus, if we could visualize disease processes as dynamic pathways of pathogens and immune system agents, we might just be able to drop a few advantageous bridges that makes the game less dynamic and more controllable. The question is that if such complex systems could be deduced to such observational granularity, to present the problems nearly statically. 
The question is hence one that concerns the nature of reality. In the aforesaid forest, if we are to be deprived of our advanced senses and computational power, the speed at which trees fall and decay, makes it impossible to find solutions such as using them to bridge ravines. It is our high frequency of processing with regard to the pace of change of the forest floor that gives us the advantage. Hence, it appears that by abitrarily increasing the speed of data capture and analysis, we would be able to find evident solutions that could stabilize the system to new states of equilibrium. But at a very low level, it is difficult to prove that objective observation is possible globally. Quantum states are notorious for this effect. In case where the observation influences the system, the distinction between the observer and observed or the possibility for objective observation and hence empiricism is endangered. In fact if we see that things change when observed, suggests that there is no way to exclude a simulation, where a master operator moves things as we observe. Thus, we might say that the computational complexity of the problem space, in relative terms is material to the notion of science. That is to say, if we consider the computational power of the tribe vis a vis the rapidly changing forest problem space, it is difficult to undrestand, even if the tribe is able to find the tree trunk and lay the bridge – how they would handle the solution. Given the bridge exists, concepts like trade and urbanization could not come in their grasp. If we should consider the microbiome of the forest, we might see that the relative computational complexity of the system defies our apparisal. If we should use techniques of dynamic analysis to find possibilities for concrete bridge like solutions, the stabilization of the system has little meaning to our exploitation. The system itself is too short lived to be capable of showing any lasting stable state for utilization. For instance, if we could add an agent to the system to make the forest floor more permeable to water, it would require that the computational probe of the system renders some apparent logical inefficiencies, which though the system is at high frequency is long lasting and repetitive enough to allow for mechanistic fixes. 
The possibility of non mechanistic fixes, is what is baffling. If we use active fixes, we would leave the system to be stabilized on the go. We would see the result of our intervention as well as the system entwined, making the whole observation open looped and only pseudoscientific. This hence poses the problem if the mechanistic approach is indefinitely extensible or that we would need to rely on active agents who would address high frequency systems locally optimizing them for solutions, so that bridges are laid by tiny humans who know how to reap the solutions in high frequencies. As a result of such active agents becoming part of the system, the system might become productive as a whole, such as adding earthworms to soil. Thus, the problem is fundamentally epistemic. In case if we employ incremental computing power to map the problem domain, do we expect to see potential global , static machines or do we expect to see very narrow local optimizations, which can only be handled by highly local machines which come with problems with us being able to verify such active machines. It might even be a function of investing in experiments which might unravel static mechinization potential if the system is observed over long periods. If no investment is preferred, quick fixes by active agents might cause a different kind of solution to be implemented.
That leads us to a different kind of question, that whether a high frequency system could be augmented with sensorial resolvers, that could interpret the system for us and whether macroscopic governance of such systems might be possible using mechanical notions. We have done that with microscopes, which render the world for us and based on which we can control the domain with mechanistic agents. In case where the system is not mutatis mutandis shrunk spatially, but complex behaviourally and operates at high frequencies, we would be presented only statistical summaries of the solution (in the spirit of statistical mechanics), such as the probability of finding a certain pathway designated by an edge. This statistical mechanical representation of the system, might require that the system is ergodic in terms of repetitive actions, which maps a few essential functions to multiple actions or demonstrations. If such a scheme is in fact present, we might apply statistical mechanical fixes, without the need for active agents to solve problems. That is to say, to reach equilibrium within an experimental frame and thus have an independent observation coupled with theoretical deduction from an applied dynamic schema. The resulting solution might stabilize the problem space to an extent that is linearly improved by our observational capabilities. Likewise, slow systems that defy our understanding like astronomical or evolutionary ones could be controlled by statistical mechanics by relying on general formats of the problem space and applying mechanistic fixes.
We might consider that even transformations could be structurally represented and not simply osciallations. Ongoing continual transformations like ageing could be built into the model by Functional analysis or the calculus of variations. Likewise, the mechanistic reduction of non parametric models have been dealt with in the do-calculus of Pearl. But it is also that Pearl thinks of using causal inference derived without experimentation, by plain observation only, but we on the contrary look to build full parametric models from which we can deduce with certainty and do tests only to verify from complex systems. Pearl is concerned that it might be ethically infeasible to run some experiments, because of the weak nature of hypothesis, but if we are able to build strong parametrized models by the aforementioned way, we could infact attempt experiments. Again, it relies on the premise that the universe is fundamentally simple and mehanistic as against a notion of essential complexity. 

On Preserving the Scientific Method
We had reinforced that mechanistic models, or parametrized or analytical or reductive models of the universe could be made ad infinitum, potentially. We had also of the opinion that complexity is partially a synthetic construct, that does not occur in nature. In the wild, it is always pseudorandomness and not a full blown pure randomness. It does however exist in the mathematical universe, just as infinity and null exists. The general combinatorial complexity all arise because we define the problem that way and not inherent to the problem. Thus, we still uphold the validity of classical mechanics for most purposes. The method of objective observation and logical theorization are both incomplete due to the uncertainty principle and the incompleteness thoerems respectively, but they serve as useful incomplete models of the world. A great deal of the workings of the universe could be emulated on them. Then it implies we believe the universe is made out of simple, elegant mathematical constructs, with minor imperfections, enough to keep perfect understanding at bay. This dose of mysticism is an essential part of an essentially simple universe. Hence, it is best to accept it and dedicate progressive efforts to the solving mechanistic problems further. It is rather a cultural wall that humans have hit when it comes to solving problems reductively, rather than a real wall. 
Thus, we might say that science is still capable of solving complex problems. The method of logical sensibility and sensorial observation, repeatedly assert that the universe is logical and humans have a special power to observe the universe in action. As for those areas where science is helpless, we would have to improve the analytical methods to build better models, rather than resigning to the fate and going the dynamic way, which is the way of all creatures or more generally of complex dynamic systems. The present developments in datascience could be analyzed using discrete mathematical techniques and methods of analysis such as calculus of variations. Even ordinary calculus was invented as a way to build analytical models out of a fluctuating system. Therefore, it is not that in the past walls were hit in system behaviour. The method of science could be made relevant to complex systems, by proper instrumentation using datascience methods rather than a dynamic stance. From here it follows that the human position is special and hence the universe is probably anthropic. In which case, it would follow that human progress is tied to the essential notion of ‘tool users’ and mechanics. The idea is that the use of tools in the material dimension liberates humans from finding solutions by synthesis, dynamics and dissipative stabilization in the realm of the non material, such as policy and spiritualism. Thus, we consider the use of science to the governance of human systems recursively as a non science, which would then include economics and management. It is in science to deal with only material things on which an objective frame of reference could be established, even if imperfect as in the case of biological or meteorological systems. Policy engineering would then be likely pseudoscience. 
Given, that specific essence exists to the human nature (disucssed as our niche in the cosmos by James  H Williams Jr cite) , as tool users and cultural creatures, we would then have a consolidated identity of being human. If this is then specific, then does it bound us to an ergodic fate might be an anxious question. But the infinite expansion of humans in terms of experiences could be carried out on this profile itself. In fact to make sense of such growth a fixed identity is required. Given that the universe is designed for human use, we might then conjecture that humans are unique and created such. It would be more general to think of life as bearing a fundamental spark, which would have been delivered as a fundamental ‘interrupt’ to the processing and a set of apriori values with respect to things like symmetry, inertia and conservation. This might explain the repeated emergence of homonid species. It might just be that life is unique rather than the uniqueness of humans. It might explain that in the case of humans, the case is special, in that there is a near complete shift to an imperative for objective progress, rather than local solutions. There have been an abandonment of genetic success in favour of empathy and development of knowledge. This is the reason why science emphasizes on objective knowledge and makes it an infrastructure over particular engineering development. The growth is attempted to be made into games at higher domains rather than being based on lower domains. We had discussed (cite) in our earlier work the nature of homologies between problem domains. We might have reached a domain, some kind of a liberal domain,  where it does not make sense to progress, but only stabilize, where the concept of peace and love holds good and sufficient. The imperative for progress is to be reflected upon in a critical moment and that is what is the idea of this work and the necessity to overcome entropy which is a spontaneous and directed phenomenon. Therefore, progress ought to happen nevertheless and arbitrary. The question is whether progress among living systems is different from general dynamics.
In such a case, it would be essential to carry over the spark from one level to another by a preservation of continuity form our initial state or that we can attempt to recreate the phenomenon. The latter would model the universe as having been put together by an indifferent entity rather than a benevolent maker. If we consider the universe to be ordered and benevolent, then we would like to preserve the considerations of our body. But in that we have argued that complexity as an illusion, it might happen that humans might upon assuming enough computational power be able to simulate their body. In such a case, just as with multicellularity, it might happen that humans could evolve a consolidating entity. Even at present, humans are able to network to such an extent that virtual entities assume almost bodily forms. To define multicellularity, one might think of two sides of the control, for the whole organism, a traumatic damage sets the cells to extinction. Likewise the interrupt of the cells arises by their ageing and the definitive bounding in time of the organism. Thus, the interrupt or the finitude of their computation had been determined by the ancient prokaryotes which went into as mitochondria into eukaryotic cells and thereon to multicellular organisms. This essential spark, if it could be ported into a system is an interesting question. If the universe is mechanics plus an essential spark, then if we could play the role of cells in a multicellular creature, could we port our consciousness to a new system. Say, the new system would be specializing functionally by say, allocating land to cultivate, allocating land to defense in the level of a nation say, then the equation of the whole collapsing on the cells as well as the cells interrupting the whole by some process is preserved. Then is the conscious capable of being transferred beyond carbon based schema seems to be a valid scenario. If the human conscious is the highest level of consciousness, then the question as to why a possibility exists is a perplexing question. It seems that if we should have a free agency, we would have an agency to port our conscious, or simply to destroy ourselves and it would lead us to the Camus dilemma (Myth of Sisyphus cite). 
Thereupon, we propose that should arbitrarily limit our liberty, in attempting only to do tool based fixes and being loyal to our bodies, rather than attempting a greatness where we shift our consciousness to greater entities. If universe is anthropic, human consciousness is the highest level of consciousness, in which case an experiment to port to an external system would regress to the human state. But that is not enough reason to withhold the experiment in transhumanism. One can only say that it is wasteful and we might bask in the summer of our current situation. That would be an apriori statement on the level of progress with no possibility of falsification, making it a non sceintific truth, which indeed it is. Hence, the belief here is to support the point, as some kind of orthodoxy to hold on to certain positions and considering it to be suffice  for all possible uses. This notion of ‘possible uses’ is operative, in that, the for all ontologically concievable problems could be found in the present state giving no reason for us to embrace the so called progress. All things then that attempt to embrace such progress is bogus and unreasonable, relying on poorly posed problems and their half way answers, like the paradise of trisanku or of the myth of Midas. But when it comes to pure experimentation, one can say little about finding the path, other than to object it as a matter of belief. But for a world that is still fraught with real problems, the position that such a higher order progress provides a solution is a worthy fallacy of being intellectually decimated.
Hence, we might summarize our belief as 
The scientific method is to be defended against purely dynamic fixes. The scientific method is incomplete and useful for the society to port problems to the cultural domain. The universe is mechanistic with an element of mysticism. The mysticism is important for the identity of humans and is transcedent. The development of science, automatically enables development of transcendental faculties. There is a benevolent order to the universe. Thus, in brief we support the Cartesian dualism of the presence of a mystical element, which is unknowable and a rational element which could be observed. We acknowledge that such dualism had in the past been extened to social progress in imperialism ignoring the effects of reflexivity in governing foreign populations and similar tranregressions. But in this present era, in the post modernist world, it suffers a crisis. The monistic constructs in quantum mechanics, postmodernist culture, game theory based policy, expectations on distributed emergent intelligence presents a specific extreme position, which ought to brought to debate and hence our assumption of the position as defenders of the scientific method and dualism is zeigteist for a higher calling, to which we have no epistemic way to state. Therefore, we posit that the present crisis of postmodernity in terms of lack of control over environmental issues, the problems with runaway corporate economics, crises of computational superintelligence all arise as a result of a destabilizing course from a stable state reached by the critical role of postmodernity to the cultural phenomenon of scientism that culminated in the world wars. The critical role had been overdone and the destabilizing course it is felt might lead to a new equilibrium from which there could not be an easy reversal. Such a new equilibrium might involve high degrees of machine intelligence is our speculation. Hence we wish to dampen the course by applying a critical control by invoking the methods of dualism altered to accommodate modern developments in technology.

A case study in development
If we examine India from a development point of view, we need to examine its history. India is a peninsula with the largest quantity of arable land, which commensurates with its carrying capacity as the second most populous land or if we are to take the subcontinent, as the most populous region. It is also the poster child of underdevelopment. Geographically, India is a fertile region well bounded and irrigated with respect to the surrounding steppes and unfriendly landscape of the west. The incursion from the west via mountian passes are a regular part of the history of India, due to the attraction it provided to the powerful and aggressive tribes that inhabited the steppes. The threshold when such a tribe might enter into the subcontinent moves from being an occupying power in a region to a complete integration and assimilation into the landscape is a period of high degree of uncertainty. Likewise, the climate of India is dependent on the monsoons which makes it a fickle host while being generally generous, suddenly prone to withdrawl. In this environment, it was first the people who settled down in the western river valleys that displaced hunter gatherers of the inland, likewise to be displaced by the tribes positioned in the Ganga yamuna doab. The first displacement of munda speaking tribes by dravidian speaking tribes is prehistoric, but the latter displacement seems to be one of high degree of absorption. Many legends of the new civilization that flourished on the ganga plains, featured coloured Gods and heores, such as Krishna. There was in fact a clear seperation of languages and customs that allowed for certain eliticism, but it was not too restrictive. Therefore, the direction of progress of the civilizaiton was well rounded, many philosophical attainments, empire building and cosmological plural philosophies, scientific development was significant during the period. This might particularly be seen during the era of the Mauryas circa 300 BC. 
This period was much like the renaissance period of Europe that went on to produce enlightenment and powered imperialism finally leading to the world wars and disllusionment. The concept of renaissance involved a rediscovery of the older works of the Greeks, particularly of the Platonic tradition, aptly Plato himself discussed knowledge as being only rememberance of some golden age, when we actually had all the knowledge apriori. It also picked up from the Aristotlean tradition of teleology of the end, while the method is what is left to be refined. The Platonic approach naturally created space for romantic wonderment, through the concept of a golden age of the past. The renaissance is difficult of being seperated from the development of technology. If we look at the case of the enlightenment it involved the development of dualism, as the essence. Descartes postulated the existence of clearly seperable notions of tractable observable phenomena and the intractable mystical observer. This also provided the platform for romanticism, alongside the possibility of scientific inquriy. He properly settled the apparent contradiction between spiritualism and science. We might infact look at the very essence of civilizaiton as being the ability to hold this duality, one of objective observation and to hold in reverance, the privacy and the mysticism of the individual away from rational inquiry along with his practice of mythological rituals, spiritual and hedonic actions. Thus, we might say that the civil society was formalized in the enlightenment. But it is ironical that no civil society could exist without a military society. While the essence of civil society to to allow for rational thought to coexist with mysticism, it is constantly exposed to more efficacious ways to work through the problem. Therefore it required a military organization to defend it in the long run. If we consider the essence of this mysticism to be the origin of liberty and the essence of the objective positions to be the origin of the concerns of equality or justice, then we would need to look at how this contradiction can sustain. This is where the military might comes in and in order that people live with that, what they need is an overarching meaning of the fight. If they are to loose their life, liberty or limb in a battle, there is a need to appreciate why the battle is for, there is a need for chivalry and glory in winning and loosing battles and this could only be constructed in a secular way. That is to say, a certain truth that is overarching apriori and not constrained by local mythologies and religions is important for civility to sustain, mainly in giving meaning to military conflict and defense. This overarching secular morality is best captured in the notion of ‘fraternity’. Hence, the importance of a secular goodness or morality is important for civility to sustain, which then prompts the need to reflect on what is bad, or even if depersonalized into conditions. But it is not always that reserves for such secular morality exists and that defines the critical point of development.  Thus, we might dare say that the notion of renaissance is not a unique thing in history, but the European renaissance might have been the best documented one and the most recent one. 
The Indian situation post such a glorious period under the Mauryas, started detoriorating under the Guptas. We are speculating here in terms of the Gupta’s preference to mythological, rather than rational truths and deriving from very narrow sources. But it might be that under the subsequent dynasties than the Mauryas, the general state of affairs was not so encouraging in terms of universal brotherhood. Genetic evidence suggests strongly that the caste system became cemented into the society roughly about the period to rule for over two millenia unfettered. Thereafter the subcontinent assumed a passive attitude to civil organization and glory in general. The southern subcontinent was ruled over by strong dynasties which probably promoted secular credentials, due to which the epicenter of cosmological philosophies like jainism shifted south about the time. There was however a growing incursion from the west starting the Iranic sultans and the turcic sultans about the turn of the first millenium, which made Indians more comfortable with accepting what comes up, rather than shifting loyalties and standing by honor. The volatility of the situation increased as fragmented fiefdoms fought all through the 11th and the 15th centuries. Thereupon a few strong consolidators that rested claims on large identity based formed, prominent among them are the Moghuls, the Vijayanagar Empire and the Marathas. The bhakthi movement that took over in the south in the 12th century onwards criticized mythologies and opened doors for philosophical debate and some amount of progress. But the general attitude was one of resignation. It was worsened by the European incursion and the shift in the dynamics owing to the catalytic effect of foreign presence. At least the rest of the invaders slowly shed their foreign power identity, but the Europeans were staunch on governing as foreign powers. 
The notion of development arising from the European renaissance was thrust upon the Indians. Actually post the world wars, which ironically caused Europe to realize how powerful paradoxical effects to rational statements could be made out, there was an embrace of the Indian way of approach to the world. The post modern philosophy in Sartre, Camus with his absurdism and many other approaches that directly worked to weaken the position of absolute observability (well backed by science in quantum mechanics) in social sciences and economics lead to the desire to associate with the Indian spiritual way in the 1960s. But while the European system was still ambivalent in the early 20th century, and the war was brewing, Indians were progressing and forming civil societies along the standard form of secular overarching morality, equality and liberty as a new renaissance under Gandhi and Nehru. The divisive politics of british as tactical tools post the mostly muslim mutiny of 1857 had a ghastly aftermath during the partition. The Indian approach to civil organization worked mostly on letting things self stabilize. The roads are riddled with potholes, to the traffic moves slow, but vehicles count is less, leading to roughly similar travel times. The rules don’t operate as strong, but people coordinate adhoc and the crime rates are as low as in the west and similar numbers are achieved in accidents. Thus, the road to peace for Indians was through chaos, rather than through organization. In India hence no great mafia formed nor was there such strong dictatorial tendencies. The road to stability through chaos seemed to be validated in the western postmdernist philosophies as well. Then, we might say, that it is a globally valid condition and not what might be called as underdevelopment. This is discussed in postdevelopmentalist literature extensively. 
But is it not to be criticized is an important point to take up. The attainment of stability through chaos and distributed planning, where everything happens by local equilibria, where the boundaries are highly porous and ambiguous, however causes a passiveness which is likely as unnatural as an aggressive posturing presuming to be armed with the ultimate truth. The past of India was riddled with foreign occupation for this reason (which was assimilated and responded in the society, with scarecly any bloodshed, as with the Vijayanagaram expansion into the hinterland). Likewise, it might not be unfair to suspect if this approach would in the long run give rise to oppression. The same might be happening in our current phase of history as computational power rises along with mass media to modify the world, without active participation of the people. We might say post the 70s, the world quite impressed with the non committal and passive nature of nationalism of India, adapted a stance of neoliberalism, wherein no specific identities was required to add value and transact. In fact one might surmise if it is the television that caused the decline of the planned economic policy and helped unleash the truism of market economics world wide.  This however met a disillusionment in 2008 economic crisis, followed by a resurgance of local identities. The point here is that the global notions of development might incorporate both these positions, one of passivity and observance and the other of active proposition and defense. Not all historical circumstances allow individuals say like Da Vinci (who helped see the unification of romantic art and science through the lens of overarching morality) to shift the course. 
But nevertheless we make herewith to generate a spark which might proceed to create a renaissance in our present society by enacting the overarching notions of fraternity. The method by which the attempt is made by speculating on the intimate connection between science and morality. We might say that renaissance was powered much by the enlightenment rational notions being both satisfying and practical. Likewise, if we can formalize and defend the enlightenment principles of science in the modern period of distributed intelligence and dynamic action, then we might probably generate enough critical steam as to shift the course of history into a renaissance. It might even be scientism to put science as the kingmaker, but without the dogma too, it is hard to dismiss the instrumental value of science to its contribution to the highly civilized society. 
To summarize our position on science might be that which is further summarized in the principles and mission of a certain foundation in one of our later essays (cite)


















Is Culture Narcissim?

Often seen in british literature in the nineteenth century or Victorian england. There are so many references and self reflection as a person standing for some purpose, or a complex of place, climate, fashion, heritage and philosophy towards science and the cosmos. The development of culture is important as an arbitrary strong hold to which to legitimize ones battles and propoganda. One knows it is a fair and just thing to do. But if a romantic culture could transcend particularities of historical sense, such as those of language and climate is an interesting topic. If one is attempting to create the English culture or something it stands for is pertinant. That is to say, are cultures mere referenes to some more powerful concepts. This puts us in a plane of reasoning that it might be that people would wish to recount their glories via media their culture, which makes it more of a narcissitic endeavour. But it might also point to a greater glory of the maker, who had set the initial conditions and the cognitive conditions that allow such reflection. Thus, narcissim and devotion are practically indistinguishable and that leads to a lot of practical problems involving god men. Over time God men, or even religious people become so much lulled into narcissim that they end up doing stuff that an ordinary person unintialized in the wisdom of Godliness does not contemplate. 












Solution Apporach
A Redraft of the deed of trust
==========
The leafycampus foundation for studies in information technology and development is constituted hereby as an institution to pursue the ends detailed out hereunder:
To constitute a dedicated institution consisting of people, facilities, principles and a hinterland of people, working systematically without being pegged towards the pragmatic notions of material enrichment. This institution shall primarily work intellectually to acquire knowledge in ever increasing reflection of its pure form and base it to create pragmatic adaptations, so that consistency is advocated as a higher ideal than completeness. 
The completeness of social groups is to be attempted of materialization by complete knowledge based productive centers which are embraced by specific social groups which define their completness in cultural and artistic terms. This could be achieved only by a context bound definition of development.
Hence development of specific social groups is to be attempted by intellectual pursuit of pure knowledge, standardization of what constitutes the bounds of consistency for an applied system and leaving the incomplete areas to be managed culturally and by spiritual contemplation.
The knowledge might be the base for pragmatic adapatation resulting in productive free energy in the social system.
The standardization of consistency of such adaptive systems might arise from a tolerance and entertainment of pluralism over an intellectual arena which has due processes of critical examinations and peaceful processes and reflection of universal notions of morality.
The instution shall play the role of emancipator of specific groups, concrete or abstract by inspiration, publication, experimental and model projects.
A redemptive and context bound notion of development is proposed in a critical frame to the postmodernist construct of feedback driven, unbounded and highly subjective notions of progression of human activities, leading to emergent unifying entities, that cause fragmentation distress to individuals.
The authority of the institution is aimed to be moral and it is concieved as an idealistic one delinked to practical, material considerations.Its energies might however be directed to immediate humanitarian work even if it does not involve intellectual dimensions.
This is critical of unified notions of development over incremental problem solving power, the ability to produce greater choice to the individual while the costs themselves are collected by means that are neither transparent nor tractable. 
This engineering of unbounded, action cross-messaging platforms with minimal policy orchestration, it is felt might lead to emergent, intractable complexes. These complexes bind humans to closed infinite loops of production and consumption, with intelligent random moves migrating into the network. 
This is a metanarrative of the individuality and pluralism of narratives of development and the singularity of objective reductive knowledge, itself contained recursively in the metanarrative of plurality of ideas, openness to crticism and peaceful methods.
-
There are risks with this approach as well - conflict had arisen in the past not only out of resource sharing but also from egotism of identity (we can ontologically seperate this from evolutionary concerns). 
Cultures are identity reinforcing, independent of narratives of development. In Postmodernity mass media, consumerism and urbanization disperses culture into monotones, with microcultures emerging as transcient and forceful from time to time. These microcultures are vigourous and celeberatory or of anonymous heroism, rather than sustained in tackling problems. There is also a merging of leisure and work - as with standup comedians, representing a more general ambivalence characteristic of postmodernity. Emergent microcultures, as city belongingness satiate identity needs while being benign and open. 
We argue for restoration of leisure, work differentiation, concrete permanent identities negotiated and conflict mitigated by meta identities that overarch and sustain balance. It demands an optimism in human conflict resolution abilities, of Chivalry and Grace. It demands commitment to institutions and acknowledgement of imminent conflict, capable of resolution by deliberation, specific appointed venues like sports meet, fair towns and overarching ideas like regional agglomerations.
We present hardening of recursive structures as natural and emergent and hence their explicit construction as being desirable, as just one point of our critical representation to the general trend towards postmodernist homogenity.





Elaboration

This knowledge would be consistent and objective and be the foundation for pragmatic engineering adapatations. The development of pure ideas is the realm of idealistic institutions. Once these productive ideas are put into use, there arises in the society free energy which furthers the imperatives of growth and incremental stability of all living beings. This energy is to be appropriated for reflection on how the energy is to be applied, which include ways in which such energy is dissipated by bounding off rational enterprise to certain arbitrary bounds of consistency, so that there are not made far reaching completeness propositions by arbitrary agents. 
This enterprise of administering the standards of consistency and its boundaries is important in that it embraces distinct knowledge enterprise and bounds it from the pragmatic enterprise out of rigourous epistemological examination as human relationships, art and culture. 
The development of such standards, their administration and delivery of the philosophy which upholds an arbiter of the society as an agent of defragmentation of a geographic, cultural people, by providing opportunity to generate surplus from knowledge as well as delimitation of the knowledge enterprise from assuming high degrees of completeness at the cost of consistency, which is often by fluidic categorization and open feedback loops. 
This arbiter does not derive authority on the society by political means, but by moral means, where individual dilemmas are better resolved by referring to the leadership of the institution. 
The institution proposes scientific atonment and standardization of knowledge by its actions, inspirations and publications thereby allowing people to boldly defy complex undercurrents in the highly  globalized, market oriented, postmodernist, technologically advanced society. 



This is expected to produce in the long run, self contained unique entities which consist of clearly delineated and well formed institutions that support the pursuit of problem solving in nature and documenting them as models, which could be adapted reductively in practical adaptations towards generating gains, while being aware of their reductiveness. It encourages the other side of such society to engage in cultural utilization of such surplus, by pursuing non utilitarian problem solving, cultural celebration and to build ways of romantic reflection in which problems could at its bounds be capable of appreciation. Thus, the moral leadership by an institution which nurtures the development and standardization of knowledge as an agent of development in the form outlined above is warranted in contemproary times. 
It is required to work without bias nor the proportionate enrichment of itself or its members in the pursuit of such actions. It would encourage the application of surplus in time and resources to leisure and spaces of leisure which would not simply be hedonic but to a greater extent, a dedication of time to arts, sublime knowledge acquisition and provisioning of arbitration of a decidedly pluralistic pursuit of ideals under one roof. Thus, pluralism, leisure and reflection attained through standardization and delimitation of knowledge, the inspiration and publication of conscious influential principles of human morality form the objectives of this institution. It may obtain necessary facilities to dissemniate its ideas, carryout campaigns and actions that influence societial direction etc.
The development of idealistic institutions in the society is of high importance as long as they adhere to moral standards and standards of pluralism and peaceful processes. They are important substitute to market oriented and computing oriented processes.
Part 2
The foundation would be working on the following important aspects 
As we have discussed erstwhile it is constituted in the form of a campus, wherein we attempt to contemplate a place and ideal where it is possible to host diversified ideas and discussions without material ramifications. That is to say, we believe that it is possible for human intellect to connect with a higher order intellect that is pervasive and universal in order to set the field in greater aesthetic beauty. It is granted that this scheme is explicitly to be provided for and there is in fact a preoccupation of the human mind to improve things by reflecting on the pure form of knowledge, to which we have been party to in some indefinite past as suggested by Plato and now in need of recollection forms the underlying narrative of this initiative.
Given, that there had been a golden age, perhaps as a convenient point of reference, we can extend our narrative to include efforts that seek a redemptive schema to reconcile our present position to approach such a state of pristine knowledge and peace. But it is neither terminal nor an aim of closure, but it is a dynamic attempt to constantly reinvent and remember, as it seems to be the destiny of our present state of which we are content with. Therefore, it is for humans to reconcile to poetic truths, through the medium of epistemical truths. The realm of epistemic truths allows to construct structures of knowledge, which makes sense in the realm of the human apriori knowledge of justice, aesthetics, redemption and even development. 
This being a metanarrative that is pervasive, we seek to find, why it needs to be emphasized now and here. It is often seen in practice that human endeavour is to seek dynamic solutions to problems, through the medium of the market, computational problem solving, adaptivity and generally evolving to meet the circumstances. This is a potent and pervasive force and it is important hence to reflect upon it. We had already proposed that there is a direction to seek higher truth than considering evolution as the end in itself. The scale, we feel is tilted in this time in favour of the method of dynamics. Therefore, we consider it worthwhile to actually dedicate resources for reflecting on such pure truths, to seek a verification and vindication of such truths by observation of phenomena and experiments, to share and discuss to refine the truth by synthesis, to contemplate nature and arts to provide a perspective upon the intellectual faculties, to accept and provide for the evolutionary drives by satisfaction of basic needs and tackling problems in the state of nature by appropriate technology and to create a climate for critical experimentation in the pursuit of truths which might involve dramatic enactments and stories. 
While this seems to be a grand and pervasive narrative, it is in fact the point that we do not feign to have come in possession of a messiahnic, timeless truth of infinite completeness and consistency. It is just in the reflection of the time and place or the context, that we have seen the need to concentrate efforts in a manner that is an answer to the prevalent challenge. This prevalent challenge is the extensive embrace of dynamic problem solving, to the detriment of the pursuit static truths, of which we believe the universe is organized by the benevolent maker. Therefore this mission, is well timed, finite and of course falliable pursuit of a cultural stance to pool resources, unify people and generate a coordinated expression as ideological expository material, experiments and practical guides and reference implementations in order to prove that it is in fact possible to sustain approaches to problem solving that are adapted from static truths. 
Therefore, it is important to define that the purpose of existence of this foundation is to put together and run a mission that would work on a particular method of pursuit of truth, which would leave the pursuer and the hinterland both in a greater aesthetic completeness. Recursively, the institution ought to attempt to reflect the outcome of its proposal to the general society, by applying these methods in order to generate an environment of intellectual reflection, experimentation and contemplation of nature and arts, to entertain critical and diversified synthesis of culture and discussion. The aim as it could be inferred relates aesthetics to the state of affairs, where the organization of the human society is not defined by the most energy efficient algorithm, as is formalized in Lagrangian mechanics. The redundancy in grand buildings, in open parks and play grounds, in cultural rituals and artistic practice, well laid out communities, in the prioritization of development to cover more basic things that alleviate suffering rather than construct economic orderliness, mutual care and space for spiritual reflection all denote the achievement of a state of affairs of higher beauty and perfection to the golden age. As said, the golden age is a construct that is not absolute, because as said, it is not possible to capture the whole import of the aspiration in epistemic, verbal terms. In this context, it appears that it is a useful reference point upon which it makes sense to build an institution to last such time as it might be relevant. 
In order to attain a greater perfection of the state of affairs of humanity, the example institution in terms of a campus that is organized around intellectual pursuit, cultural plurality, artisitc pursuit, harmonization or conservatism and respect to natural forces, commitment to technology that could be put to use for definite apriori designs is required. It is this institution that this foundation tries to put together by invoking popular support, by bringing the idea as being zeigteist, by demonstration in action and by publication. 
The principle having been explained, as set forth above, as being committed to attaining greater aesthetic perfection of the human society, by pursuit of static, timeless solutions and using them in greater proportion to solve practical problems than it is currently in use now. Secondly, to build a demonstrative social organization around such intellectual pursuit, well designed technological solution, romanticized reflection, art and contemplation of nature. It shall, with respect to its method stand committed to peacability, humility and a commitment to the due process.
Technically, at this point, the institution is concerned with a need to revise the notions of static or scientific truths to meet the challenges of dynamic problem solving powered by computing. It is also important to analyze metaphysically the present state of flow of history to appreciate the need for such intervention or culturally diverent demonstration. The running technical projects therefore aim to formalize a system of analysis that could bring the scientific method upto match with dynamic methods and the development of rational systems of cosmology, psychology and theology in order to model the existing dynamics of the position of mankind and the social organization, so as to be able to present the dynamics of the flow of history to the faculties of human judgement. We feel that it is important that these methods of reason would help humans to be able to observe a model from their innate, apriori frame and deal with dilemmas that are rooted in morals and aesthetics. The liberation of people to experience this dilemma is ironically a service to mankind. It also attempts to keep the pace or moderate the pace of technological progress, so that material needs are met and yet the mystical side of human reasoning and intentional action in terms of allocation of resources, dissipation by culture and art, consolation by spiritual reflection remains forever.
To enumerate the initial projects to be:
1.	Formalizing the metaphysics of a method of science, that extends the conventional determination of scientific truths to include degrees of dynamism to the truth. This is by combinaiton of ideas from control theory into scientific truths. The truths are controlled to demonstrate in extended time, even if in restricted context to an arbitrary limit.
2.	The development of metaphysics of computational networks and building on the idea of matrix, wherein the networks arise as a medium of diffusion in dynamic progression of agents.
3.	The development of a theory of dissipative and encoding structures which serve to stabilize the environment of truth. Thus, there is a partial link between the truth and the context, which might be treated as concrete subdisciplines in conventional taxonomy.
4.	Develop applied methods of control and modelling of dynamic complex systems, such as biological processes.
5.	Formalization of cognitive psychology to allow modelling of distortion to cognition and to integrate notions of health and stability.
6.	To strike a methodical balance between the holistic approach and the rigourous approach as a right answer to the dynamic problem solving. To demonstrate its superiority from a human perspective in terms of introspection and thereby deal with dilemmas of simulation, human existence and redundant looping.

It is inspired by the dualism of scientific rigour being well defined and formal thereby liberating people to practice arts and do things determined by their aesthetic and moral impulses in order to attain completeness. The dualism also incorporates the ability for an objective frame of reference to phenomena, the immutability and non explainability of God and the soul as philosophized by Descartes. It incorporates notions of realism of mathematics, existence of objective knowledge and referential nature of languages. There is a belief to benevolent order in the universe and the possibility of universal, objective apriori intelligence. It is apprciative of the quest of knowledge as with the mission of St.Ignacius of Loyola. It is inspired by notions of mathematical aesthetics and reflection as paths to knowledge apart from experimentation, as contained in Einsteins philosophy of science. It is inspired by the transcedental and romantic movements lead of Emerson and Thoreau. It is also inspired by Gandhian philosophy to the right method in approaching truth. Modernity is embraced in as much as it is well circumscribed, formal and definitive. It values the importance of control as discussed by Wiener. It is critical of postmodernity, monistic, nihilistic philosophies. It is critical of pragmatic methods of defining equilibrium by market economics, impetus and holism in the nature of second order cybernetics. Philosophies in the nature of dogmatism, desconstructivism of structures to fluidic idenitites and relationships, support to connectionism and emergence, evolutionary theories are also criticized. All ecompassing experiments, even if considered progressive are criticized. Quasi fatalist cosmological models over least energy pathways Conservatism is preferred over the impetus to fast development. That is to say, pluralism and conservatism as ways to reach just and progressive configurations are considered superior to completely engineer systems when it comes to human societies and the environment as a whole. Dynamic problem solving is appreciated as experiments leading to static knowledge, but except in urgent humanitarian situations, relying on dynamic fixes is believed to lead to regression and hence criticized.
Summary (reproduced from essay)
The scientific method is to be defended against purely dynamic fixes. The scientific method is incomplete and useful for the society to port problems to the cultural domain. The universe is mechanistic with an element of mysticism. The mysticism is important for the identity of humans and is transcedent. The development of science, automatically enables development of transcendental faculties. There is a benevolent order to the universe. Thus, in brief we support the Cartesian dualism of the presence of a mystical element, which is unknowable and a rational element which could be observed. We acknowledge that such dualism had in the past been extened to social progress in imperialism ignoring the effects of reflexivity in governing foreign populations and similar tranregressions. But in this present era, in the post modernist world, it suffers a crisis. The monistic constructs in quantum mechanics, postmodernist culture, game theory based policy, expectations on distributed emergent intelligence presents a specific extreme position, which ought to brought to debate and hence our assumption of the position as defenders of the scientific method and dualism is zeigteist for a higher calling, to which we have no epistemic way to state. Therefore, we posit that the present crisis of postmodernity in terms of lack of control over environmental issues, the problems with runaway corporate economics, crises of computational superintelligence all arise as a result of a destabilizing course from a stable state reached by the critical role of postmodernity to the cultural phenomenon of scientism that culminated in the world wars. The critical role had been overdone and the destabilizing course it is felt might lead to a new equilibrium from which there could not be an easy reversal. Such a new equilibrium might involve high degrees of machine intelligence is our speculation. Hence we wish to dampen the course by applying a critical control by invoking the methods of dualism altered to accommodate modern developments in technology.

Principles
1.	We will try to describe development 
a.	Development is measured by the degree of problems solved, which is indirectly observable by the variety and extent of leisure and of cultural expression. Hence renaissance might be said to be one of the high points of development. 
b.	Problem solving is attained not only by technological means, but also by mitigation through cultural accomodation and invalidation by philosophical and spiritual contemplation. 
2.	The notion of renaissance science
a.	The scientific method is integral to the development around a strong civil society and the promotion of an overarching secular morality. Science might be instrumental or even essential to the cognizance of apriori morality.
b.	The renaissance science is essentially identified by the dualism, wherein there is a recognition of observability and the mysticism of the observer and the belief that the universe is constituted on orderly basis. These values powered the civil discourse during the time leading to the modern civil society. It also made it possible for a culture of romanticism, rationalism and chivalry to flourish. 
3.	We will proceed along these lines to reason why science is not a narrative or a historic happening – 
a.	Given that science is able to confirm theories in phenomena, we might see that there is a fundamental order and direction in the universe. This is in contrast with the monistic or nihilistic conceptualization of the cosmos. 
b.	Due to pure mathematics, we might say that hope and an expectation of benevolent order in the universe is reasonable. 
c.	Given that there is a benevolent order, we might say that the notion of goodness exists.
d.	Given that goodness exists, we might say that its antithesis exists. We take a biased stance that while goodness is inherent, malice is due to ‘conditions’ or certain self sustaining phenomena that could in principle be dispersed and order restored. Conditions are complexes and situations of helplessness.
e.	From the above, we might say that science is motivated by universal apriori yearning towards goodness. 
f.	Hence might say that science is not a historical happening, but a realization of the notion of goodness, persistent yearning to build a civil society by a dualism of rationalism and a desistance of rational observation upon mystical components and uniting the pluralism by a realization of secular morals. This is an alternate, equally satisfying course of development to the self stabilizing, distributed passive model of stability. 

4.	Hence we will define science
a.	We might hence say that science uses scepticism as a methodological technique only. It could be described completely only from the stage of hypothesis generation to the confirmation of thesis, which involve hopeful contemplation followed by a faux scepticism and a confirmation of the underlying order. Hence, a scientific exercise is a discrete, tangible, portable and bounded functional behaviour. 
b.	Its antithesis is not the presence of non epistemic knowledge, which is acknowledged, but the presence of pseudosciences arising from an ambiguity or even falsity of representation as rigourous knowledge while being methodologically different.
c.	The development of technology in dynamic models, being a particular route to epistemic knowledge that could not be rigourously formalized as science is a source of concern. The truths obtained from dynamic models are local and context bound and hence not scientific. But what is disconcerting is the treatment of dynamic models as ends in themselves, rather than means to the end of obtaining epistemic knowledge.
5.	Why attention is drawn now to the precise nature of science 
a.	We presume that science is a source of reliable truths. 
b.	Dynamic models have been always a part of human technology, even with the bicycle. These have however become more common with computing. 
c.	Dynamic models are in principle bridges to our understanding of theories, so that they could be evaluated on linear principles with regard to public uses or absorbed as heuristics for private use.
d.	An observation of our society, shows that dynamic models are increasingly built using computing techniques to help people decide questions. The truths exposited by such models are not scientific enough. 
e.	The establishment of the dynamic method to problem solving, which is not ‘scientific enough’ is disconcerting, particularly when applied in the long run, without regard to the necessity to  . This alternate route to truth when presented as legitimate in power relationships, create anamolies, which could be considered complexes.
6.	It becomes hence a worthy pursuit to inquire deeper into the complex and analyze if it could be analyzed. In contemproary times, it is felt as a principle that the spark of renaissance might be reignited by criticizing the passive, distributed, dynamical solutions to problems arising as a defacto truism which could not be falsified globally.
7.	The method would be to 
a.	To criticize the dynamic method from a holistic point of view. That is to say, to evaluate with respect to their social instrumental value.
b.	To state the dynamical models as instrumentation or as simulation and thus integrate it with the scientific method.
c.	To address the metaphysics of science more energetically. It may need to revise its notions of complexity, stability and non parametric models and incorporate into its instrumentation layer prior to observation or as simulation environment in the experimentation in order to promote analytical models. 
d.	There is also a need to build theoretical frameworks, that could allow for better analysis in complex domains.
e.	To examine the well posedness of problems with respect to formal statement of the solutions or goals when the dynamic method is chosen.
f.	To reinterpret multivariate dynamical models as subphenomena, which are akin to experimentally controlled phenomena. These subphenomena could then be analyzed for linear truths, without detriment to their necessary use. Such systems are ubiquitous such as aircrafts etc. The necessary use might proceed undisturbed and theorizing to produce unified notions could heppen with greater stress than it is now assigned. The exercise might lead to the production of theories as well as material products as infrastructure rather than competing technological solutions.


Postulates
1.	There is a belief in the notions of redemption, stoic duty and satisfaction in the comtemplation of nature.
2.	Rational methods could be applied to metaphysical inquiry of the cosmos and the psyche.
3.	The notion of essential complexity is capable of distinguished into mechanistic and random components.
4.	There exists objective realistic concepts which are referred to in science and mathematics. Therefore science possesses more than just instrumental value and is satisfying of the intellect in its teleos.
5.	There exists a differentiation between technical and non technical problem domains. All well posed problems in the technical domain could be solved analytically. All problems which are ‘moral outrages’ could in principle afford an analytical solution in the long run. The use of dynamic solutions in the long run, is in principle criticized here. 
6.	For those domains, where the solution and the problem could not be seperated objectively and whose technical status is ambiguous, the pursuit of solutions by local searches and emergent cooperation could be motivated as a step towards analytical knowledge, either as instrumentation or simulation experiments. It is provided that urgent humanitarian work, where time is the essence, the method ought to take a back seat to intuition.

Corollaries
1.	It follows that the use of science for good things arise naturally from a commitment to rigour and certainty and admitting the intractability of the observer. The romantic world view which might arise therefrom normally guides as to goodness and on the diligent use of science and technology to those ends which matter more in humane terms, such as an application of resources for epidemology being preferred over market analysis. While the general social norm is for science to not to comment on these topics, we wish to extend this metaphysics of science to suggest the positioning of science in the overall schema of human knowledge.
2.	From the notions of the sanctity of the observer, it follows that human nature is intractable and it deviates from possibilities of logical inference through laws of inertia or principles such as shortest path principle.
3.	There is required of clear seperation of domians in terms of physical and organizational. It is an intention to maximize the possibility to translate problems in an analytically tractable manner, so that organizational problems are minimized as the infrastructure is strengthened.
4.	It is accepted that there are domains which are to be kept intractable by definition and kept outside the purview of scientific reduction. This is in general concerning problems in civil organization, policies and art. The capability of complex systems to organize by global propogation of local equilibrium  or by synthesis with rhetorical methods is recognized. For those problem domains where physical pain is not involved, the scientific method is in general not fit for application, such as for instance the case of restriction on technologies in Formula races. 
5.	The expressive power of art and spirituality is respected and held intractable by definition.
6.	To promote the principle that symbolic processing is both necessary and sufficient for all business knowledge or reasoning behind reward related behaviour. It follows from the teological nature of scientific research that it is important to promote the principle of incentive free scientific research.
7.	Underdevelopment is seen as a condition where there prevails an axious confusion over the self stabilizing approach to society vis a vis an explicit organizational approach, though both might be on par as methods.

Mission
1.	To promote the scientific method, dualism and analytical models based on extended metaphysical notions of the scientific method as viable alternate to dynamical problem solving.
2.	To inspire by the possible emergence of a satisfying and redemptive civil setting arising from the practice of these principles within this institution. 
3.	To exposit, demonstrate, experiment metaphysical methods to support conceptual analytical models as well as applied analytical models in complex domains like medicine, climate, meteorology etc.
4.	To critically examine the application of scientific method to sociological and economic problems, where the problem domain involves not materials but human actors, who are deemed in par with the observer, unless in exceptional circumstances as disaster management or healthcare.
5.	By criticizing the status-quo of postmodernity, dynamism to promote a notion of development around stronger civil organization.
6.	To remain free from dogmatism and operate in a fair, diligent, peaceful, incomplete manner. To restrain the mission as finite and fallible and hence promote a climate of well circumscribed individual commitment to the mission.
7.	To promote best practices in engineering and to assist exploration of scientific frontiers and in technology of humanitarian and environmental nature.
8.	To critically examine the well posedness of problems in complex domains and methods such as machine learning. Where the solution is not well defined in epistemic terms and change is made based on ambiguous notions of problems and solutions, it is to be inquired if the problem solving is motivated by notions of progress at all.
9.	For problems that openly looped to the real world, to launch a critical inquiry along the lines that inertial machines provide all necessary technical solutions thereby making automatons or energy conserving machines as only methodological scaffolding.

A critique of the approach above
In describing development as a state of increased cultural variety and share of activities, we have a tricky terrain to cross to concretize what is development. In stating that there be many activities which are cancelling out, we signify that the system is stabilized by opposing forces. Hence, we might say that development is stability by the achievement of a significant degree of its potential. Those species that are highly stable satisfy their niches well, such as crocodiles. They have hence stopped evolving or dynamically negotiating with the environment. They might be called highly developed with respect to their natural niche, or telos. What they do, upon entering on a stage of high development is to divide and reproduce. Hence, we might say, upon reaching high levels of development humans might as well propogate by reproduction and replication (say space colonization, like what E Musk suggests, cite) rather than dynamically negotiating with the environment. But that would require the concept of a telos or natural potential to humans. If that be granted, then we might say that humans strive to attain such level of problem solving that they are content with zero sum games and cultural expression, creating a highly stable system which could then reproduce its own image by replication. Therefore, we would have to state development in terms of natural potential or cosmic niches. The attainment of high degree of development would then be marked by a reduction in the use of dynamic methods and instead preferring planned replication into new terrain. 
The use of dynamic methods might hence be considered subordinate in terms of development. But the existence of dynamic solutions is yet another question. If we look at problem solving itself, we might say that the entire enterprise is egotismal. The universe is not asking for fixes. Whatever is formulated as problems could be solved by either a technical solution or a cultural mitigation or a psychological consolation equally efficaciously. Hence, problems exist within ourselves in a Kantian sense and in the process of addressing such problems, various levels of equilibrium or stability is possible. It is possible for a gatherer tribe to be perfectly content in their niche, till a raiding pastoral tribe displaces them or assimilates them. Therefore, every tribe and every individual should attempt to reach a level of development which is natural cutting across all divisions of tribes and holds for mankind in general. This universalization of man, suggests that development is a satisfaction of narcissitic tendencies or egotism, as arising from global apriori morals which define aesthetics. It coud as well be applied for the individual vis a vis a town or humanity as a whole. This being the case, the notion of development is an apriori goal and consciousness of our niche in the cosmos. It is the ability to desist the use of dynamic methods and instead stabilize and replicate as a strategy to overcome entropy of the physical world. The preference of this strategy is however subjective. 
Now, returning to the question of realism of dynamic methods, we might have to look at exhibits such as the aeroplane and the bicycle. These systems don’t have static formulation despite their ubiquitousness. The principles of balancing of the bicycle is still a subject of discussion ( a recent paper implicated gyroscopic forces). Whenever multiple forces are in action, analysis suffers. That is to say, if we place all agency on the observer and the system is inertial then the system is explainable. Where there are multiple agents, or even a random generator to the system (any random event generator is deemed to be an agent) apart form the observer the analysis becomes difficult. This is because, by analysis we restrict ourselves to linear analysis. But in fact, a system can have multiple actors working together and have a high dimensional shape. It is possible to evolve aircraft design itertatively, by dynamic methods and the technology helps (Edit We might much easily see that electromagnetism is treated from a purely dynamic angle as a zero friction conveyance of force, causing action at a distance, but its dynamics is statics in a sense. It is a stable relation best described at the differential level and it is only a matter of description. If we are to dwell on dynamics, even solid state molecules are oscillating about mean positions and electrons are moving in orbitals, but these are not treated dynamically for macroscopic purposes, because they do not have material effect on the expostion. Hence dyanmics might be a new level to statics that is relevant for a certain study). The outcome of such a multidimensional analysis is a shape, upon which local truths could be pronounced but not global ones. We had elsewhere expressed reservations on such non linear models becoming the basis of power relationships. In fact humans generally do not base power relationships on such models. The model itself is not a machine, but a phenomenon. Thus, the aircraft would very well be treated as a phenomenon. If it should gain popularity, one might see there is a reflexive route to it, such as a certain model developing market share, spawning following and serivce networks. There is also an analytical route, where a model is analyzed linearly to metrics such as speed and mpg and based on that evaluaiton is done.
Whether the market should decide or a central authority that relies on linear analytical truth or simply metrics should decide on the outcome of a contest for superiority is largely a policy question.  Therefore, a suggestion to use a static analytical model such as a fair game to decide on truths (such as whether model A is heavier than B) would involve a narrow game environment. If that forms the basis of people making choices with respect to productivity or perhaps formulas built using logical connectives over such parameters, then we would have an analyzable world where say products are selected for application of resources by logical planning methods by a policy making town council. But that would require a high degree of morality and the strength of moral apriori truths to sustain an analytical way to propose solutions for material problems. If a strictly analytical method exists, such as by analyzing a model or proving a non linear model over a range of linear truths (with always a residue), then the whole system might be seen to be relying on logical methods to solve problems materially. Such solutions would be mostly infrastructure, where fashion or innovation matter little. Strong infrastructures hence suggest high degree of development. 
Thus, we might restate development as a condition where most material problems are solved by the infrastructure, rather than dynamic methods, thereby allowing a system (being the subject of the examination for development) to be stable, preferring to replicate itself as a strategy of growth rather than dynamic evolution. It is also evidenced by the system exhibiting high degree of internal energy, by zero sum games socially. The system might hence be seen to be having both the properties of strong organization (by unambiguous methods to solve problems by singular infrastructure) and a high entropy by cultural experimentation and variety. Such a social system would then be said to have satisfied its telos with respect to its niche in the world. If such system is to be observed with respect to its niche in cosmos, we would have to include the entire humanity to be the system. Thus, high degree of human development might be said to have accomplished by the above features. 
However, given that dynamic solutions exist and they work, wherein human singularity as observer is not final, we would have to concede that a growth and sustenance by evolving dynamic methods is an equally plausible way to attain stability. The necessity to interpret non linear models into linear models is a way to rationalize decisions involving legitimacy of power ( or winning in a game determined by linear scores). There is additional work that is needed to interpret such models analytically and we say that such work is necessary for us to completely say that a solution had been achieved to a problem. Had it not for such linear interpretation, the dynamic solution could only be seen as helpful phenomena, which in turn leads us to redefine problems mentally, as interpreting the world as a more congenial place with helpful phenomena (like rainbows and mild weather). 
It is infact a highly subjective argument in that we say phenomenological approaches creates a bliss, that shifts our attention away from the real rainbows and breeze. Thus, accepting dynamic solutions ought to be restrained by a process of logical evaluation to specific formal problems and fixed into infrastructure (the material representation of the logical tree). Thereafter, we say that we would have some kind of an oligarchy involving wise men, we apply the rational methods to interpret solutions, which themselves might only be experiments or subphenomena. This, then becomes a philosophy, which like anyother is dependent on its aims or initial conditions. Hence, it might be argued that a dynamic approach is good as much as a static oligarchic approach. As long as both philosophers are authentic (for which reason philosophy often is seen to involve personal attacks along with criticism), we might say, each is a local truth in its own right, which can then only be set to a context based on a  dialectic synthesis. Gandhi believed in local truths as being proposals in his satyagraha, whereafter the singular truth might arise by sythesis. 
The use of analytical methods presents development as infrastructure and bureaucracy which would then liberate people to romanticism. Whereas the dynamical methods allows a liberation of people without the need for an apriori stoic construction of duty and labour. It would suffice to have love and peace in its place, as the dynamical camp would argue. There is really no moral reason to choose one over the other. But we might say, that an allegience to analytical methods, would help us appreciate culture and history better and work with machines all combined to solve problems. It might be a conservative stance to take and hence partisan. Our merit is only that this camp is overwhelmed by the popular culture now and hence deserves support. 
PS 
We might speculate that the use of dynamic methods of negotiation is monistic, where we see no distinction or cognition of the unfriendliness of the environment to how much we have achieved as well as such instances where the environment is benign and helpful. This might be a construction of the mind, but it is useful for our satisfaction and in the realization of the existence of the maker. The burdens associated with the use of such distinctions however arise due to the natural tendencies in the need for dynamism to answer questions and solve problems coming in and defeating the plans. Hence, it is of high importance to value the natural dynamic tendencies. It is not the dynamism itself is problematic, as much as a reductionism with rationalism is problematic (as with eugenics). It is rather than dynamism ought to be employed consciously, used as technical instruments allowing for a reflection. If there exists a telological balance which would be achieved, this enterprise does not have any reason. But if it accelerates the convergence to the natural balance, by the most important instrument known, ie by critical dialectic, then this is merited. 
We might argue as well that the static approach is optimistic and attempts to find solutions to cases which cause the greatest moral outrages, rather than regress to an average case scenario. This is seen as an innate trait of mankind, to treat outrages with greater empathy than routine progress. Hence, this is an effort in optimism. 
Part 2
Hence, we might state as 
Given humans have apriori moral values, it is possible for humanity as a whole to define singular concepts of aesthetic development. Hence, humanity as a whole could be the subject of examination for a certain state known as high development. Highly developed societies, of whatever degrees satisfy their potential or their telos in their niche. For humanity, it would refer to a cosmological niche. Those that satisfy their niches, have high stability and counter entropy not by dynamic evolution, but by replication. Hence, human societies that have attained high development levels might be identified by a lesser need to negotiate dynamically to solve problems. 
Problems are perturbations to the psychological reference plane of aesthetics. They may have material solutions or cultural and psychological solutions. In such societies where the physical problems are solved by static means, such as a strong infrastructure taking care of most physical needs, there is a high degree of development. There is however a natural balance to dynamics and static approaches to problems or the selection of problems which have static and dynamic solution. The natural balance is attained by a high degree of activity in zero sum actions such as culture. 
The polarized approach to problems as some being amenable to static solutions and some being amenable to dynamic solutions, allow a society to look at the environment as distinct from the fix. The fix where static, would help see the problem distinctly with the environment and hence appreciate the fix, by reflection. It also helps see the goodness in the benign aspects of the environment and romanticize. 
The development of dynamic solutions to problems in such societies are considered phenomena themselves, just like experiments are controlled phenomena. Thus, dynamic non linear models are interpreted analytically and linearly in order to satisfy the explanation of a decision and hence lend a rational legitimacy to power relationships. 
This approach is optimistic in that the natural balance is achievable by concentrating on the static side to the problem solving, by planning more that is, in the given contemproary state of the world. Thus the approach is optimistic and pluralistic. 
We might criticize that a monistic approach to problem solving by pure evolution, is regressive, ignores moral outrages and presents no frame of reference to cognize the nature of problems and benignness in nature. 
Hence, we might say, that given that we recognize that there is a benevolent order to the universe, as evidenced by the realism of pure mathematics and in the ability to theorize over phenomena, we would admit of an effort to work towards adapting such a cultural stance, that it is important to develop the methods of analysis in order for us to feel such benevolence and to labour hard, in the intermediate and non productive effor of interpreting dynamic solutions,  in order to contemplate the beauty of nature. It is presented as a pluralistic and context bound cultural initiative here.
Hence, we might say, in order to attain high development, it is important for mankind to embark on the strengthening of methods of analysis that could analyze non linear models and present them as linear truths that could be evaluated as metrics and formulae for resource based choices. This increases the orderliness and optimism in the world and makes the balance more aesthetics. 
Hence, the dualism of the scientific method, the analytic reductability of problems with only an additive noise and the treatment of dynamical models as experiments that permit the development of analytical knowledge as good policies in which to invest resources and intellectual effort.
It had been the ability to reflect on the ability to solve problems that had in the past powered, pluralism leading to militaristic conflict and proven dangerous to the future of humanity. But the reactionary development in monism, had  created another kind of extinction threat, by passiveness and destruction of environment. It is hence to assert that a critical mindset is important for the pursuit of truth and that is what this mission emphasizes, even if it exhibits pessimistic outlook on the status quo. Thus, it might happen that reformation does not mean a linear progression, but an embrace of a past state of development, such as the period of renaissance. 
RAF Ficher says that progress is not a law of nature and hence it might involve a search for a balance by moving backward or forward. The need for critical approach to problem solving was stressed in the methodical discourse of Popper, supported by the scientific philosophies of Heisenberg and Einstein. Dualism being redemptive, in making the observer a mystical and soverign entity was enshrined by Descartes and actually powered the socio cultural transformation during enlightenment. It also inspired the transcedentalist movement to contemplate nature and assuage problems which could not be solved physically as well as to reflect upon such things that are benign and graceful. This development resulted in an overarching moral framework. We see that this development of strong dualism in what is a scientific problem and what is not paved way for such a state of development. The historical attainment of such a balance, we postulate could be recreated.  The timeless formulation is the development of analytical methods to truth, which are epistemic truths and to concede the existence of non epistemic truth systems in order to build a high level of development. The use of dynamic methods in technological problem solving is temproary and a methodological step to truth. It is finalized  either as heuristic, which has no general implication on power relationships or analyzed to build analytical truths. 
In order to attain such a developed state in contemproary context, it is a strategy to work with criticizing the indefinite application of dynamic methods. Dynamic technological models are to be subjected to analysis and measurement and resource application to such dynamic tools ought to be subjected to analytical models as soon as possible. Thus, an indefinite use of dynamical models is disconcerting.

Part 4
This is in fact a social initiative. But it need not be so, It is in fact an initiative that attempts to hold within a finite set of recursive symbols all that means material value in the world. By doing so, it is believed that those actions of people which do not have material value is liberated from symbols and analysis. It reflects the very basic postulate that humans are more than their function. 
Part 5
In all our previous discussions running into the past decade, there seems to be a definitive stand which we try to repeatedly substantiate and argue for. It is something we profess definitively and would want to present it to the world. It runs in our discussions right from the work on computerized societies. It is a preference to a world order where the computers seems to be intrusive in attaining an equilibrium and the need for a deeper analytical theory of computing in order to make sense of its role in the attainment of a near perfect equilibrium. To make this hypothesis, that is to say, that computing might be intrusive in attaining this equilibrium, we construct a theory of history, much like Marx, attempting to explain the course of history in terms of certain concepts. We postulate the existence of the common man, which is reminiscent of the golden age in much of romanticism. It says that humans are capable of attaining a perfect state of harmony which could be described statically, rather than dynamically. That is to say, progress is a definitive state rather than a dynamic equilbrium that arises from striving to reach an ostensible inifinitude of perfection. We start from a golden age and then we reasoned about how the essential nature of human temptations dispose us into disharmony requiring dynamic fixes. The temptations over power and control cause the development of systems around symbols which encompass human behaviour and in the course of settlement of each emergent level of problems, dynamically builds a layer of complexity leading to a probable situation where computing becomes so dominant that human liberty and harmony becomes finally meaningless. This is narrated as some kind of historical inevitability in the nature of things. We say that the present state of swing from the golden age, which we again place in the seventies, to be not so good. Therefore, we attempt to bring back the romanticisim, by demonstrating in a counter culture and inspiring the people en masse on the ability to humans to take charge of their destiny, just like it had been with people who had fought dynamics with static ability to give meaning to systems. We had talked about mythologies as being the products of historical dynamism as well as the world ridden with excessive organization as being dynamical rather than meaningful. To this spirit, we add a specific technique of recognizing the polarity of the preoccupations to attain such equilibrium, as synthesized from cold reasoning of science and pure senitment of romanticism. The ability to keep these things in their original contradicting forms in a state of history, we say ought to become possible only by human endeavour. This human endeavour is not a well organized world government, but patches of counter cultural representations that inspire and thus unite. We also dedicated a great deal of thought to the idea that idealism could become dangerous, in that people who chase ideas become immune to the temptations of pleasure and thereby would impute suffering on themselves and others over higher truths. But we also recognize hedonism, might also desentitize to pain, due to preoccupation with local truths, of individual pleasure. We hence philosophize broadly that the dilemma could never be resolved and the recognition of the dilemma is what is the ultimate truth. That would make our definitive idea always being kept subject to the notions of humility, non finality and peacability.
Thus, in our heart we are romantics, but we believe that change could not be achieved authoritatively, but only critically. Hence, we place ourselves to build a system that could criticize and inspire the community at large. It would rely heavily on the dialectic of having to cultivate pure forms of knowledge, rather than merely useful forms. If such pure scientific approaches are strengthened then they would supply the basis of sustenance of a romantic preoccupation of the society. Thus, we prefer the growth of infrastructure to commerce and of bureaucratic governance to emergent ones. We have elaborately discussed in our second work, cybernetics and design thinking, the problems with attempting to solve the problem dynamically. It also gives a practical adaptation of the dualist approach, where we place certain constraints on freedom of the operatives in the business definitively, such as timings and rules in order to liberate the rest. Thus, we say liberty and equality ought to coexist and that would generate an overarching sense of fraternity. If it is pure liberty and equality as being merely emergent, or the other way around, we would not have fraternity or any sense of secular humanistic meaning.
The third  work deals with the dilemma of fixing problems by structures by computing, which we might interpret to be the static and dynamic approaches respectively. We had also dealt with the notions of epistemology in understanding the nature of problem solving with respect to dynamical approaches. We again visit upon the definiteness of the human state in the course of history and place it as the limiting point of entropy in allowing further development of orderliness. This was titled entropic intelligence. The subsequent works on the nature of work – in the oracle gratis and the work peace, love and computers, we dealt with the problem of how dynamic systems impose cost on the subjects, leading to the necessity of work, as a voluntary choice, much like virtue and restraint instead of being subjugated only by the immediacy of circumstances. We had also dealt with morality as an apriori force, which lends us meaning and context and hence assures that we would be able to redeem and resolve the dilemmas with love and care. That brought us to the establishment of inspirational entities which could then be functional operatives of the society  in the mendicant leisure class. The work on the engines of postmodernism dealt with the cultural side of dealing with the dismay that arose from how idealism desensitizes people to pain. But we present how hedonism and market oriented culture make things dynamic and makes projects into chaotic survivalist entities leading only to soul less emergent equilibrium rather than meaning. In this work, we deal with the problem of intelligence and the need to understand incompleteness more completely. We then posit that given that all knowledge is incomplete, we ought to represent whatever points we have critically. That does not mean, we should not be definitive. Since, we have a metaphysical model where development could only be achieved by critically prompting a revisit of the role of computing in modern societies, we wish to inspire and publish technical points on how this could be demonstrated rationally. The latter would involve works which would cause the dynamic processes to be treated as experiments and in closed loops to promote symbolic knowledge to cover problem solving of material nature. The remaining problems could then be solved free from symbols and in freedom. Hence, our approach is two fold, one is a definite pursuit and the second is a matter of method, to keep it critical and retain a base of pluralism ourselves. Technically, we base our works on the principle that we appreciate the non finality as well as the role of pure science as a romantic practice as well as of instrumental role in attaining or approaching a golden age.

Hence, this is practical utopianism, we say our approach is open and critical. We say that our reliance is on dialectic of sentimental and rational approaches and hence it is pure liberty available by virtue of work done on the real problems of the world and being able to retain a surplus of such work at the end of a static demarcable period, to enjoy the fruits, rather than to employ it dynamically in time and in negotiation in space. We say that this is not only a static utopianism, but one that could replicate and thus spread in an unlimited manner. We base our metaphysics also on the sufficiency of symbolic knowledge and complexity as being a theoretic concept, rather than a practical one. We say that all phenomena could be split into certain and uncertain additive components and analysis is sufficient for progress. We might be anthropic here. We are also historic deterministic, but only critically, while discussing the elements of computational progress. We side with the scientific philosophy of Popper, Einsten, Heisenberg, Descartes and romantic philosphy of Emerson, thoreau, gandhi, the development philosophy of nehru. But we strongly stand by the ability to represent ideas of such overarching nature not only by words, but such symbols as a campus or a town square. We also stand by the idea that ideas arise from the universe rather than from the ego, due to which, it is essential not to rush them, as with nehrus urgency to independence and put undue force behind them. 

Part 6
We have to deal with the contradiction in classical philosophy to fully appreciate our position. While descartes claims that I think therefore I am, Kant claims that I am therefore I think. Kantian assignment of high importance to aprioris was a watershed in history that set in motion the point that pure reasoning does not explain the human view of things. It is rather that only the individual who is privy to the innate truths revealed to him apriori could make decisions and rationality does not completely explain the world even to an appreciable degree. Therefore, it concentrated soverignity on the individual and served as platform for refutation of rationality, as just another authority. Paradoxically, the cartesian position on primacy of observation and the ability to conclude on objective truths, puts the individual in position of mystical superiority over the observed phenomenological universe. Thus each of these puts the individual as some kind of mystical and quasi divine. But we see that in the descartesian reasoning, the stress on the need to appreciate the world objectively, which is related to the idea of justice. That is the toleration of suppression of individual wishful thinking in favour of some legitimate form of reasoning, which is in this case logical process. In the Kantian approach, the priority is on liberty and on the need to pursue self interest unfettered by authority. We might say that the state of nature is very free and full of liberty, so that the need is pressing for justice and equilibrium. On the other hand, we might suppose that the world is full of pain, that people ran to establish the civil society in order to realize their liberty, in the cartesian reasoning. One can never be sure of the state of nature. The romantics believe in the presence of a golden age, but pure wilderness is not one of golden existence. It would require the establishment of some stasis but well defined place and form. 
The point is that both the elements of liberty and justice are acknowledged in romantic thought. The dilemma is irreconcilable, except by the invokation of the aesthetics which arises from the heart in a contemplation of nature. It also claims that this could be extended to the social group.That is to say, we might say, that it is not only the individual that has the special power, but also the social group. The social group is then assumed be more than the sum of the parts ( contradicting Popper) and having mystical qualities. The world order post war recognizes the mysticism of the unit of the family and no farther. But we claim that the family could not exist in just any environment. It could exist, but thrives in an environment of heirarchial support for mystical qualities, such as a tribe and a province and so on. The whole point of rationality, which involves encoding to strings of symbols so as to communicate and even that of fraternity invokes apriori of social grouping. Therefore, we seem to be making it a political theory, in that we consider that there is a need for authority, in some form or another in order for the society to survive in some static heirarchial definitons, such as a tribe in fact. Thus, we might look like political conservatives. The whole notion of critical approach is apolitical. That is to say, while we would require the existence of authority to concentrate and legitimize power and we support the existence of pure forms, as against polluted forms (for which reason environmental pollution arising form domestication and thus making grey forms of wilderness looks so disconcerting to us), we would be implying that there could exist well organized bounded communities within a restraining boundaries, a system of rules and book keeping, a process to elevation of arbitrary levels of power. Thus, power is legitimzed as a political solution and complying to authority gives rise of stability by justice. The counter weight to us is not political, but cultural. Thus, we say, the whole idea of leftist politics is misplaced. It is in fact of recent origin post Marx. In the past, it is a competition between legitimacy of power narratives and never was critical forces a part of mainstream political power race. Periyar was very much right here. Hence, we in fact require and support conservative forms of authority, in pure forms, which might be inconvenient and wild. But it is required that games and fights happen to rise the best forms of power by legitimizing narratives rather than some critical entity rising to power. The romantic philosophy demands that we remain apolitical, respect well organized legitimzed power under stable narratives and we criticize them by cultural means and development of artistic expression, remaining on the sidelines like the hippies. This is our political stance  and in respect to our recognition of pure forms and hating pollution in anything, we would like to develop pure sciences as legitimate power structures and play out in art to express subjectivity. We might even use the metaphor of four seasons to express our point, not just verbally but as civil constructions or otherwise. The seasons are repeating, but alternately hard work and enjoyment and there existing a clear distinction between times to work and time to play. 
Part 7
We had discussed on the nature of our stance and its uniqueness, but it is more of a non technical and moral point. It is also important to consider our approach to problem solving, but that forms an important part our technical approach. To quote from Tolstoy, all happy families are alike and each unhappy family is unhappy in its own way. This signifies the natural tendency to happiness. This may infact be streched to apply for any mystical entity, say a person, a family or a society. Every person or nation is qualitatively the same. It is only on certain quantiles such as the pleasantness of weather, the GDP or something else that one can order things. On the whole things would regress across a multitude of quantative dimensions to make entities almost equivalent. This being the case, one ought to respect the natural state of systems, intractable and complex as they are. That would leave us from directly manipulating the complex systems, because we recognize the mysticism of complexity. We might then see that there are systems that could be subjected to analytical inquiry. Say, a community resides in a fertile valley, the whole is in fact complex of the valley, its fields and its people. But the river could be evaluated for construction of some dam so as to remove the randomness from its behaviour. As long as this is analytically approached, we employ conservational physical laws and the whole is transformed without any side effects. But that puts us at a point, if we could say that all problems  are solvable analytically or that problems are solvable, because anything unsolvable is not apriori a problem. That is to say, do we mean that a problem is a synthetic term defined by our ability to solve it, or imagine a solution for it. It seems more likely that it is so. To emphasize this we would say a problem to be well posed, if we can imagine some answer to it. For instance, immortality is not a well posed problem, because we do not have a map of the solution, say what we are going to do with the infinitude of memories and such things. 
We had said that it would make sense to maintain purity of problems and solutions and more generally entities to be able to grasp meaning in a world. Then, we might say, if we are to approach problems and wish to put the weight of authority behind it, it ought to be a well posed problem. If a country rises against slavery, it is because slavery is unreasonable. That  which could not be reasoned is onerous and it is unfair that a certain authority backs it and hence this becomes a ground for conflict of rightousness. This would also suggest that the only problems are external and analytical and we might not have bitterness within the social setting. But we ought to recognize that the nature of human temptations and social institutions to sway to certain chaotic patterns are also problems from the state of nature. They might need to be analyzed and engineered of solutions, much like we take mood control pills appropriately. The point here is that such engineering of solutions ought to be analytically tractable, much like the social contract or the Leviathan. That is to say, all that coercion that backs the possibility of unfair competition, mainly by tactics of terror and apprehension or trust breaches, could only be put to rest, by demonstrating their futility. This would mean a coercive backing of punishment to such actions. Therefore, it might require upon humans not only to analyze and solve natural problems, but also to formalize a Leviathan in its bare frames so as to be able to sustain their civilization. Given, this is acknowledged, what is left is a disucssion on the dynamic solutions to problems. If things are stabilized by the analytical approach, by formulation of problems and hypotheses subjectively and each pocket approaching the problem towards a solution, one would consider it not entirely out of place. One might always seek to find solutions to problems, subjectively, without the weight of authority,at his own peril, irrespective of whether the problem is well posed or not. Such an effort is good as long as it is goal oriented. If it is simply a strategy of adjusting to the moves of the other participants in the game, it becomes dynamic and without a goal or meaning. Such an action starts with being devoid of virtue and amoral. Such actions cause the system to move entirely on tactics. It is true that dynamics are an important part of our problem solving even of well posed problems, such as with aerodynamics. But the solutions are machines that work in dynamic environments to accomplish certain specified goals and optimized for them. Each of their actions are experiments and strengthen their theory and they become optimum solutions to dynamic problems. Such solutions could be replicated and discretely built upon. They could be quantified and analyzed over optimum geometric spaces. Thus analysis is not just linear analysis. Non linear analysis can give rise to critically testable knowledge, even if not logically provable. The knowledge arising without a goal and the possibility of some critical evaluation either on the concepts of quantitative ordering or geometric stability in a critical space, is not real problem solving. To quote Longfellow – ‘something attempted, something accomplished, has earned a nights repose’. It is simply adaptation that serves the interst of the anxious psyche rather than the intellect. 
If the anxiously disposed psyche should have its way, it would likely push the human civilization, considered mystical and aligned to some higher ideal when compared to the other phenomena of the world. It might be considered the culmination of life and if we should grant it so, we would still be wary of its possible negation, of its being banished from the universe. In such a case, the problem that we could identify, with the baseline possibility of a happy existence (as Tolstoy said) might arise due to our tryst with computing as a powerful enabler of dynamical approaches to problem solving, which gives rise to and positively feeds back to incremental anxiety and the mainstream politicization of critical approches as being the critical problems of our times. The former proposes solutions that are inherently uncertain. This leads us to hedge forward to its future stability, leading to combinatorial wager, rather than an intellectual approach to objective solutions. The wager sometimes pays back, but at no point is it satisfying except when one walks out of the game with a closure. Hence reclaiming our sentimental comfort in meaning, closure, goals, virtue, the ability to self determine dilemmas become pressing problems or rather meta-problems which one can deal with inspirationally. The point if that such a metaproblem be materially backed and could people put together their effort to solving it. We, as humans have done it in the past, despite our shortcomings, we have stopped evil in history more often than we perpetrated it.
Part 7a – On the halting problem.
We can further our reasoning along these lines. In that we say that every person is qualitatively equal to any other, we ought to reason if a person with shoes is better than a bare footed person. The former seems to have solved an analytically tractable problem. Afterall the rubber in the sole and the rocks getting trodden over do not seem to mind, everything being designed around conservational laws. But we ought to remember that humans naturally possess an ability to be attracted to problems and attempt to solve them. By problems we mean here, problems to which we could already imagine a plausible solution, anything else is not problem, it is just depression. Given, this, there ought to be a reason why the person is barefooted, rather than him being short of intelligence. Even avoiding mathematical intelligence might be a way to sustain an equilibrium, like with amazonian tribes. The person with shoes might merely humbly showcase his talent for the possible absorption of the fellow tribe, rather than consider himself more developed. Hence, development is local to the circumstances. A tribe that is better developed could infact diffuse its knowledge and be apprised of the other tribes accomplishments, but there is no definite notion of development. However, if leaving the complex systems untouched, the analytical problems could be solved to the maximal extent then the development is at the optimum. That is to say, when problems are solved in the sense expressed by Tolstoy. What is a problem to one society might not be so to the other and hence while the expression is local, we might conceptualize development as being about anlaytical problem solving.
We have thus always been critical of dynamic problem solving. But if we see inward, we see that people are tolerant to dynamic inequality and dynamic simulation. That is to say, even if people are qualitatively equivalent, if they up front contract to compete on quantitative metrics, they are willing to accept inequality, but only on the premise that they would at some point in time, be capable of reaching the maximum possible position in the agreed contract or game. Likewise people have been capable of satisfyingly chase simulation goals, like years of examinations or sport performances, if they know that it could in some point capable of being applied in a real world scenario. Thus, these are dynamic and that is what is satisfying. But when it comes to dynamic problem solving in the natural world, we are highly critical. It is mainly because the absence of contracts that go with it. The primordial interrupt which the dynamic process, which are in principle the same as computational processes is lacking when we go with dynamic approaches in the wild. It is always the point to frame dynamic approaches within either the scientific notion of hypothesis, experiment and a new discrete version of the theory (and closure until next hypothesis) or within the notion of the social contract. Where the approach is open ended, it does not solve problems, but only depression. Attacking depression by material action creates anxiety and the problem becomes intractable and uncontrollable. Thus we say the most important problem of our times is the halting problem.
All our mission steps deal with strengthening approaches that would prevent this problem from manifestation. We attempt to strengthen the scientific approaches and the rigour to social organization by the construct of social contract and liberate the definiteness of human action from nihilism and vagueness. On the technical side, our work is to define new notions of theoretical completeness, by looking at systems in terms of not just satisfying quantative hypotheses, but also geometric ones. That is to say, if we always stick to axiomatic theorems, we would only be able to define so much of a definite point,on which to define a new version. But if we are able to formalize a geometric form epistemically and we wish to baseline from there, we would be able to control dynamic processes effectively. But in the long run, it would still be better to hold that solutions to complex problems and dynamic domains ought to be approached cautiously and they might even be rolled back and we would still have progress. To generalize, the whole idea of progress could be rendered, paradoxically on our ability to reverse it on will.














Some past work.
1.	All dynamic solutions including capitalism are provisional. Also with respect to mythological truths which are inevitably from historical dynamics (say racisim discriminating against colored people).
2.	Redemption at individual level could be projected onto the social level.
3.	At social levels ideas rescue humans from evolutionary predestination.
4.	None can state any belief to absolute certainty. We can make a stance as reasonable as any other. No messiahnic truth is practical.
5.	It is possible to explain ideas as forms. Such as universities where the only ideas are the reality and the need to nurture ideas.
6.	The pangs of hunger are real and hence dynamic solutions are a necessity but as a bridge.
7.	Just as there exists randomness, there is also ideas. They flow though humans rather than always having been conjectured by them. The ternal battle of good and bad goes on between organization and choas. It is an idea whose time has come.
8.	Order has to come from somewhere and hence it could only be extracted from chaos.
9.	Nature is funcitonal, it does not care for strucutres. Hence are attractors. Structures are choice.
10.	If we say all order I slocal even the solar syste and it is computational power that helps us to see orderliness, we would still need the apriori because computers are essentially normative. Either we acknowledge apriori order or a configurator of human level observation.
11.	It may happen that computers simulate humans their reality, an inversion of control. WE see companies simulating society in the workplace team culture.
12.	Putting all profit related thought into recursive symnol sets is our mission. What remains is liberated from symbols.
13.	The principal point is not that order can be determined, but that th eeffort is not futile. Thus all these teachigns is important not in content but as an effort. This is a fully reursive defintion of the mission, an effort to make an effort and so on.
14.	The form is one of contemplation of resence, but technical work is in strengtheing philosophy of science and developing methods that allow evaluaiton of dynamic systems to specific goals.
15.	The impulse to escape simulation is as storng as avoidin subordination under foreign powers.
16.	Development might be seen in temrs of degree of detatchment of the society from materially consequential games.
17.	It is important to note however that puritanical idealism causes one to devalue pleasure and thus become desentitized to pain and likewise with hedonism. Hence, an effort is required to be good. It is the recognition of the dilemma in the pursuit of truth knowledge. One can only speak of the existence of this dilemma and humbly and sincerely pray for guidance. Hjence, this is a meditative attempt to understand the state of affaris in plural terms.  It is not that knowing is bad, but knowing too sure is and our broader philosophy deals with recognizing the dilemma on what is too much.
18.	We wish to promote community level engagement, since we see that there are problems such as freedom from simulation, the need for closure and a fairly defined game as being evidently essential for humans and our social interaction for economic reasons is eroding it.
19.	The technical side of our approach is concerned deeply with the halting problem. While dynamic inequality and dynamic simulation and to some extent dynamic engineering are acceptable, due the ability to define a neatly versioned scientific process and a social contract, open looped dynamic problem solving has no such universal interrupt.
20.	It is good to define what is a well posed problem, the qualitative equality of all individuals and the need to respect problem definitions by societies and cultures. Definite effort by humans to earn their solutions and the nature of cyclicity of work, rest and celebration like with seasons is to be appreciated.
21.	The idea of goodness arises from our philosophy of being capable of positive action, while any approach that untruly poses to solve problems that do not exist or to hijack solutions to problems which are not formalized is considered detrimental to common good. They are however approached critically, humbly and peacably.

Another Redraft
This is a forum in which is aimed to bring together people and material, required for no particular goal. It is the organization of people and material, which would by its existence, away from the utilitarian material concerns, inspires the larger spirit of humanity as trancedenting beyond material dimensions. While it expresses its existence in cultural vibrance as the positive goal and not act in a particular way,it reinforces the point that such an existence is  









Problems with the approach to dynamic problems- on design
We had approached the technical solution in terms of extending the analytical method to cover geometries. That is to say, if there be a theory promoted along a linear correlation between axes of a quantitative n-dimensional formulation of a qualitative system, then we understand the system in terms of equations that relate these axes. The equations or sets of equations might be solved to reflect on solutions, which express the values the variables might take in order that the observations on a linear number line expressed in terms of the axes make sense. It might be seen as the fundamental relationship between the axes, which then places everything in a linear comprehension. None of the axes would then be qualitatively different from the others. The ability to summarize in linear terms allows for logical proofs, because logical truths are linear as well. If we use methods which involves qualitatively irreconcilable dimensions to objects, then we can describe theories as optimum spaces, which are stable. The notion of stability is tricky here, because it could not be proven by known techniques, but only tested.
That is to say, we might have probability distributions of past observations of interlinked dimensions, say an aircraft design. It might be a proven design, due to its having been battle tested for decades, but it does not signify a proof. The dynamic nature of the optimization might allow a positioning in the stable space, but might hide other spaces that exist, if a discrete shift is imagined. 
We will discuss here the nature of dynamic problems and their solutions, in order to appreciate the nature of the path taken by analysis to cope with these developments. Dynamic problems are manifold and they have always captivated the fascination of people. Flying, buildings, bridges and automobiles have all values that goes beyond their utilitarian values. They have all been solved not by mechinists but by designers. When the artisan gets to work, like cutting a board, he cuts to linear metrics and is rewarded in a linear format, such as by comparing it to some standard quantity like a yard stick and using simple arithmatic to calculate rewards. Likewise, the work could be accounted (ie counted and accumulated), compared and planned using linear formulae using basic arithmatic. However, where the problem is one of optimization, the designer produces outputs which could not be meaured linearly. Thus, design is a work of art, rather than one of physical engagement of labour alone. An automobile designed in a certain way, doubtless involved computation on balancing against multiple compelling constraints and discovering the best sweet zone to place the vehicle configuration to produce a usable product. But since the use value of the product is only a linear measure and the optimization is done on multiple axes, the method is not one of logical deduction, but also involves to a great degree intuition. This intuition of aesthetics, stability and utility forms the design. The design is a cultural statement and not only an optimization. For a long time in India, there were automobiles that was optimized around 100cc engines, save for a single model that was optimized around a 350cc engine configuration and it was of iconic status. Likewise, designs are cultural statements, because there is no direct way to measure and compare them. They would be presented as a spatial truth rather than a linear one to the consumption of the human senses. Plain rationality could not handle it and it becomes a sentimental object. All non linear truths are coupled to emotional response, the fire that could spread exponentially is associated with panic and fear being the right emotional response therefor. If one is to rationally and linearly treat the problem of a spreading fire, one would be absolutely off the mark. 
The artistic nature of design helped humans to find a purpose of sticking together. If a certain design for cathedral domes could be used, they were passed around by communication among the masters of the trade. They were repeated and followed within populations. The cultural identities are also built around design of systems including physical and systems of bureaucracy, authority distribution, trade among others. Therefore, the solutions to dynamical problems had been analyzed to an extent, but every outcome of such an analysis is not just functional, but also artistic. No wonder that Da Vinci made a good designer. We now look at the problem of analysis of such a representation of optimization problems. As we have stated elsewhere, the concept of stability replaces the concept of solution in a dynamic problem. The stability could not be proven, while a solution could be. The stability could only be tested. Testing was something that was erstwhile limited by the nature of communication and the slowness of history. Hence, stable designs was associated with cultural values, like the first muscle cars of the US and passed around in the like culture and over generations. 
This piece of cultural legacy was deeply disturbed when the ability to test in simulation was greatly extended with computer aided design. The optimas once discovered by some intuition could then be randomly perturbed using dynamic simulation software. The ability to discover test outcomes by repetitive application of inputs to dynamical models easily simulated in discrete forms in the computer paved way for the fast forwarding of history leading to quick emergence of design, often the utility taking the front line over the pride of the designer. There were great economies achieved in design and development in civil and material engineering. This flooded the market and made things hitherto considered high value as ubiquituous and even banal. This was a situation which we might hint as when problems which were really not problems were attempted to be solved. It might be insensitive to comment so broadly, but the mass production of goods was greatly prompted by the ability to simulate designs in computers so that things were produced en masse and many of them simply went to assuage the ego of people rather than real needs. Thus, we might think of the development of material solutions to psychological problems as being a temproray and dynamic fix, which has no tangible limit. It necessiates economic expansion and exploitation of resources to the brink of collapse. 
But it ought to be remembered that even before the electronic computing, mass production had taken root. The designs were no longer cultural, but involved the ability to work upon analytical formulations that could be presented as blueprints of components. If we look deeper, we see that there are discrete mathematical tabulations of specific values with respect to the behaviour of specific components. These discrete values, might represent the strength, thermal coefficient and like values of material put in engineering manuals. The engineer draws from these discrete components, certain combinations from a huge pool of possible combinations based on intuition, style, aesthetics, convention and examines certain designs to be workable. This predates electronic computing. If we should formalize this approach of engineering, we have to resort to concepts of discrete mathematics, where we could identify as functions, which represent unique combinations along these axes. What might be material is that some of the factors might be non linear and non linearity is material only for dynamics. If the function has a factor involving temperature and it is a cubed factor, it is material only when the system is changing in time. The sensitivity with rise in temperature is very high compared to say, stress levels. Therefore, the system dynamics could be predicted and modelled using, for instance, multivariate or partial differential equations and much of the evaluation of such equations is by simulation, rather than solution. 
Thus, if we see, engineering involves not just neighborhood searches to former approaches, but also an inspirational element, where the engineer suddenly gets inspired by the french revolution or an elephants trunk while constructing a pipeline. This element could not be analyzed and it is a crucial element of engineering. But apart from inspiration, it is possible that even the best inspired designs might be flawed logically. That is why the engineer would have to render the design in abstract format, such as in a drawing board. He might see, statically, that the load on a column as too high considering the compressive strenght of the material if he is to use a certain architecture to support a bridge only if he applies mathematical and geometric concepts that bring together the selection of discrete elements of discrete material values (or states). Their combination might be different, under the influence of constant forces like gravity or in terms of dynamic forces that vary, such as dirunal temperature changes. Stability might be compromised if adequate attention is not given to the abstract model, which is an analytical representation of a design, rather than a simulation to evaluate stability. The decision finally is linear, where the model is evaluated at a certain point, say the lowest brick layer of the column being compressed and it is evaluated linearly against a threshold value for compressive strength of the material, but it is infact derived from a model, which allows for evaluation of linear values. The model is hence not provable as a whole, but its parts could be evaluated as tests and if enough readings of various values at stress points, which are by no means exhaustive is taken, then the structure or rather the model thereof can be ‘witnessed’ to be stable. Thus, no geometric model is provable logically. It can only be tested. This is irrespective of the degree of computational resources at hand. Therefore, the question remains that if the engineer can search for inspiration in a higher abstract universe of mathematical beauty, rather than working with more mundane and fallible universe. That is to say, if the model of the system at hand could be corresponded with a mathematical dynamic model and instead of testing it, if the mathematical model be evaluated intuitively for perfection. That is the sense of ‘mathematical intuition’  a true concept in the realm of higher intelligence is our quest.
If there should exist mathematical intuition, then we would be able to build geometric functions, or integrals (perhaps of higher order than areas) as being first class objects in mathematical analysis and extend mathematics seamlessly into the discrete domain. These functions could be numerically evaluated, which is another way of saying that they could be tested, but if they could be analyzed for stability or other dynamic behaviour using tools such as differentiation is a relevant question. In problem solving, which this book is about, we had seen that one ought to spare those problems which are likely to amenable to solutions inwardly, in culture and in meditation by contemplation of nature. The dynamic nature of engineering is hence a way to produce truths that are context sensitive and not static. We might upon a dynamic model assert a static truth of a tangential nature (say for a very tiny variation in temperature, the system would expand by such and such proportion) or we can have truths that are of probabilistic nature, such as the median and associated noise to the truth being expressed. The notions of tangential truths and probabilistic truths are sometimes incapable of being reconciled to linear truths. In such cases, they become bound to the domain of culture and the psyche. If we should go to the extent of seperating such a nexus by extending the notion of truth to stability and optimas is the fundamental question. 
If we should extend logic to cover stability, then we should be capable of performing experiments and intuitive hypothesis generation to extend the notion of stable zones, or state enumerations of complex systems. The state enumerations could then be represented in terms of recursive state definitions, where a state might be more affine to transform in time to a certain set of states over any other, probabilistically that is. In such a case, we would have states composed of networks of states and so on recursively. The state itself becomes a granular network of substates in which the system oscillates or directionally by latently progresses. On the other hand, linear logic covers systems which could be versioned linearly. The models produced using linear concepts could be represented as a tree, or reductible to a provable logical tree. The quantification and axioms form a broad field of knowledge in mathematical logic. Theorems hence subject to linear reduction the models of problem domains. This can be the only definition of what a theory is and hence if we should propose theories to recursive state networks, we might say that such models could be theoretically sound. This appears to be plausible, in the sense that if the network represents geometric stability concepts and state could be represented as a joint distribution space of n-tuples of known (tested) tangential truths of the model recursively, then we should be able to use the concepts of probability and of analysis (numerically) to be able to arrive at definitive linear concepts of comparision which could then be ported to the logical concepts of connectives and truth values. The theory would however be statistical, much like statistical mechanics. Do we content ourselves with statistical theories, where our pursuit of truth becomes one of plausibility rather than certainty is a normative question. That is to say, that given, we are well equipped to deal with uncertainty in cultural and our emotional construction, do we need to extend analysis to this domain is the question. It is not whether we can do it, but whether we need to do it. 
This way we can construct sound theories of neural nets and be capable of producing statistical truths which are served well in a situation of governance, where we are interested in policy and central norms, rather than individual cases. But the use of geometric and statistical truths to produce useful knowledge is no less dynamical than adaptive approaches. It is oppressive of free will. It ought to be remembered that quantum mechanics does not exist away from our everyday lives, hidden in the labs of eccentric scientists. The fundamental nature of uncertainty when demonstrated created a huge philosophical shift as well as the ability to accept statistical truths and thus onboard many hitherto artistic domains into sciences, such as politics and economics, management and operations research, which we see to be having far reaching cultural manifestations. The use of statistical truths puts into the hands of controllers, who have general power over a multitude of individuals, say doctors, powers to decide on the intervention to the general progression of the system. Often times, this arises reinforced by cultural factors of respect, inspiration and intuition, so that failures are handled gracefully, given that the domain is statistical. The failures due to the essential nature of the solution, in the case of analytical models fail, due to inherent reasons and has no explanations therefrom and hence no closure to the aggrieved individual. On the other hand, there is a continuous consolation and sentimental mitigation that comes culturally, when a statistical domain is to be addressed by a skilled professional and it fails in a given case.
The dilemma is hence to what degree that analytical techniques be mixed with cultural approaches to solve problems of dynamic nature. Neural nets have sound mathematical theory in a statistical sense ( afterall numeric methods provide instances of witness and not a complete reasoning), but they are widely objected for reasons of sentimental nature. The question more aptly is how much of the dynamic technology can be put in the hands of professionals. Popper attributed even a single failure (a black swan) to be dismissive of a body of truth and hence for him scientific truth is not of statistical nature. If there should exist a sound body of theory for a certain approach, then it is in principle valid. The question of a political authority or a cultural force could by approaches invoking sentimental points restrain the use of such approaches. The dynamical solutions of neural nets come under controversy because of their statistical nature and of the concept of feedback. But feedback is inevitable in dynamic systems. Or is it. We will explore the part on feedback and stochasticity. 
As we discussed we have seen that there are some normative concerns with the use of geometric models. But as we said, as long as a model has a theory, we do not have a problem in its veracity. The development of multidimensional analytical models help us in figuring out a central truth in the system much like a linear model. Hence, there is in principle no fundamental difference between the two systems of knowledge. When it comes to problem solving, let us consider the case of a person attempting to escape the cold. He might be able to linearly determine the place which is pointed to be the warmest from a finite set of shelters from the cold. That would be a perfect information scenario and the knowledge is perfect, objective and it is completely detatched from context. It might just be that in the open region, the shelter is best provided where the wind is blocked to the maximum. But imperfect information scenarios have been natural to human environments and the methods of problem solving along those lines is age old. Say, if a person would have a limited amount of woolen material and he should optimize to keep himself warm. He might be willing to tear it into several pieces to keep critical portions of his body warm, say the extremities and the head. This involves actions that are irreversible as well as multidimensional. The dynamic exercise might be based on many notions of expectation rather than perfect signals. If the storm is going to wear off quickly, there is no point in tearing apart the woolen cloth or even in the choice of the areas to be covered, one would need to be judicious and optimize. There might be multiple solutions that exist and specific to the circumstances. Nevertheless the knowledge is important and a model is not far fetched for such use cases. 
If we look at intelligent problem solving, we often see that the problems might be microscopic or macroscopic. The microscopic problems are best solved in a distributed way. Such as the tendency for sclerosis and fibrosis in biological systems are useful as ways to locally fix problems. But if they should have systemic consequences, they become mechanistic, well posed problems, that seems to have arisen, because of a path that had been decided in some historical past. Say, if humans evolved to have sparse hair in the African plains, their historical decisions by microselection on a distributed basis becomes a heavy burden sometime when the ice age comes around and their niche would have been severely restricted, had it not been for artificial clothing. Thus, macroscopic problems like atherosclerosis (by mechanistically compensating with vascorelaxants) or clothing stand to be reasoned out analytically and that could complement the system of microscopic distributed problem solving. The microscopic problem solving could not be emulated on a central system, because there is no way to determine if a tiny mutation in some remote cell is going to be of any use, or if it would make it to the germ line. These problems are best left to the nature of things and distributed setting and once they make a macroscopic appearance, it is best to press analytical resources to obtain a leverage.
Given, this principle and the need for optimization problems in imperfect scenarios, we can say macroscopic engineering of analytical multidimensional systems would be a good way to proceed. That is to say, it is possible to produce objective truths by the use of analytical models even if they be geometric. But the notion of objectivism is associated culturally with authority and centralization and this becomes an irritating issue to the ideologies of democracy. Therefore, often times there is a preference to distributed exploration. Hence, it becomes important to distinguish between microscopic and macroscopic problems. This is where it gets blurry. Lets suppose a problem is capable of being solved by random attempts at the periphery, much like the market and a central token issuer is sufficient, then we might say the problem is handled as if it is microscopic. We have already discussed the nature of intelligence as emerging from distributed actors connecting through a diffuse medium. In this perspective, we might even postulate that a market with a mass medium might constitute an emergent intelligent consciousnes, which will resist disintegration. Hence, the question is whether such an intelligent emergent system is taking over macroscopic problems as well. 
If we look at market aggregators, we might say that they are infact using a central analytical model to simulate the market. The central analytical model is so complete with all the notions of local and global gradients, presenting the problem as some kind of terrain, that they are able to make decisions on tangential questions. This was erstwhile addressed in a distributed manner but now capable of being simulated centrally. The taxi networks are classic examples of this approach. The taxi networks analytically compress the whole range of complexity that arises from provisioning taxis in stands, in negotiations of pickups and drop locations and adhoc relationships and long term friendships between the business partners. This system is however provided with the interrupt at all levels, so that any insensible infinite loop is avoided. Thus, these kind of systems are capable of solving low level problems using a central model. If the same could be said for healthcare, where the whole network of states (we might think of edges as the ‘state of flux’, but it is better to keep them first class). This arrangement of describing the system as a map of rugged terrain and presenting the problem as an optical map, could help reduce the problems associated with application of skills and intuition. But as said above, it might have impacts on the notion of closure. When it comes to regular work, such as driving taxis or hopping rides, this might be liberating, because people never hardly get lucky or feel unlucky. The system handles luck for them. It is not distant from driving buses or riding them, everything is mundane, only that there are no regulars here. The faces change randomly, but functionally they are all the same. The point is that such a work is urbane and if restricted to working hours could liberate the worker for a happy time with his family or friends, as romanticized in the movie ‘one fine day’. It might be more difficult where important decisions are involved, such as in healthcare. 
But dwelling deeper into this model would reveal that the constitution of multidimensional analytical models is not entirely versioned neatly. They are infact behaviourally dependent on a greater network and hence not well bounded, due to which they could not be versioned in principle. The outcomes of such networks are dynamic and the same fare might not be produced when the same ride is fixed for the second time. But as long as the difference is quantative, everyone is fine. Even, if the difference is boolean, its consequences could be quantified. Therefore, every software application could be generalized as a multidimensional anlaytical model, where tangential truths could be obtained and sometimes dynamically, as long as the deviations could be quantified. Hence, they work best in the case of games with buffers against real and painful consequences.  But it is difficult to estimate collateral damage. A taxi ride might be all the difference between life and perishing in a certain case and we would never know. The ride application might simply refuse to serve assuming outcomes to be quantitative. We have discussed elsewhere that morality is concerned chiefly with the concept of ‘collateral damage’ (cite). The two important parts of the problem is the collateral damage notion with the dynamics of using an analytical model and having someway to shield it from a real consequence, such as always having an analog fallback. 
The second problem in this system is the tendency to address microscopic problems. This might happen by a creep of neighborhood searches. For instance, in expanding the non linear analytical system, the system might be configured, either through manual process (agile approach of development say) or by automatic process to search for features in the periphery. For instance the analytical model of taxi routes might be incrementally extended by experimenting with routes at the periphery and based on feedback reinforcing such routes. This involves not charting the terrain from existing data, but generating new ones. But even such approaches of charting terrain uses stochastic approaches, by random moves and then finding demarcations and structures in the terrain, with probabilistic certainty. In approaches like MCMC, stochastic sampling is used to outline the structure. It would become natural of such systems to use such stochastic approach, to explore the periphery. If the environment is passive, this does not make much difference, eventhough such approaches might infact lead to historic mistakes (but that could be subjected to human rational examination, say like optical illusions like mirages could be dispelled by rational inquiry). But where the subject matter is responsive, like taxi consumers, it might refexively affect the environment, causing a reconstituion. Say, if incentives are offered in edge cities, to take a possible case, it might prompt migrations to edge cities, or it at the least influence consumer movement patterns in selection of their workplace and routes. It might choice engineer. It is what happened when the railroads were laid, but the reflexivity was practically one way. In case of distributed systems probing the edges, it would necessiate multiple options being proposed by paralell actors exploring edges and them being able to communicate through diffuse media, giving rise to emergent consciousness.
Therefore, it might be of importance to define the boundaries of the system, have an interrupt and a fall back in order to keep it normatively acceptable. The ability to bound the system based on past interactions, then the map generated of the terrain with all local and dynamic gradients would need to be completely backward looking. It could be distributed as with neural nets and work based on feedback, making random moves and adjusting to feedback, but all the dance, is to be restricted to perception of things that have already happened. It is like representation of what is already there, as with our sensory organs. Thus, non linear analytical systems might snapshot a terrain and present it as the state of affairs for decision making to simulate a distributed setting and thus make things more analytical and delineated. But the system would still not be self contained, it might expect other ends of the network to produce the desired result, as with the modern microservices architecture in software engineering. The versioning if it happens very quickly, consuming the data just in time and reinventing itself, then too, the system is likely forward looking dynamic. Secondly, if the system lives on a network and the versioning is unsynchronized, it might happen that in effect, the whole network would acquire forward looking properties. Thus, it becomes an intuitive question as to decide the turnaround time. Neural nets are particularly in the microscopic problem management domain and their dynamics are going to set in motion quite some reflexivity and coevolution. The notion of fallback plan (to enforce isolation with a buffer against real pain), sensitive subjects and manual interrupts are also unclear and they could only become a subject matter of intuitive governance. 
An approach that utilizes lesser of stochastic methods and more of mathematical objective statement from a set of prototypical mathematical models might become handy. But there is no limit to the extension of applied mathematics. From the above we realize that the non linear or dynamic models necessarily transition into evolving highly reflexive systems. In the past, there was a certain slowness to this. It would allow the society to assimilate the change into its culture and provide meaning. Now, dynamic solutions, like one metrolines and bridges are purely computer proposed. They do come with stochastic risks that are ‘acceptable’ in the norms of the society. Hence, the question becomes one of frequency or speed of change. If a change happens sufficiently slow, then the society might be able to cope with it. It is a proposition that says that there is a natural speed to human ability to handle transitions in a dynamically stable way. It also resembles the notion of conservatism in politics. All these point to a non technical solution. 
But if we should bring forth a technical solution, it might look something like this. If a system could be proven, rather than tested, it would slow down progress to the effect of making sense to humans. The problem with high frequency backward reading autoregressive programs as we had seen is to able to flip the arrow of causality at a certain critical frequency. If every version is proven against a platform of theory, then we might be able to dampen progress with an inbuilt negative feedback and thus control it. If the underlying base of humanity is hopeful and not clueless, then there must be a solution to this specific problem of uncertainty, which at this point is only depicted as a moral outrage. It could be that proof systems can help. The problem with reflexivity as we see is the classical dualist observer problem. If a system is logically consistent, then it is deemed to have been proven against its axioms. If a bridge is a multidimensional analytical model represented by a partial differential equation, then one might be able to prove it by virtue of its formalization in the notion of partial differential equaitons. Even a neural net could be formalized as a non planar hyperplane seperator in a classification problem. The construction of this problem from statistical roots does not rob it of its formalism. It might be tested for stability, as with bridges, but over time, classical benchmarks would become lost and things would regressive as to what is an acceptable risk. Hence, testing is unreliable for objective confirmation. Also being capable of representation in applied mathematics which is a axiomatic theory, does not also make a solution acceptable in objective morality. A possible dampener might be to set the degree of stochasticity to absolute low values, so that the system would be longer in the making, integrate better with culture and propogate as cultural architecture as with the olden days. 
Thus, we might attempt to make a formulation to meet very high standards of hypothesis confirmation in order to accept it. This is much like Poppers standard of acceptance of scientific discoveries. A single instance of disproof of the benchmark of acceptance (not absolute truism, but even a statistical benchmark that is theorized) could disprove the theory. Hence, every dynamic tool, be it a biscuit or a cake ought to be made into rigourous theories where every side effect, health intrusion of such a product is evaluated by repeated testing and composed into a consistent mathematical model whose boundaries are well estimable then we might have a system of control. That is to say, every program ought to be checked by testing for consistency over many iterations to accumulate a legal approval, as a political tool of control. A technical platform could evaluate the tool and certify it. 
An approach from a scientific community on scientific consistency, much like how pseudosciences are institutionally deprecaded now(Popper cite), without the need for political sanction might be extended here. This would require the development of new axiomatic theories apart from the completely positivist mathematical notions. One such theory might incorporate an approach to understand the stability of the system having assured due to a pure dualistic observability. That would mean, that the theory ought to separate the non linear analytical model and randomize it. That is to say, the non linear model of the city cab network might be evaluated for consistency by treating it as a phenomenon in a new axiomatic theory which could classify the system in a static heirarchy. It should be able to quantify its degree of adaptability, turnover frequency, sensitivity of breakdown cases, isolation levels, asynchronous network components and formalize it in relation to such active entities in the society. That would be more like the classification in biology. The axiomatic direction of classification supposes an order in terms of development of complexity and classificaitons are made based on such criteria. If a theory could be concieved that could apply such measures of complexity along arbitrary axes of bounded determination of software systems, then we might have a scientific process in being able to define active entities with respect to each other in a theoretical framework consistently. 
Hence, a new science would be needed to explore such active dynamic systems which are highly variable, life like systems, which would then stand to be observed with regard to definite boundaries. Science is all about observation. If these systems could be reliably observed with respect to their properties assigned in the theory, we might be able to classify and compare them. That is to say, if there is a theory promoted on the phenomenon of the system that actually reflexively interacts with a system such as  a disease, by intervening (either by high frequency backward data stochastic modelling) or by a distributed diffusive field mediated forward read, we could see it not to be any better light than the underlying natural phenomenon. Hence, we see the solution as a symbiotic organism in the niche. We can only develop an axiomatic theory based on measurable parameters (such as complexity or even the count of limbs in biology). The observation of these measurable parameters leading to the classification of these models might hence be scientific. 
In the same spirit could we say that economics is a science, because we can observe the subjects directly is also moot. Here we observe dynamic systems from the point of view of certain social metrics. Likewise, we might even postulate economics as being capable of supporting clear observation on a statistical basis much like statistical mechanics. Even if duality is not preserved, it could be on a statistical basis. The observations could then be adapted into policies. Economic theories might be axiomatic theories much like biological ones. It would involve the study of complex dynamic systems, their equilibrium and their control. The subjects of study are markets, money or governmental rules. Likewise, we might use a scientific system of classification and metrics to study the subjects. But we have elsewhere expressed reservations on the applicability of the scientific method to human problems. But a science that deals with phenomena and now that we have problem solving as a phenomenon (much like economics) we might as well study is from the important view point of reflexivity or circular casuality. This is reminiscent of cybernetics. We would need to have a science to study and stabilize the adaptation and normative utility of such cybernetic organisms. 
Thus, to summarize, we may say the primary problems with the use of multidimensional, analytical models would be 
a) One of Reflexivity, this is hard to escape in that any system that is aposteriori to observations without sound apriori principles, necessarily impacts the system in a way that causes the subject system to regress the gains obtained from it. This might be the law of decreasing marginal returns, of central regression of coevolution. On the other hand, the maintenance of objective points of view allows the system to idealize, rather than reflexively optimize.
b)The second one is of integration. Every dynamic solution approach is something that is statisticial and has counterfactual possibilities. The major problem with the statistical failures, which are inherent to the model is that these have no meaning where an analytical model. Normatively, humans rarely tolerate pure randomness. They would want to fix causality or even reflect on a cultural universality of such inexplainable mystical happenings. Therefore, all failures of past design approaches was tightly integrated with the culture and when things fail, there was a cultural and psychological closure to it. Here the dynamic analytical approach falls short of integration with the sociological and anthropological dimension, leading to frustrations. Moreover the weakening of traditional social organizaiton structures around designs and architectures are eroded by the analytical approach. These could not be easily quantified leading to poor integration with the sociological dimension and hence a large spread of anamoly in the dimension. On the other hand, the economic dimension is well integrated with. One might reason that there is a line of causality and energy preserved in economic dimension, such that the systems energy cost could be estimated around the cost of extraction and conversion, but there is a large source of normative bottlenecks and wastfulness which is unaccounted. These might be hoarding, speculation and restrictions that rig the market and hence all extraction bear the transaction cost of having to exist in a society of humans. The result is that the true cost of extraction is not reflected in the cost that the system attributes to the design, or more often, quantifies the cost savings arising from a design. The quantification of the market despite its shortcomings hence integrates well with design approaches that select analytical models as being more useful, while the sociological dimension is neglected as unmeasurable.
c) The third one is one of intuition. Every design involves intuition when performed by humans. These draw knowledge from an innate sense of aesthetics, inspiration from sources of nature and fellow humans and so on. Thus, designs have a certain mystical quality about them and having been blessed by God. This area is difficult of being addressed analytically. But we might say in lesser terms, that such designs are trusted and appreciated by the people culturally. Analytical designs do not carry any holders of trust. 
We had proposed that these are very important problems with the nature of analytical problem solving systems. Or otherwise, we can call them purposeful systems, to borrow the term from cybernetics. These systems could then be treated as phenomena and could be observed from desired vantage points and measured and compared. Hence, they meet the essential of being designated a science. This could be an extension of cybernetics or rather an elaboration of cybernetics to study such systems.  We may use methods of measurement that could go with classifying the sytems and ordering them as with biology and studying their (emergent) structural organization in order for them cooperate and achieve certain ends. The study could cover 
a)With respect to the reflexivity, a measurement of the possible impact on the subject matter, such as holding out and nudging the subject system to reorganize, the benchmarking to measure the descend into regression all help in providing useful metrics to categorize the system into some taxonomy and analyze typical behavioural patterns.
b)There might be developed formal prototypes of organisms which lend an informal proof platform to the organisms that are being observed and measured against such benchmarks for consistency.
c)There might be developed approaches that foresee the risk of catastrophic failure much ahead. This might be achieved by reversing the same method used by the cybernetic organism albeit with a different end of avoiding catastrophic failures. This might mean the construction of propogation pathways of simple organisms, such as for instance a system that manages the supply chain of a noodle maker. Such a system might over cost reasons select upon a vendor from a grey country leading to heavy metal contamination in the food, that gives side effects far further down the line and mixed with the general noise to be capable of distinction. Morality is supposed to stand guard here, but where cybernetic organisms are involved, we would have to develop systems of regulation and control beyond moral safeguards and the use of punitive deterrants. We would need to decipher the noise and see the side effects of specific product that is sub clinical at a certain point in order to construct its course in terms of social cost, rather than immediate economic value (which is tainted by speculative interests). Hence, this kind of analysis by a central system serves as a selector of cybernetic organisms and puts pressure on them to be tame.
d)There is needed of recoginition of trust networks to quantify the concept of intuition and trusted sources are deemed to come with catastrophe closure mechanisms and afforded an advantage.
This might be a new extension to the science of cybernetics. This might be a way to theorize the second order cybernetics of reflexivity into a new metacybernetics in which reflexivity might be seen and quantified and controlled, so as to retain the ability to observe and thus be a science. This might be the third order cybernetics.



Causal Decision theory and Cybernetics 
The possible futility of long term dynamic solutions
If we look at the problem domains from a cybernetic or control theoretic points of view, we see some interesting patterns. We see that while dealing with problems, we typically apply a static and linear point of view to make sense. For instane if we look at medical diagnosis, we see that we often define the diagnosis as stages depending on its meeting of certain criteria. The initial diagnosis starts with a certain set of symptoms and then we have more conditions which influence the trajectory as the disease proceeds and sometimes resolves inbetween or moves further upwards through the stages. We see that this approach, though evaluating the system on a multiplicity of factors is not strictly dynamic. It is nothing but an identificiation of the state from a finitie yet large combinatorial set of n-tuples defined over n dimensions. The temporal evaluation of conditions happens by the nesting of the evaluation structure to an arbitrary depth. The result is that while recursion provides some degree of dynamism to the solution, it is not purely dynamic. It is only a emulation of a possible dynamic model. There is a definite order and heirarchy to the evaluaiton of condition and the entire thing happens on a single thread, due to which the evaluaiton is scorable in a number line. But in reality, the processes of disease or any other complex system works in a paralell manner and its enumeration in a line is not often possible. Let us say, that we deal with a disease system, we could visualize it as a composite of paralell systems working in a common space. 
Let us say, one of the systems involves healing the microscopic lacerations in vascular tissue by sclerosis. The second one involves regulating the blood pressure depending on a control mechanism of peripheral sensors and a third one involves a compensatory alteration to the excretory mechanism to take care of the increased blood pressure. These systems work in tandem but each works to a loop. Some of these loops are positive feedback, such as the sclerotic loop. As the scar tissues form, they thicken the blood vessel, leading to incremental blood pressure and detaching plaques causing even further lacerations. The progression due to a positive feedback loop is often dampened by a negative feedback loop. This might involve a compensatory mechanism kicking that has global relevance, such as adrenaline levels. Thus we might say control is essentially a negative feedback loop and the trajectory to collapse is a positive feedback loop. These work in tandem and it could be the case that a single detatched plaque could set in motion a composite of events. This is often dealt with as interesing chaos, which somepeople feel to be journalistic hyperbole, but it might in fact be so, only that there is no way to know the initial tiny error that perpetrates itself. 
One needs also to appreciate the nature of non linear dynamics in order to proceed further. Non linearity is often experienced in real world systems not a whole number power values, but as discrete shifts in values that could not be fitted into a smooth curve. We might say, that in non linearity, things are vague, because the function essentially maps a line to a higher dimension, say an area, in case of a quadratic function. In realizations of such a mapping, one might see that there are random samples drawn from the area, which is now the sample space. How granular the samples are drawn from the sample space is of crucial importance. We might see that not often that samples are drawn at randomly switching spots in the sample space. The first draw might be close to the past draw and they might concentrate into a certain subspace of the sample space before switching discretely to another subspace. The quickness of the switches, its abdruptness might show some oscillatory pattern or otherwise. Either way, we might say that real life non linearity differs from mathematically smooth mapping between lines and spaces, by virtue of the granularity of the randomizaiton in the draws. We might also say, that the line might be a limiting case of a sample space and a mapping between two lines could in fact be considered in the same terms as a non linear mapping, mutatis mutandis. One ought also to know that a single mapping between two lines is not sufficient to determine the nature of the function analytically. We need atleast two mappings and the mappings might infact help compute the function as a differential function between the two lines. One more interesting thing is that if we are to map a line with a near infinite dimensional space, one would not pick up any signal but pure noise. For instance, if we should pick up all the dimensions in a populaiton and map it to their life expectancy, we see that there is nothing particularly that could be known about it, or that there is no information content in the model. However, if we could get with certain specific subspace, then we might find interesting patterns. One also ought to appreciate, that all differntial functions are in fact differential against time say dy/dx = dy/dx.dx/dt. This is because we talk about random events and these events are discrete in time. We only compute relative changes between values such as x and y with respect to time. Time is also not a dimension that could be built along with the rest of the dimensions into a model. It always remains separate and the source of randomness, the container of events and a dynamic model is valued only by its ability to extend in time with a suitable degree of consistency. One ought also to appreciate the problem sociologically. Socially, every individual requires that events arise as one continuous meaningful flow, while in physical reality, it appears to be random and unconnected with the past. This is probably the root of all frustrations. Say, for instance ,if a lightning should strike a certain person, the next strike is not conditioned on the previous one, but is a fresh draw. But a normal person would not expect the misfortune to repeat. Often times, reality shows a continuity of events, where onething implies the other, but sometimes they are also random, which is perplexing analytically. The implication is dealt with in terminology as causality in mechanics, but implication is a broader term than that. For instane, diffusion is nearly as common way to realize implication.
Thus, given the mathematical model and the importance of models of calculus to understand and appreciate the propogation of the system to dynamic changes, we might visit upon another and simpler system. Lets assume, we have a taxi aggregation application, which could compute the fares based on the terrain as we discussed above. But the terrain would be changing in time, because as we said, we could not model the gradients with time as a dimension. Thus, if there is a certain monsoon festival in a town and the gradient suddenly dips to the sea shore, then we would have a change in the behaviour of the riders and the drivers. If such a monsoon festival should happen, it would happen that people would be booking enmasse to go to a certain part of the beach and start celebrations and most of the taxis might linger around the same region till the people start off back to home. Should there happen a downour and an inundation of many of the major roads that lead out from the ocean, the taxis would have to negotiatiate a single narrow road. If one bottleneck on the road be created due to some lingering drivers parking their cars near a road work, it would cascade into a complete logjam, without anybody getting anywhere. If we analyze this situation, we might see that this involves a situation of agents who are goal oriented locally and they had attempted to maximize their gains. There are numerous feedback loops that exist between entities, not just riders and drivers but also between taxis. If the minimum space between the taxis in a narrow road should reduce below a certain threshold, then the feedback becomes meaningless and infinite loop sets in. This infinite loop would require someone to enter into the scene and manually regulate the traffic intuitively by supplying an interrupt. We might arbitrarily extend the situation to a substrate of fuel suppliers, who would have a discrete function which evaluates to zero beyond a point. So as the energy demand of the city increases dramatically due to infinite oscillatory motion of the cars stuck in a narrow road, at certain point, the fuel pump ceases working and the system stalls. This is the collapse. Hence, a system of cybernetic agents could collapse by developing infinite loops. 
As we might see, a regulation is needed. All cybernetic systems report to higher order goal directed systems, which control the underlying agents by a few well defined means. The methods of control could be classified to be diffusion based apriori or aposteriori, policy based again apriori and aposteriori, virtue based and education based. Thus, all questions of cybernetic goal directed systems involve a control of such systems themselves by higher order systems. Much of the control does not involve supplying an interrupt out of the blue but a steady dampening loop. Thus, in case of a diffusive control system, the demand on fuel naturally sends out higher price signals for the fuel, which would cause more taxis to go of the road instead of attempting to push through the traffic. Price signals are not directed upon certain individuals, but diffuse through the system. The price hike might reflect upon not only petrol distribution networks but also other people like food vendors etc.  In fact the free market is a very strong control mechanism by the state, by the issue of distributed tokens which is centrally regulated. Thus monetary system might in fact be a powerful central control. To add minor nudges to the diffuse control systems makes is an even more powerful tool and greatly supported by the monetarists. This is mostly aposteriori. If we look at the other mode of control, such as policy control, we would see that in an aposteriori mode, it would involve sampling from the output space of user actions and see if the action is one that infringes upon the rights of others. This presents very important challenges, one is that a sampling based discovery of anamolies would need to be backed by a multiplicative penalty, which would then be retributive. Likewise the enforcement of such penalties, would need to be delegated through (or diffused ) through a limited subset of agents, which presents the dilemma of control, as who would regulate the regulators. There might infact be apriori policy based controls. These might be something like five year plans to injunctions on specific actions. Either way, it involves redundancy. For instance an apriori rule that speed ought not exceed 50 kmph involves a lot of redundancy and a broad margin of error. Hence, aposteriori control is more preferred for its granularity. The idea however is that intellectual work in aposteriori control involves mostly forensics, like how we discussed in our earlier work on how to integrate cybernetic organisms to sociological systems. While monetarism presents a diffusive control, the use of forensics in its place to distribute penalties is discrete control attempting to attain similar ends. This is expected however to reduce the problems of systematic manipulaiton of money by inside networks which is advanced by strong critiques of monetarism. 
The use of forensics as the intellectual frame for regulation is also with some shortcomings. If one is to discover that there has been a certain transregression, such as for instance a food additive causing cancer in consumers, then the action of retributive nature would rely on how aware the agent perpetrating the use of such a chemical was of the possible consequences. This is the principle of mens rea in law. Thus, if the agent had randomly picked up an additive in good faith, then it ought to be treated as a random problem or an accident with no retributive action and hence no means of central regulation. However, if one ought to place mens rea, then one needs to admit that the problem could be detected by an apriori model. Then, we might say forensics could also rely on such apriori investigation rather than any methods of pure dynamical analysis. Thus, we might say, apriori static theoritical knowledge is necessary and sufficient for policy based regulation. The other types of regulation such as virtue, as being volunary restriction instead of relying on apriori regulations might work, but with redundancies. The fourth form of regulation that is education based is interesting. It involves getting the system to a stable situation, by causing the lower level cybernetic systems to acknowledge and understand the goals of the higher level cybernetic system.
We might say that in modelling the taxi network on a single computer, we are in fact attempting to simulate the market. We attempt to diffuse the signals by regulators and expect agents to pick up and propose their prices in the virutal world which could then be used in the real world. The fundamental aim of such a central regulator is the stability of the system. Thus, if the system finds high energy systems such as cars concentrating and oscillating wastefully, while people with high potential energy also wait in places, then the system is fundamentally unstable. A stampede might ensue as the high potentials attempt to cancell out. The system would attempt to run cool, by diffusing the tension, may be by offering incentives along certain routes. This is an aposteriori control mechanism which could work. There might also an apriori planning by restricting the maximal density of taxis in a region. Either way, the catastrophe starts with a single taxi parking illegally and a downpoar. The failure could be avoided by listening to weather predictions and modelling them to diffuse dampeners in the network. But unless we know that the erring taxi driver parking on the driveway has mens rea, there is no way to establish accountability of the situation, inspite a high power forensics being able to replay the situation to the root cause. The model is of no use. Thus, in the real world, there seems to be a very high preference for causality. Somebody ought to have caused something and it has to be deterred. Thus, causal modelling acknowledges the notion of good and bad causes and the presence of authority to deter bad ones and encourage good ones. It is a conservative world view. Therefore, one might say that causality allows one to have a workable model, on which feedbacks could be launched and the world rendered meaningful in a battle of good and bad. The use of pure diffusive controls on the other hand, acknowledes no causality, but only the emergence of certain accidents, which could then be controlled by advancing the dampeners for instance. There is no good and bad but only desirable and undesirable happenings towards which the central regulator could nudge. 
Thus, we approach a world view, where we say, that building sophisiticated analytical models relies entirely on diffusion. The ideas of retribution, closure and the establishment of reasons rather htan randomness is not acknowledged in the diffusive model. If the diffusive control model is however given free rein, we might see as in the taxi instance, there is no way to avenge or to control by means of good and bad. Everything becomes a game and the outcomes determine the course of life. One might not need a higher order control, but rely on emergent order and one might align with camps as a purely intellectual exercise, rather than moral excerise and gain or loose to ones decision. This state of affairs we might say is a novel window in our exposition. But it is not always that there are notions of good and bad. There might be somecases where there are genuine games. For instance, a foreign incursion might be legitimate on the point of view of the invader or even an infectious disease might be legitimate from the survival of the parasite. In such scenarios one would need to adapt dynamic approaches. But even in such a case, knowing the cause of how the randomness is able to fare better is always helpful. For instance, a historical wrong might have disposed the enemy to plan an invasion or perhaps a weak border caused the entropy to seep in. Likewise, one might fix repsonsibilities to the governments that fail to manage epidemics rather than germs themselves. Thus, randomness is properly adapted to causal perpetrators in every case in order to attain closures and meaning. The modern method of dynamics so different from the aristotlean and platonic methods, lead often to attainng power in the place of meaning (as was commented in homo deus cite). The use of dynamic methods can run simulations of reality to a good extent. The simulations helps to appreciate the direction in which the system is heading but nothing can be done to lay down policies or to penalize unless causality is fixed. A general tendency of the system could be managed not from outside of the system, but by being participative in the system as with second order cybernetics and being able to decisively nudge the system towards certain ends. Thus, one agent might be goal oriented more globally than the other and lays out a coercive framework to collect taxes and apply it to maintain the commons. This is a rather tricky situaiton, as to how one agent being within the system in a manner detrimental to classical observation is able to coerce and control the system. On the other hand, there might be employed dynamics in attaining a virtual super system, by selecting from the subsystem agents to some kind of a self regulatory panel, such as with the federation of industries. Thus, control necessarily involves observation and how the observation is managed legitimate by suitable dynamics might be admissable in theory. Likewise, it is possible to employ dynamic methods to nudge the system, but only those leading to concrete proposals, that could enrich apriori knowledge. That is to say, all dyanamic knowledge could do is to run simulations against scientific apriroi hypothesis and strengthen them. Whatever knowledge generated by them is not actionable because they are not causal. 
If we should however look at a case where dynamic systems are treated as cybernetic organisms and be treated by special audit systems to regulate them, by dampening any possible misadventure, we might have a different case. Lets presume the taxi system is able to source fuel from a grey source, which is risky in terms of emissions but the taxi regulator knows the effects would take a decade to make itself felt on the regulatory authority’s sensors. It predicts meanwhile a full transition to electric might be obtained and the noxious fumes might diffuse into the higher atmosphere once the transition is made. The system might reason not just in terms of economic incentives but also in terms of long term risks. But it is always plausible that the system optimizes so hard that it might enter infinite loops, such as with taxi logjam. Even, biological systems like the spiral death of ants or even paranoia in schizophreniacs are known be manifestations of such cumulative infinite loops. Therefore, the system optimizing so hard, would then have to be controlled by modelling their future course and and actually simulating the system along with other systems in a super simulations and this time placing greater weight on avoiding catastrophic failures. The outcome would again not be a policy regulation because of all the uncertainty as both are just simulations and would instead be only a game equilibrium. The potential for infinite loops is not vitiated in a dynamic model. Thus, we might say that all dynamic modelling in cybernetic organisms are no more than empirical fields that could vindicate or vitiate hypothesis and these hypothesis contained in closed form algorithms (or offline, neatly versioned, serialized computer systems) are the only the legitimate forms of knowledge. Thus, we might say that even in medicine, dynamic prognostic approach and the use of sophisticated mathematics like lagrangian dynamics only serve to recaliberate or launch a neighborhood search for hypothesis or vindicate an abductive hypothesis and not much beyond that. 
Further investigation along the lines of do-calculus, inferring causation from statisics (which however is difficult of application because mens rea involves apriori knowledge of causality) or it might proceed in the lines of causal decision theory and how it integrates with cybernetics. The recent advancements of cybernetics by umpleby is also relevant.
In modern media renderings, one often sees that there is a bias to criticize authority and view upon self regulation as being possible and reflecting upon the golden age of some romanticized past. The problem is that self regulation in all times had appealed to some authority, at its very basic, a moral commonality among men. If it should be pure game positioning on a dynamic basis, it is no better than an evolutionary niche and the possibility of infinite loop theoretically arises. This is a theoretic and not an engineering or normative problem. Once an infinite loop arises, it is physically unsustainable due to the law of thermodynamics and some random error would appear leading to a random progression of the system. This is what empirically happens in evolution. A random evolution of the system as a whole as the idealization of progress causes people to evolve into something else, without any notion of the past, much like how we don’t reflect culturally upon over ape past, but we do upon our feudal ones. That would be probably the end of stories, including this one.
Science vs Metaphysics
Or why Bayesian statistics is metaphysical and not scientific.
1.	We will first try to describe what is a decision. A decision is a certain action which makes no sense to the environment, irrespective of the degree of intelligent of any component of an environment. This might be a well matched game or a harsh desert one is setup against. The decision which makes sense to any opponent might have been ‘caused’ by the opponent, by influence, diffusion or through a chain of actions (imagine setting the course to a trap). This notion of causality is broader than the mechanical conceptualization. 
2.	The notion of free will and randomness of causation is taken as a unique human attribute. Hence, we might say in inverse that no other part of the universe has such free will, other than fellow human beings. This is the axiom of choice. 
3.	The universe is hence random, as inferred from the conservative nature of the physical laws. In the long run, all interesting phenomena are only local. This is with the exception of entropy law, but even this is moot due to the proposition of dissipative structures and similar emergence of cooperation in a metaphysical plane. A very highdimensional space when examined from a single quantile produces random outputs only. 
4.	Therefore the axiom of choice becomes the substrate of ‘belief’. The belief is however seperable from expectation. That is to say, while humans believe that they are unique, they would expect to see contradictions in terms of conspiratorial suspaces of the random universe, or non random happenings in the universe as phenomena. Some of these phenomena are good, or desirable and some are bad. The ability to judge what is good and what is bad is an apriori in a Kantian sense and hence we might say, while humans believe in their uniqueness they expect to see pralells in the universe and some of them might be bad and some might be good.
5.	Thus we might say, humans seek randomness of the universe in principle, but never expect to see it perfectly random. They would expect to see bad things they want to work upon so as to make them good and good things they want to reflect and be happy upon. The natural human behaviour of attempting to disperse any bad phenomenon, by mechanically fixing them or helping others is matched by their attempts to disperse good phenomenon in dispersing it as charity and work. Thus, the human condition is recursively defined, as an attempt to see randomness, prove their uniqueness but always expecting to see such a proof impossible and having to work to attain it asymptotically. 
6.	We say that implication of these notions of expectation and belief into decision making makes the method metaphysical. Thus, theories of decision making and even Bayesian networks are metaphysical. They attempt to implead beliefs and expectations into the system, which appeal to aprioris and hence metaphysical. We might even say that statistical problem solving by any form of two pass smoothing is metaphysical (as the analogy of Ulysses decision theory problem attempts to demonstrate).
7.	We say that metaphysical methods of problem solving had been superseded by the scientific method of reasoning in history as the enlightenment. It is not uncommon for people to forget historical achievements and use older methods. But the argument as above is informal. There might be inherent goodness in the metaphysical method and we do not go into it normatively. It is just an exposition of what is scientific and what is not.
8.	The scientific method involves deciding a problem based on a given set of axioms. The axioms have normative basis of being utilitarian and instrumentalist, but the methods involve attempting to frame hypothesis impleading general behaviour from abstract models such as in mathematics into the particular case. The hypothesis is then attempted not to be supported by evidence, but by determining if the observation of correlation is fit enough to be confirmed given the complexity (or noise) of the system represented by the model. Therefore it might prompt a greater componentization of the model or extension before a hypothesis is confirmed and made part of the theory. 
9.	Metaphysically science is instrumental in helping society solve problems and liberating the metaphysics of the problem solvers from having to be influenced by material results. The metaphysics of problem solvers in general might flow as romanticism and science could take care of the material part of existence. Likewise the practice of science itself might be a repeated confirmation of an inherent order in the universe and the ability to overcome unfriendly phenomenon by friendly ones or bad by good ones. We also consider pure randomness to be bad in this point of view. 
10.	Hence, metaphysically science leans on the side of philosophy with respect to its treatment of randomness. Science considers pure randomness as having arisen spontaneously and increasing with time. Such a randomness might corrode both phenomena which are desirable and non desirable. But it also corrodes the models of science which represents phenomena being studied. The corruption of these models is undesirable in science, because it looses informative ability over time. Science is optimistic about its models and hence treats randomness as bad. 
11.	The method of reasoning that relies on network inference does not explicitly model the environment, but obtains information from the environment itself treating it as a graph of interconnected influence pathways and observable frames (edges and nodes). We might consider these as fields and objects or in other analogies. Hence, it is agnostic to randomness and explains locally based on the orderliness. Hence, while science might be opinionated, the reasoning from belief based influence networks is local and involve bounded rational agents. It has no global conceptualization of humanity or any entity. It is the bounded rational agents who solve problems. However it is optimistic about the ability of such orthogonal agents to attain equilibrium despite entropy. At this point, we might argue that it is opinionated. That is to say, statistics argues for an inherent order to things, which might be realized by unassuming and local work. This emergent order is the central notion of the instrumentalist proposition of statistics. 
12.	In science statistics is frequentist and hence subserved as null hypothesis rejection tools. But a global statistical network view point or bayesian statistics supposses that no theory is required and it is culturally supported at present because of the historical truth of such theories causing factionism and wars. Many of the theories of imperialism was defended on pseudoscience. Hence, the principal danger of having an authoritative science is its possibility of usurpation by pseudoscience. So, the world said, lets be done with science as an authority. 
13.	The notion of causality is central to science, because it implies agency and is embodies the notion of work to attain goals. The objective specifiability of goals indicates the ability to have an agenda for all humanity, as with the enlightenment. 
14.	The notion of ethics has been to disperse the orderliness in nature, or causations in nature by appropriate work. What is good is to be neutralized against what is bad. This would involve the ability to generate objective labels and invoke concerted work through specificiations. In statistics, there are no objective labels only narratives by bounded rational agents. The Witgensteinean notion is critical of such scientific labelling of good and bad as a zero sum game. For instance one might argue that instead of spatially dispersing bad by good, one might as well do it temporally, by hoarding the good against possible bad events of the future. The notion of spatial justice is limited and is not theoretical antogonism to temporal justice. The fomer is labelled good while the latter is labelled bad, in general. There might not be a general defintion of good and bad things, but there might be particular definitions (I know it when I see it) in a different kind of epistemics. Hence, ethics might demand a particularity to decisions and hence back a dispersed agency rather than a central, authoritarian one.
15.	The ability to codify and realize at a national level, the concept of ethics had been the pursuit of dharma atleast from Ashoka onwards.We might hence say that truths represent good things, such as when we can see that whether we could distribute or hoard, our apriori ideal might guide us. Rather than saying, ethics is local, we say that ethics is individual and private. It could never be the subject of any epistemic study. We might have it that the pursuit of knowledge gains consciousness only after the definition of the axioms. Anything prior to it belongs to philosophy. Invoking peace and ethics in the pursuit of epistemic knowledge, such as with Bayesian statistics, is a proposal to follow the metaphysical path to truth, rather than the scientific one.
16.	Circular causality could help bridge the gap between emergent phenomena which is already intermingled with human action. Say, a study of storm clouds might be done scientifically, but where the biological system is to be studied, while people are making decisions on controlling it already is difficult in science. The axiom of indepdent observability is violated. But it also makes sense to understand systems that are evolving in a multimodal epistemology, even if human agents are not part of it, like gut flora. In examining such systems one might not hence arrive at fundamental and objective truths, but only as equilibrium configurations of the whole. Such equilibria like Homeostasis or aerodynamic equilibrium is important practically.
17.	To understand equilibrium configurations of distributed systems, one needs to look at past history and apriori notions of symmetry, such as there being conservation of energy, by least energy principles and stability stickiness as with lagrangian dynamics. The dynamical method examines equilibrium configurations as stable states (as nodes) and its range of oscillations in terms of a recursively granular subset of states as subnetwork of nodes. The network could be recursively constructed to a full graph. The graph might then be capable of analysis on its patterns of oscillations in time and the ability of interventions to regularize some oscillations by eliminating chaotic residues to it. Thus, an aeroplane might continuously oscillate between stalls and dives to maintain equilibrium, but the control tool dampens chaotic escalation of oscillation (into a possibly larger cycle like the ENSO) by small control signals. 
18.	The use of dampening control signals as in engineering servo mechanisms, do not produce objective knowledge but solves problems on the field. Many problems of engineering, biology, chemistry and even economics and organization theory could be well posed as problems of stability. If control systems could be employed, there is only a need to ‘develop’ systems and not build them. The development and control of dynamic systems hence has a broad utilitarian course, not easily dismissed.
19.	The ability to represent dynamic systems in statistical networks is well established, in statisical mechanics and even gene is a statistical concept (cite P Lukan – Probability and metaphysics). Thus, it is possible that it could eclipse the concept of theory itself as with the empiricist philosophy. Construction on relatively inert environment might however not be dynamic, as with standard civil engineering and treating it thus, would be detrimental. Geometric and newtonian physics would work very well for such engineering. The question is whether these two sets of problems are separate, vaguely reminiscent of the NP/P problem in computing. 
20.	The primary criticism we had on attempting to solve NP hard problems was that such dynamic computing looses independent observability and meshes with the system at hand, leading to solutions which when they fail, have no causal backtracking, leading to a lack of closure. It becomes pointless, unprofessed and unowned endeavour. Just as equilibriums could emerge, they could also collapse. There is nothing in dynamics that could explain or even meaningfully accommodate collapses. We had also mentioned that dynamic models are better where none exist and they should ideally try to be applied where clear boundaries exist from the real world, such as where such engineering utilities are expendable. 
21.	There are dynamics which is mathematically describable, as with railroad dynamics. These could be formalized in convergent mathematical series. It is not a solution, but an asymptote which has infinitesemal failure margin, not much worse than the explicit noise in reductive modelling. The ideas of infinite series and anymptotes thus moved mathematics more in the direction of analysis (which stands for dynamic analysis mostly) while statistics was gaining ground as well as a way to statically make sense of it. Dynamics was the method in practice and statistics provided a framework for epistemic appreciation by building an influence model, rather than a causal model. Hence this came to be called evidenciary decision theory.
22.	Deciding based on evidence and contrasting it with causal decision is however only a part of the debate. We would now visit the full import of the debate.
23.	If we reflect, we might see that dynamics is not an epistemic proposal that is poorer to static modelling. If we in fact model the system in terms of equations in a static setup, we do it with differential equations in a dynamic setting. We can still propose first class scientific truths with the dynamic method. For instance in flight dynamics, if the rate of change of wing span correlates with the rate of change of gliding distance of a coasting aircraft in standard atmopsheric conditions, then we might have the base equation of the analytical equation capable of being derived therefrom and one can decide between two possible wingspan designs based on such equations. The equation might describe the length of the latus rectum of an elliptical trajectory based on the solution for the differential equation.
24.	Likewise first class truths of the static structure of a system could be derived from equations of dynamic behaviour (equations themselves signify, we relate the parts of the system algebraically that is). There is always a presumption of the infinitisemal in mathematical analysis, which leads to minor asymptotic errors, which as we said are no more worse than static measurement approximations.
25.	We might infact place the mathematical basis of statistics from dynamic theory. Statistics considers as discrete ‘events’ the continuous variation between linear systems. The event space of a probability distribution is actually the integral of a dynamic function. The function as the integrand could be derived as an analytical equation of a curve, say the gaussian distribution curve. Likewise, the multivariate gaussian curve is not much different from the shape of multivariate differential calculus. Even the shape of the curve resembles the solution to the brachiostone problem in functional calculus.
26.	The frequentist probability representations on a given linear quantile might be seen as integration to a dimension and probability values could be obtained by integration over definite intervals to a particular axis in the cartesian space.
27.	That makes it pretty clear that dynamic solutions do not loose their scientific rigour just because their equations are one level higher than standard equations. Every equation might be considered as an integral as well as an integrand. The Taylor series provides clues to this viewpoint. Epistemically the infintisemal is no worse than a measurement error.
28.	Now if we look at the dynamical method of problem solving, we still have a frame of reference or a frame of knowledge (cite Minsky). The aircraft winspan and the force of gravitation is modelled in a frame, which could be imagined as a cube into which we have modelled the statistical parameters of the fluid, namely the viscosity, temperature and density of the air. Without this frame of reference, the dynamic models might become open ended, which is what we are going to explore further down. We think, as long as the dynamic model provides knowledge on the internal organization of a static frame of reference it is legitimate knowledge. Our point of departure would be where the dynamic model is open ended.
29.	We might think of Bayesian statistics as an open dynamic model, because it keeps the subjective notion or the hypothesis itself open to feedback. The feedback from the system being observed, goes back to influence the observer, making him shift positions, making the method metaphysical rather than rational. This is because the observer is a mystical person, who could thereafter attempt to modify the ‘method’ based on his ends. That is to say, an observer constantly influenced by what he sees could not stop himself from adding additional nodes to the inference graph and thereby modify the formal structure of the problem to reach specific solutions. This method is hence metaphysical and not scientific.
30.	Likewise, in computing, we can think of the system being able write its own program. In computing the network that integrates across domains influences the programmers to change the formal structure of the problem based on feedback, particularly given the recent process popularization as agile and continuous integration. In Bayes networks as well, the backward pass smoothing allows the revision of priors, leading perhaps to a divergence to a completely different branch of the program, which was hitherto ignored. Thus, the program has dynamically reconfigured, does not decide the problem, only constantly updates probabilities in each node, which in turn cascades changes to other nodes. Solutions are local and transient. The system is not framed, started nor stopped. The solution system is specified only by the number of nodes and edges, which is very minimal to make the solution well formed. It is thus not a well posed scientific approach.
31.	Hence, we might say that open dynamics is a methodological situation. But we also ought to examine problems in which the integrands could only be reverse engineered using statistial methods, reconstructing it from discrete events that resemble the integral. The use of numeric methods, such as the Finite element method to reconstruct the original function depends on approaches from dynamics such as lagrangian error minimization (simplified in the least squares method) and apparantly it is successful to a good extent in creating the structure of the problem to a good extent. In problem domains such as climatology, these are the only options available. They could still provide valid truths such as comparing two storms to decide on specific measurement criteria, perhaps to choose a navigation path.
32.	The use of such reverse engineering statistical systems could be very useful. The perceptron is one such system, as the name signifies, it is able to cognitively percieve the system, by using simple sensors, encoded as to reason over a complex problem topology. It hence is able to repeatedly apply statistical and dynamical methods to represent a mapping between geometric concepts and logical truths (with the excluded middle) or provides a smooth central regression curve. The method might be scientific, again depending on whether the network is contained in a frame of reference. That is to say, if it aids scientific observation in a repeatable basis, it is scientific. In case if the network that determines the structure of the problem, feeds back to the observer to modify his stance then we might say the problems formal structure is modified. But even conventional science gives feedback to reject hypotheses.
33.	Therefore, we might say, that the scientific method distinguishes itself on the frequency of the feedback, on the relative stability of its axioms. Its stability is higher than the open ended dynamic method, but what is a higher stability is an arbitrary concept. Even neural nets are not unbounded automatons, they evolve in discrete but distributed steps (much like the computing networks developed in agile). 
34.	We return to our question on ethics. We had seen that the idea of spatial neutralization of non random events (pitching good against bad) is charitable, but provisioning to temporal randomness is not. This is arbitrary. The idea of what is bad might in fact be exemplified in falsehood. If a certain proposition is false, it is only so, because it contradicts with a previous proposition. Previous propositions hold only in the civil society, in military social perspective, there are no falsehoods, there are only ‘sides’. Each one is true to his side. In civil societies, people express falsehoods while they have ostensibly signed into the social contract, hence they are seen as free riders and queue cutters. 
35.	One might as well reason that the falsifying agent in the civil society is infact a military opponent. That gives psychological comfort in placing him as the enemy and setting an agenda for avenging ones ‘side’. This notion of side is heavily dependent on the ability to logically and discretely differentiate between militaristic actors who corrupt the civil society (say one considers them as spies, detractors or any such). But it might also be that there exists a continuum of how much one is integrated into a civil society. The society itself might be pluralistic and dynamically oscillating about an equilibrium. But the ability to logically distinguish inside and outside, to an arbitrary degree of discreteness is important for psychological comfort. 
36.	Thus the notion of good and bad, might boil down to sides one takes in some kind of a military view point where civil engagement is mostly a veneer over a battle essentially to determine the ‘truth’ adversarially. The goodness and badness involve a struggle between evolution and morality. This might be the one true battle, metaphysically, so extensively culturally discussed (cite bedazzled, faust, a winters tale). 
37.	The inability to frame this battle becomes a psychological crisis. The absence of appreciable (not purely discrete) boundaries between sides, is as difficult as having a poor boundary of the self. Thus cognitive distortion by high frequency dynamic solution stream, might cause the boundaries to blur and create a psychological crisis of modernity.
38.	In this exposition, we did discuss the solution as a stream and a flux (cite). The dynamic approach to problem solving, where high frequency feedback from the problem domain causes a quick and distributed versioning of the problem solvers could thus be seen as evolutionary. On the other hand, a system where the truth is framed and recursively reconciled to result in singular truths is seen as being essentially static (even if it involves nested dynamic methodological components). Thus the point we discussed on scientific method being distinguished only on frequency may not be accurate. The scientific method distinguishes itself rather on the pursuit of singular truths. This might be asymptotic and provisional even (cite Popper), but the determinaiton of singular (or objective) truths is what best describes the sceintific method exemplarily.
39.	But again that does not leave out the grey area where the provisional nature of the truths is exploited by metaphysicists to signify an arbitrarily high degree of to allow for direct metaphysical methods to be applied rather than facts being documented. The idea of the asymptote is based on commonsense (as in the asymptotical convergence towards objective truths) would need to be reinforced by the philosophy of science. This metaphysical methods like agile, bayesian statistics, computational network based problem solving, coevolution are powered by metaphysical narratives of postmodernity. 
40.	That being so, science ought to defend its own metaphysics. The scientific method brought into consciousness only on the beginning of the experiments due to its denial of aprioris, is easily outwitted by loaded methods of problem solving. Hence, the only option seems to be that the metaphysics of science to be strengthened and its method ought to declare to an asymptotic pursuit of objectivism and be prepared to strongly criticize the divergent approaches to problem solving as being metaphysical.
41.	Thus, what is science essentially boils down to whether a convergent solution to problem is sought or a divergent one. 
42.	The instrumental value of science (rather than its definition) arises from it being a convergent solution that is an antithesis to the divergent nature of cultural pursuit. Hence, it stabilizes the society. This instrumental formulation relies on the premise of a dualistic conceptualization of the universe. If there is needed for systems that are dialectic, this presents a stable arrangement. This does not apply where one relies on monistic conceputalization. But dualism is important to appreciate science and we also feel it to emulate the level of comfort obtained by the better definition of the psychological self.
43.	This discourse would be incomplete if we do not appreciate the view point of applied sciences. It is in fact legitimate and common to assume a divergent world view while solving problem by engineering. It is yet that pure sciences are concerned with the convergence of knowledge. 
44.	One might as well speculate that the dualism is not objective at one particular level, say of culture and science, but it is recursive in that there is a dualism of divergence and convergence within the convergent side. Likewise polity might search for universals within the cultural side, much like yinyang depiction in chinese art.
45.	Getting further into the nature of engineering systems, if a system is so designed as to predict the future state of a system from its present state, then we might decide on how to spread ourselves. If a predictive analytical system is able to predict the outcome of a medical condition, lets say a state a&b&c lead to outcome o1 and state eORf leads to o2, which are polar opposites, then we can spread the resources accordingly. That is to say, taking up a high dimensional event space consisting of individuals, if there be clusters of individuals (sharing certain features) such as a or b and they happen to have a certain predictability of outcomes, we might then say that for the cluster a&b&c we might provision ourselves to intensify care and for e and f clusters, we can discharge them quickly. There is likely to a broad noisy space between these distinct clusters which suggest no meaningful strategy. But the presence of clusters, ie a set of initial conditions which converge to the same output, as an attractor is mathematically recognized. Likewise the concept of multimodal distributions also exist. Causally, they may be difficult to analyze though.
46.	To understand this concept one needs to look at how the Markovian structure works. Logic has the additive connective and the OR connective. That is to say, we say a few things add up to a whole. But we also say that several things converge to something. These ideas of convergence and things combining together to produce a new concept, particularly the former might be seen as teleological. Logic is undirectional and events can be seen as evidential precursors or as independent causes for events of interest. Hence it is teleological as well as inertial. 
47.	In a Markovian process, we see that the system is charted as points in a n-dimensional space. Mathematically, we can see each of the events as a point in the space and some points might be connected to others. In vector algebra, we might see that the points could be joined by lines. These lines might intersect for convergent vectors and joining these points and the solution together might be a single line being a vector, much like a matrix row entry with no interesting RHS. These lines actually curve if we represent the points in lower dimensions, say by paralell axes. But placing these points might be difficult in a line, where more than one line cuts through it and they have other points which are not in the same plane. Then curves becomes imperative even in n-dimenisons. The main reason is the multiplicity of equations, which may not be completely solvable to roots. To avoid the curves, we can join the points through edges and call it a graph.
48.	For the graph, every point contains the complete information. That is to say, if we say a bully grows up to be a manager, he might be too authoritative or we might say that his present position is explained by where he comes from. To achieve Markovian property, one might as well increase the parameterization or dimensionality to quantify the bulliness index for instance. But it is not possible to quantify every parameter to completely represent the individual at a point. Hence, we might arrive at a probabilistic technique. 
49.	In this technique, we set aside all the other past influencers as probability functions. A bully and a manager might turn out to be a good manager with a certain probability and this could be arrived by understanding how often the good manager trait occurs at random and how often it is conditioned on the past causal trigger. The causal trigger might also be reasoned to be an observable evidence of a future outcome as is dealt in most literature. This gives us the Bayesian probability formula. But at the node level, we might perhaps argue that it is frequentist.
50.	This Bayesian estimation of the predictability of the causality of a certain node to a certain effect or synonomously, for the node to act as precursor makes the Bayesian network a subtype of the Markov network. The d-seperation of Bayesian network also indicates a Markovian property. This is however revolutionary that we estimate the probability of single events and hence subjective (cite Lukan). There cannot be an objective frequentist probability estimation of such node to node switches. 
51.	This presents an interesting problem. The use of subjective methods of probability produces no scientific or objective knowledge. A hospital attempting to estimate disease prognosis, attempts to optimize to its priors, say its policy to treat a particular demographic as well optimizes to its specific advantages in strategizing its spread, say it finds it difficult to acquire high cost equipment but space is not an issue, this comes to influence the pathway of the graph model. 
52.	This kind of optimization or more formally, control to specific objectives that is of importance to a given goal and state in a higher dimensional space is common in engineering, again going by the example of flight dynamics. In general these control mechanisms are instantizations of pure scientific knowledge and the goals are specified and stable for a longer time. Where a bayes net is adapted, the system is open to the arbitrary revision due to the larger world, say economic cycles require a revision of the prior or the spread target, or more formally the goals change due to external reconfiguraiton of the system. Thus, this is a control problem of an open looped system.
53.	The use of subjective probabilities to estimate control signals and in general use of markovian processes to subjectively, or more precisely, locally decide on the system leads to a divergent world model in terms of technology. Thus, it might happen that technology is sucking the wind out of science and they come to antogonistic terms. 
54.	But as discussed science has more than instrumental value to the society, it is the essential fabric of psychological health. The development  of high throughput computing that could weave the world into a probabilistic model which can be evaluated arbitrarily in terms of beliefs and expectations is sufficient for most technical problem solving, but it leads to an evolutionay distributed model of development, which is divergent.
55.	Altnerate apporaches which causally and plurally model the world might be helpful, such as mechanically modelling a case where certain factors like blood volume going low causing hypoxic conditions and its ability to cause irreversible tissue damage. But we also know that the system is not passive and the intervenor is not the only one trying to fix the problem. The body compensates, say by adrenaline production. Thus, intervention might be planned by complementing the signal (which might be chemically similar to the intervention drug) or by altering other parameters of the mechanistic model (by surgery say). Hence, a mechanistic construction does not preclude a dynamic approach. Only that it involves keeping a closed loop from the world in general and using concepts of causality, where possible. This is because, causality would not need noise adjustments and hence appeal to subjectivity and context. Some other forms like diffusion could be likewise modelled, but not a general correlation. 
56.	Cybernetics might be seen as another term to understand the dynamic model. Mostly cybernetics is just controlling a dynamic system, keeping the frame of reference intact, atleast in first order cybernetics and hence just applied dynamics. Using dynamic models one can prove more reliable control can be achieved than using subjective probabilistic models. Thus it is that engineering approaches might be applied to complex domains like flight and meidicine to produce good results. 
57.	Engineering by definition involves the use of deterministic models. Evolution is not engineering and it is open looped, pluralistic and there is no way to delineate it as a formal exercise. 
58.	Apart from subjective probabilistic models, models that involve filtering and smoothing of averages from relative frequencies, relying on objective notions of stability and scattering of errors is not wrongly poised. It might involve a picturization of the system, just like how our eyes picturize the world for us. Human eyes infact rely heavily on belief and expectation of what one wants to see, to quickly complete the picture and hence essentially bayesian. In fact an important step in science is to clean up this subjectivity. A proper scientific observation would need to be done in the mind, rathe than in the eye. Once something is observed, one needs to close ones eyes and picturize it abstractly. Perceptrons or other neural nets rely heavily on feedbacks to revise their priors. The term for feedback here is back propogation.
59.	The perceptron is a learning machine, which hence tunes to revision of beliefs, for learning is by definition revision of beliefs and it relies on test goals to realize what its beliefs should be. It may even start with a random prior, but over time, it ends up being an open loop, much like the markov chain or the bayes net.
60.	This attempt to eliminate subjective probabilities from science and engineering is however a cultural move. People like Judea Pearl have worked with techniques like do calculus to help provide momentum to a possible alternate. We might side up to the causal side and perhaps construct models of dynamics to provide similar results in the interests of science. We might say a method is scientific if it is framed to work between two known states of the system and not a cluster of states. Once many states are introduced and their respective transitions are built in the system, it becomes Markovian.
61.	We might as well try to tame the power of subjective probabilistic approaches and use them to supplement scientific observation, as we did with the human eyes.
62.	There is nothing perse with the technology of neural net or subjective computing. Only that it is not scientific to do so. It promotes divergent and subjective problem solving, which eventually need other dynamic, dialectic balances to solve the problem. It is like the gene machine, we carry around. It is as discussed a choice between evolution and convergence as a basic ethical question.


Notes:
Titles:
The Crisis of Science as a cognitive component of psychological crisis.
Scientific metaphysics and the convergent realist hypothesis.

The paper is however a hypothesis on how the postmodern monistic conceptualization destabilizes the psychological conceptualization of the self. The field is too immense to be capable of empirical measurement. But so for this is a cognitive model, which relies on the axiomatic narrative of the distinction between the self and the other is to be maintained in order for human cognition to function without delusions. In this case the cognitive distortion by the environment is capable of inducing an unreliable and shifting model of the world, leading to delusional coping mechanisms or an affective afterload which destabilizes the affective equilibrium. The part over environmental distortion of cognition as being detrimental is well established. The part on the distinction of self as important too is well established. If we could relate the environmental distortion as arising from divergent problem solving, then we can perhaps attempt a quantitative model of how the shift from a high granular divergence to a low granular divergence could destabilize a given system in theory. If this is possible to prove this hypothesis mathematically. 

The hypothesis is that if a system be composed of n components in equilibrium then increasing the components to n+1 while conserving energy would dispose the system to greater instability(see note2). We consider the sociological construct of the society as not a self stabilizing system but as one which could be introspected upon and hence controlled. Thus, we might also consider the computational complexity of the control problem of the system as correlated by its component cardinality. This might already be in the research base. If that be so, we might speculate that postmodernity as being destabilizing and control as being compromised. We would then construct our theory on reduced control by introspection as being cognition distortive. These are weak causal correlations and unsubstantiated by evidence, mainly because of the noise, considering the breadth of the domain spanning sociology, systems theory, computational theory and psychology.

But if the mathematical inevitability is established, it would still be a coherent model, which is beyond a hypothesis (within an axiomatic theory). The model could be free standing from theory and come with associated arbitrary noise and proposed subject to empirical verification, as is often in climatology. Hence the proper title might be

A systems model of the society attempting to simulate/mathematically equate the influence of postmodern problem solving approach and its effect on cognitive evaluation of the degree of subjective control 
If the model is to go beyond simulation, one should attempt to establish mathematical equations that relate the complexity of the control problem to the granularity and thereafter speculate that the loss of control causes internal model of the controlling agent to be distorted, to a degree that his fundamental interrupt (as with paranoia) fails to function. We may also model all affective problems as arising from the paranoid disposition, of anxiety being suspicious of malicious natural agents and depression being lack of hope due to the overestimation of randomness. The cognitive deficit in guaging the uncertainty in the system might set it into an infinite loop, such as when a paranoid individual is liable to suspect advances to assuage it (interrupts from outside) with suspicion. The loss of the command port is seen as the common pattern in all mental illness. This depends greatly on the overstimulation of the agents internal world model due to rapid distortions, leading to the energy to control it, leading to the loss of reflexive control from the cultural part of the society, leading to a spiralling detorioration. Thus, we might even separate that the society to be dichotomized, one as being functional and the other orienting (such as culture). Hence we might not need to put forward a reflexive relationship with the society. It might rather be that the high noise in the functional world seeps into the individual and his coping mechanisms in culture is overwhelmed. The control loop with the society diverges and the person becomes dysfunctional or more rigourously his privilage to control the society is rejected. 
Thus we might consider a multicomponent system being controlled by an agent with an apriori external mystical control. It might happen that the multicomponent system develops dynamics making it difficult computationally to control it (which speculates that if the control problem is a linearization problem) again leading to the rejection of control privilages of the agent. This is a fully computational model of psychology.
Note2
The case of increasing the number of components of the system while conserving energy seems paradoxical, since the internal energy is inferred from the number of particles in the system moving about. It should rather be a conservation of space. The entropy increases with the number of particles in the system (because things spontaneously break down in entropy making it a rather tragic law). An increase in entropy is associated with the channel (or rather the medium becoming more uncertain) leading to high error correction demands on the control system in knowing the current state. Thereafter the control system ought to plan on issuing control signals. The result might be divergence. 
Note 3
After the points beyond 43 was added, the argument tilted as a full drawn attack on subjective probabilistic techniques and its remedies.

On Ethics and teleos
We had seen that organisms contest to make their case and prove that they are the worthy inheritors of a niche. There does not seem to be anything wrong here and it seems natural. The question is whether humans present a special case. On an observation of the universe, one sees that it is mystical. That is to say, we might not be able make any decision on it, except provisional and local ones. The universe seems to be ordered like a beautiful carpet, but with frayed edges. From another frame, it looks like it is just a mess of threads which had by some symmetry come together at particular points to look like patterns. But one thing we can be completely sure is that it is not completely random. There is an order to it, hence knowledge is not futile. This property of the universe is what causes us to search. It is alluringly ordered by frustratingly random. The elements are well ordered but the molecular structures seems to have properties of randomness. The solar system is well orderd but stars seem to be scattered. Thus, the intent and the nature of the maker is above all, mystical. We do not and possible could not know what he considers of the universe, yet we crave to know it. Therefore, the human nature attempts to contiuously and asymptotically analyze and test the universe. 
In evolution, we see that the organisms compete to assert their truth, by putting intelligence behind it. Humans on the other hand, do not consider an evolutionary preservation as good. They would rather want to seek orderliness and symmetry that is singular and apriori. Thus, in dedicating one to safeguarding a town, what one is doing is that he sways to the tune of an apriori notion of truth on what must be done, rather than inferring it from the circumstances. Therefore, paradoxically a truth is something what you already know to be true. A person hence seems to be teleologically bent on doing something other than evolving. There are so many instances in which humans sabotage evolution by what is an expression of good character. These seems like the hive mind altruism, but it is neither that. In only certain circumstances that a horizontal distribution of resources considered just as compared to a temporal hedge by hoarding resources. It is indeterminate in general as to what is right. But humans crave for general solutions to problems. That is why they don military uniforms and serve the country, become activists and attempt to douse foreset fires. They stand up to injustice not because it is beneficial to them in the circumstances or for the ‘greater common good’ of their family or city or even the country, but it seems to them that it is the right thing to do. What this right thing is however could not be formulated. Morals we might call them arise apriori in every individual to every particular circumstance but escape formulation in general. Paradoxically, people accept this. Thus, they seem to be content with their teleos as the limiting case of evolution, of either to defend against evolution or go bust. Thus, evil we might say is materialized in the pursuit of evolutionary advantage. Eugenics might be the worst evil in this case and there have been tyrants who organized and executed eugenics. 
The good person sees this to be wrong, because for them humans are not concerned with evolutionary development, humans neither argue that everything is meaningless and hence no struggle is required. All that they argue for is that what is their lot, what is their teleos and what they stand for should forever remain mysterious. It should not be capable of being formalized and sanctioned by authority. It is from the nature of morality that liberty arises and from the nature of liberty and its ability to find common ground justice arises. But the only way to stand up against particular formulation is to prove against it materially. Thus, it is always a struggle to upkeep goodness. The evil would come up with new plans, proponents of evolution, whom we treat in good faith to be ignorant or even critiques of the mysticism of human nature, much like the mysticism of what God is up to. This evil would then need to be settled, by a struggle. This struggle is not violent, it involves conflict only in so much as a fair competition is agreed upon and collateral damage avoided. Thus, all war effort on the side of the good involve forewarnings and a due process, even if it is to end up violent. Goodness means not desiring a finality to ones ideas, it is actually defending against formalization of final ideas. Hence, in standing for the good, one ought to struggle against the nature of evolution and its agents to take over as well as nihilism to attempt to destroy the metaphysics of there being good and bad. The nihilistic metaphysics also by defining something attempts to frame humans not as proponents of evolutionary struggle, but as passive bystanders. This is also a universal reference, which is untenable. Hence, ethics would need the formalization of non knowability of what humans are and it would formalize the knowability of something that attempts to destroy the notion. If this itself is to be guarded against developing into a dogma, those who stand for goodness should be unassuming like the village blacksmith. Their inner call to stand up in a situaiton is to be mystical as well. Their stance is often critical and reactive. They would come together in a project to defend and perhaps restore justice and disperse. They would suffer from having to disband and come together and in going through the notion of fairness in every move, but as one can see, they still triumph. Humans still are organized not around evolution, but around good things. This could not happen unless providence means this.



 








Now, How to solve it
We had made so far a detailed discussion on what is the essence of problem solving. But all that is written in the domain of philosophy is eventually subjective. It is just one of the ways of summarizing th world. It might be rationally justified in the sense that it is as good as any other philosophy. Thus, it is difficult to argue on the objective truism of dualism in the face of monism. These are just subjective reflections of individual philosophers, which over time might accumulate following and hence grow to become intersubjective. The material dimension to philosophical treatises is usually policy, rather than mathematics or science. Policy is intersubjective and a product of inspiration from meditating over plural philosophies. Some had ruled the world for centuries like say Taoism or confuciunism or even buddhist cosmological philosophy. But projecting philosophical findings on to the material dimension is difficult. That is to say one cannot see straightforward how philosophy can do ‘work’.
In the material dimension all that happens is discovery. We might say something to be an invention, but it is rather a discovery in a narrow way. Say you invent a sealing material that is better than rubber, it is still a discovery of the natural property of the material X with regard to, say thermal stability. Any exploration that is to create objective knowledge ought to deal with what is natural. What is natural and what is objective is synanomous, because what is natural preceeds the composition of a certain philosophy and interpretation thereof. There might be order in nature and the exposition thereof becomes the realm of problem solving. This orderliness might be something say like the thermal stability of certain material being a characteristic of the material or a dynamic equilibrium of a chemical reaction, which could be a natural property of the chemical system. Therefore we might say discoveries are ways in which we explain phenomena in terms of controllable components. Control is a technical problem and it could be achieved by multiple procedural means. The orderliness of material, or a system, if drilled sufficiently deep lend insights into the fundamental properties of geometric forms and their relations. If a system could be reduced to its abstract mathematical form, then it could be controlled in a deterministic way. There might be too many variables in general that deter such formalization of the system. That is to say we can discover systems of particles which would by virtue of their geometries produce certain effects. This explains behaviour or function from a structural point of view, rather than a dynamic one. We postulate whether we can deal with structures mathematically, even partially observable complex surfaces can be mapped in discrete mathematical objects like graphs. Using the static representation of the graph. We compensate for incomplete observation by defining discrete nodes which is defined in itself and by relationship to other nodes. 
With regard to the dynamics of systems, we attempt throught mathematics to define that these dynamics are in fact defined apriori. That is to say, we would have central tendencies and shortest paths as being essentially mathematical givens, to which the system would converge. This formalizaiton of the system allows us to theorize without experimentation and experimentations could only confirm them. Thus, the instrumentality of materialism to generate ideas is made secondary by a mathematical approach. Therefore there is no bridge that can cross between natural sciences and mathematics. Sciences deal with phenomena that arise by accident, such as a study of platypusus, but mathematics deny the pure chance element or history as such in the form of things. Therefore we might surmise that they have polar concerns and synthesize to create usefule kowledge. The branch of mathematics that deals with such discrete historical happenings, one in which dt is not a smooth and continuous element, but rather a discrete, random nature to time, is the field of discrete mathematics and aptly it is called applied mathematics because natural sciences could be augmented with its methods. There are very few pure mathematical cases in sciences, such as electromagnetism. Otherwise, one needs to deal with events and point observations. Mathematics eschews history, because it is perfectly symmetric approach to problem, numbers themsleves have no mechanical symmetry, the symbols merely refer to their pure forms.
Thus, we might say, that computation deals with discerete mathematical element of discrete random events, where time is not smooth, but a kinky and irregular curve, which encodes no particular signal. This might be reflected in the event of a switch, where a button press, closes or opens a circuit. If we take up this discrete event of button presses,a lot of natural philosophical happenings could be encoded as sequences of button pushes, where each button push (or a non push) is a matter of pure choice of the natural environment (be it human as well). Thus, we consider a computational system as a mathematical system that is dynamic in an open sense. It has very little by way of formalism, perhaps as a circuit. From thence, the events occuring at random (mediated by the observability of the system by the environment) allows the circuit to assume global states. These global states are not interesting, but its statistical state might be interesting in some systems, such as in data analytics. In some systems local truths are suffice, as in workflow systems. In discrete mathematics, we can speak of ‘states’ of the system while we speak of pure functions in continuous mathematics that could be evaluated at arbitrary points. Evaluation normally means differentiation in analysis. The question of control arises where the system is capable of being theorized upon. That is to say, if on a statistical basis, a theory could be developed on the circuit, followed by a suitable action, then the system is said to be controllable. The entire designation of a ‘system’ applies to a dynamic entity which is closed enough to be capable of obsevation in a static frame of reference.(This is something which we have hinted in our earlier treatise on scientific method being capable of encompassing dynamic entities). Thus the frame of reference becomes important in the examination of dynamic systems. 
We now draw to a special case of learning machines. If we look at systems which are represented in a circuit, say cars traversing a tricky mountian valley, where a lot of routes exist and they heuristically pick up them, we might see there exists a potential for automating the signaling switches to the routes to optimize traffic. We might start by saying that no circuit could modify itself. A network is something of a circuit, which in mathematics is a graph with weighted nodes. A network could only modify its weight if it is capable of being observed in a static frame by a controller, which counts (and hence has a central memory) of cars passing through sensors placed on the roads so as to modify weights. It might also count closely lying routes as statistical distributions of a mean route and filter noise(here, temporarily we might consider statistics as being inherent uncertainty to things such as routes, and not just observational ).That way it could accurately ‘represent’ the underlying phenomena (such as peak time dynamics), allowing an observational frame to make reallocation decisions of resources to specific routes to synchronize with the underlying dynamics. It essentially streamlines the underlying dynamics actively, rather than only representing it. Thus by noise filteration, we are not only doing a representational filter but also a filteration by making individual actors in the system aware of the state of the system (atleast locally). Hence a central state of the system (such as an objective function, say in this case, the emission levels in the protected ecosystem that the route traverses) becomes the object of control. The awareness of local states by the subjects of control might dispose the system to chaotic instabilities or an emergent equilibrium, rather than the possibility of control. This might need a rigourous proof. Likewise, we might model a perceptron as a discrete statistically activated switch, which could update ‘relative’ weights. That is to say, the notion of central objective is absent in a neural net for most parts. The relative weights thus are fixed by incoming stimulus of a continuous nature. It is interpreted discretely by statistics and the perceptron switches the metamodel of the network to add weights. The model in runtime is entirely decided by the weights. Thus, the network represents a function that maps a given continous input to a discrete classification codomain. 
In the technical point of the neural net, the incoming signals seems to have an intrinsic possibility for classification, but the metrics in a normal sense do not sufficiently delineate systems. One might for instance want to distinguish between tigers and lions. A series of vectors might approximate individual instances. These vectors might be components of a central vector ( possibly a sum in a metric space) and two distinct vectors as lion and tiger exist. That is to say, even without a prior goal that is set and an objective to control (unsupervised learning), a circuit could delineate between distinct objects mainly because of there being a distinct historical proximity. The variance among tigers would generally be less than variance between lions and tigers, because of their evolutionary history. But examples, even when randomly drawn in both cases, might incorporate spatial elements as scale and lighting and the states of the system itself like its posture and environment. Say lions are mostly photographed in plains while tigers in jungles. But it is a valid classifier to expect a tiger in a forest rather than a lion. Hence, unsupervised algorithms could pick up valid clusters by mixing historical seperation (a subset of natural seperation) with the randomness and exhaustiveness (as an element to make sure pointwise correlation between the divided subjects exist, say a sitting tiger, a sitting lion, a drinking tiger, a drinking lion and so on). It is hence, a representational filter, which does not interact with the system, because it works on examples. Thus, unsupervised algorithms can serve as representational filters. This can then be used as inputs, like visual inputs to control systems.
The supervised function which still involves a central observation and feedback to modify the circuit (either through a manual observation, versioning loop, or by a metamodel weight update), there is a notion of goal that is apriori and thus an effective reduction of the vectors consumed, say to distinguish between a valid letter and noise, one needs to provide examples in an ordered fashion, first of letters and then of noise. Usually only positive examples are provided. This allows the system to derive the central statistical structure to the observation and thus structure its knowledge in a way that it could decide on new items that are derived from the same statistical root. The line between supervised and unsupervised learning is rather thin, in that in the former the examples are presented to reverse engineer the statistical origniator function by ordered examples and in the latter, the same end is attempted by a mix of examples. The core principle is that a circuit is capable of modifying its own metamodel not just because a central metric is met or not met (as is usually for manual versioning of systems), but because there is an intrinsic highdimensional (say hyperplane seperators) exist between observations of (representations of) the system. Hence, the system might be tuned to select between alternates (or discern between alternates) not based on a linear metric threshold, but based on hyperplane seperators. 
Thus, we might say that all network techniques are knowledge representation problems. They represent the dynamic system from specimens obtained as being separate or ordered (in continuous cases like regression curves) due to intrinsic properties. This representation is goal agnostic. The selection of cat faces has no goal to it. This representation of knowledge, if in open loop with the system, as a way to capture the diurnal movement of traffic being hooked to a toll variation algorithm, we would be dealing with a control problem, that is not different from having a linear metric (like static peak rates). The smoothed representation of the system, stops being representative, but actually smoothes the system. The actual smoothing of the system, allows the system to develop dynamic oscillations, rather than controllable flow gateways (say you extend the peak hour slot by an hour to further thin out traffic on a particular route). These oscillations viewed from a static frame of reference are problematic. We should be able to model the dynamics mathematically and use it as a control signal input, if one needs complex controls. Thus, if control systems discern not on numbers but on hyperplanes and the whole system develops chaotic oscillations as a result, then the control system should have the metalogic to converge it to linear behaviour. 
Thus, we attempt to describe here the ability to frame as a system the dynamics it would develop as being capable of being dynamically controlled, not simply by constraining it, but by providing control signals. This allows the system be capable of attaining equilibrium, which could then be used to represent ‘pure’ knowledge on the subject. Say, if a system of dynamic pricing allows for counter control points to emerge (like ticket hoarding cartels), then the dynamic control system should be capable of further using the signal to dissipate the chaos and to result in an undisturbed smooth state of the system, to allow for a pure knowledge representation of the system. Thus, pure knowledge about the system, as composed of say rich and poor commuters could be made out by such knowledge representation systems and hence help theorizing on a given axiomatic domain.
Part 2
We have attempted to work with the notion of having a higher order control over the systems in order to eliminate reflexivity and get a pure representation of the system. But it is still questionable if this representation is knowledge. Lets work on this part now. We see that while dealing with discrete events, we are trying to get the recursive data generator represented through statistical methods. Say we take a toy example of a certain phenomenon of boughs breaking under the weight of snowfall. This is a rare happening and is a discrete event. Even as we talk of continuous flux in nature, it is also true that that nature has discrete thresholds in which interesting events occur. For instance in phase change, one can see negation due to mutual exclusion, that is a material could either be a liquid or a solid. In this case of the snowfall, we might hence see that there are tree like recursive pattern to the event generator , such as two blocks of precipitation events, which are mutually exclusive. Say, continuous rain with wind or snowfall without wind can cause the same phenomenon and by nature they are mutually exclusive. Thus, nature is infact compatible with logical analysis. The problem is that discrete analysis rarely reveals itself. Different parts of the programs might be reached in different frequencies, making them almost purely random and hidden. Particularly those in the nature of avalanche might lurk in a rarely reached block and we would never be able to model it from the data. 
Now, lets say we are able to causally correlate latitude with the frequency of the bough break event (only due to snowfall). This is nearly causal to a degree, because we associate it with the discrete state of freeze point of water being crossed leading to a solid precipitation. But the same phenomenon might be exhibited in high altitudes then we might associate altitude and latitude with the event. This association is however compounding or at least additive. Therefore, we might have a tricky causal explanation at this point. Thereafter if we add the other factors, say precipitation occurs much rarer in the vast inland areas of continents compared to the coastlines. There is an influence of the ocean in tempeature moderation leading to lesser snowfall in the coasts and there are geological formations that also heavily influence where the snow falls. Now, if questioned in a causal fashion, why the bough breaks, we hardly have a simple explanation. We can talk of the immediate mechanics of the break, but with respect to why such events occur in a certain area in higher relative frequency is hardly explainable in causal terms. What we would need instead is a ‘map’. This map might be a heat map of pixels representing coordiantes to arbitrary precision and say coloured from yellow to deep green to signify the hotspots of bough break events. This map is really a function in a discrete sense. It actually maps the pixel as a discrete evaluation point to a risk factor that is an ordered set. But the function itself is not analyzable. It is infact a ‘map’, a knowledge representation. 
Now, if we look at the role of statistics in this, we might say that the bough break events is correlated with latitude or temperature or one such linear factor. This is the norm in frequentist analysis. But we should note that if we take a large set of  n dimensions, the event becomes more and more random and the model is hardly informative. It is the selection of the quantile, which is a subjective exercise that allows us to see the frequency plots of the events and thus get a notion of central tendency and additive randomness of the system. Hence, the subjective prior seems to return here as well, only that it is a random prior. We might say that frequentist analyis is a subtype of the Bayesian, only that the Bayesian works on a prior in the nature of a distribution, rather than a quantile. Thus, in Bayesian analysis we might actually start with a distribution or a map of altitude and latitude distributions and then work on how the phenomenon meets the expectations to be able to result in posteriors in which we could improve our beliefs on the system. This belief is however loaded in that it does not arise from a knowledge representation, even as a discrete function, but as a given and it is said that it is what violates the principle of ignoramus in science. We might hence employ the bayesian method to analyze priors that are arrived by a formal process and it might still be rigourous. This process might be empirical even (which would put the subjectivity and bias to some degree of control). Thus, we could have generated the map of bough breaks by empirical count of broken boughs. But as we can see it is only a toy example. But even in that we see complexity might be further added, such as for instance weather systems that temproarily influence the distribution of the map. That is the function changes in time in a manner that is non analytical. Likewise, nature adapts itself against bough breaks, because nature conserves energy and creates order, thus we may see in higher risk areas, the trees are mostly conifers whose boughs done weight down with snow because of their unique inverted shape. Thus, we might see that beyond a specific threshold, despite steady increase in precipitation and drop in temperature, the bough break events reduce in frequency. Likewise, in densely populated areas, people generally replace conifers with more productive trees, which might again increase the bough break events irrespective of minor variations in temperature and other factors. But people do not settle down in highly dynamic areas and the local part of the system with high density, naturally also tends to moderate against a sharp increase in frequency. Thus, wherever we see the system tends to regress, but in a discrete way. There are islands of forests where these events are common and become a part of the life there. Perhaps it is the only way to survive in the niche or perhaps farmers tend to such trees depsite their survival disadvantage. Therefore, we might see that these factors combine in myraid and non linear ways to produce mappings and these mappings change over time, local stability might be there, but still the function would keep changing, like a changing heatmap and thus we might talk of the function to be having a differential and the function itself being the integral (we might have to use the integral in a Lebesquean sense).
Now, lets consider that the system at each point, or pixel is formalizable in vector terms. Each point has a definite vector pointing in the seemingly orthogonal dimension of interest, say density of population, say altitude etc. The combined vector (as in a metric space) produces the degree vector of the heat map to indicate the frequency of tree boughs in a certain pixel. But there are strong covariances between dimensions, which might be difficult of formal treatment. In such case, adding more dimensions, such as ‘isConifer’ might reduce the burden, but not wholly. The system becomes more resistant to linearization and we  might have to actually formalize the discrete covariance relation between dimenisons, or that we should acknowledge the non orthogonality of the dimensions. This would mean that the mapping would have to involve non linearity. 
We had already visited this notion of non linearity and its ability of being encoded into recursive algorithm, merely from data in our work on Connectionism. We had made several plots in paralell axes and demonstrated that even where the axes are properly normalized, we would still be having specific interesting ‘pathways’ connecting these paralell axes. We had infact done some experimental work with discrete functions to emulate recursive functions, by using machine learning algorithms such as svm and naïve bayes on cloud based ML systems, such as the one from google. Therefore, we might say that it is not an impossibility that mapping functions could not emulate recursive functions. There is infact this very clear theoretical limitation of their ability to do so, due to the mapping functions being Markovian. That is they have no memory of the past states, due to which nested evaluations are not possible and hence recursion is not possible. This we had seen at the points where lines criss cross as X marks in an axis. There is no way to determine specific paths over multiple axes, where such X s come up. We have a network that could be stochastically traced. But this network and the map, we argue are not inferior in knowledge content to the recursive anlaytical function.
Thus, we might say, if we could produce a heat map that is a point wise mapped function, which evolves over time, or rather oscillates over time, there are two important things we need to consider. Firstly, where an event occurs, it has a tendency to repeat at that place, either because of niche adjustment or because of the specific climatological zoning. The second one is that there is usually smooth gradient from a place where the event hardly occurs to where it occurs the most. That is to say, the deep green patch is usually found nested within a light green one which itself is nested within a yellow one. These two principles of stability and derandomization allows us to approach the problem towards a solution. 
Say, we want to find a linear pathway through an environment, which would minimize a bough breaking and falling on our head as a risk. This could be a smooth line,because it might not always be possible for control signals to exert kinky direction change due to inertia. There might also be directional inertial latency to the system and discrete breakpoints where the control signal is responded to in an exaggerated manner. The curve that results from the navigation, though smooth might not be anlaytical. This linear truth might be the outcome of the exercise. If we look at the toy example as representing a biological system where the risk of progression of the disease to critical phase is represented in a map between some known parameters of the system, then we might realize its usefulness. The solution becomes a pathway problem, where we attempt to find a line that could allow the sending in of control signals to modify the trajectory of the patient, by modifying his current state, that he does not enter the region of high risk. This, given the stability and smoothness of risk emergence could be achieved by stochastic gradient descent. Thus, an objective element (which has its own intrinsic properties of inertia and sensitivity apart from the terrain itself) could be navigated through the risky terrain based on the map. 
This notion of the objective element and the general case of specific states (coordinates or pixels in this map) is important form a solution point. The state could be modified to a system, but the general risk of specific compounded points could not be modified. What we can do is to alter the future state based on adjusting propensity of the system in order to achieve a safe traversal of the system by the object. We might encounter an unlikely event of bough break, but we can still ignore it, because we have the map, whereas we can plan to steer smoothly against an upcoming major hurdle. Say, we do not want the patients blood pressure to drop beyond a threshold since that might be representing a state of the system in terms of the pixel darkness. But it is still difficult to estimate which parameters compound the state. It would still require a skilled navigation based on feedback. It would still be formalized because, we have access to random experimentation. That is to say, gradient descent works well in low dimensions like shortest route algorithms. But where the dimensions increase, we would still need to go about stochastically. What we believe is an intrinsic orderliness, where once we hit a stable state, we can progressively restrict the search space to the neighbourhood and ride some kind of a gulf stream through the system. Thus, it could be in principle automated. Thus, it looks like the triumph of mathematics over the sciences and we can see that some of the toughest problems could be solved by automated, yet non analytical functions. But we would still probe for a critical view.
Part 3
Now the critical part we promised. We have earlier stated that the navigation problem might be the most general problem which could represent a great deal of particular problems of control. We would examine this critically now. If we are to assume this scenario, then we might say that the control signals could be derived from the environment. Let us say we have a runaway engine, a car, that has an inbuilt navigation system, about which we know nothing of. Presented in such a case, it is infact possible, with a given representational map or function to navigate the car. But the car might have say 20 levers and knobs and we might operate it at random to have to actually ‘learn’ what to do when. The cars internal navigation program might have it head to higher altitudes if it overheats and needs to dissipate heat or it might loose speed, leading to greater exposure between sheds. These are all factors which are unknown to us. We have levers to open the engine hood to dissipate heat, but risk a higher drag or a possible stall. So far we have been using quasi causal terms. Without such terms, we see the problem as being not easily controllable. We see that having the map and the state of say 200 cars in the terrain, we can still make policy decisions. We might allocate resources (say protective covers against bough breaks) to certain cars based on their positioning in the map. Thus statistical inputs to knowledge can only help make statistical decisions. Needless to say, if the protectiver covers are not scarce, the problem would not be there in the first place. Thus, all problems become problems of policy and individual control is not addressed in this method. 
Let us revisit the formalization of the problem. Let there be two systems interacting. The map is the environment with states that are pixels. The other system is the car with its own mapping function which moves it in certain directions depending on a variety of factors, which might be gathered from its internal environmnet (temperature, fuel levels etc). We might see that the enumeration of states of the different objects in the map (as position in pixels) is trivial and it could become the basis of a statistical control by resource spread. But when the focus is on controlling the individual trajectory, we can rely to an extent on learning. Learning could work if we can make local searches and use gradient desent. It would in fact be Bayesian and we would be constantly updating our beliefs (we had erstwhile remarked that learning is by definition updating beliefs). Thus, by using learning techniques, we might be able to produce a non analytical mapping function between the systems internal parameters, current state and its future state. Remember in a Markovian process, the transition between states is purely stochastic. In an analytically tractable system, the mapping between present and future states is trivial, because the function is continuous. In a discrete system while the domain and codomain are mapped well, there is no mapping between successive states other than by a stochastic means. Therefore, it is difficult to make predictive model of two system interactions in this system, which is however the primary objective of control.
We might say however that on locating a correlation between say an engine speed reduction below a critical threshold the system becomes more controllable and this information obtained by empirical means helps reduce the risks allows the system to be stabilized in the high dimensional problem terrain. We ought to recollect that we know nothing causal of the vector components that define the density of the color in a pixel nor their covariance. Thus, the notions of altitude and latitude to not make sense. They might be abstract symbols as well. Likewise the system of interest might have abstract symbols that represent parameters which are represented by certain patterns. If the map formation is feasible by stochastic method we might as well produce a map of the car states as defined by its parameters. But the mapping between the car states and the environment states is again a problem of concern. Lets we get to map to an interesting parameter on the car – its temperature like we got to map the bough fall frequency. Now it is important to correlate these two functions and create a new function which could map the temperature output of the car to the direction vector on the map. If we had causal knowledge we might reason that a rised temperature would cause the car to seek higher altitudes rather than higher latitudes or places that are wet to be able to draw in coolant (this is a strange automated car with very high speeds). But looking at the way where the car dramatically alters its trajectory in the map (a mapping between past and future state) based on its temperature being in a discrete zone (say very high) would help without causal knowledge even, to produce a mapping between these functions, that is a ‘functional’. This functional could help in producing a linear map to a linear measure, say – the stability of the car (or the safety of the patient). 
Thus, the introduction of Lagrangian dynamics takes the explanatory power of mathematics to a degree higher and thus, again it seems that it could better causal knowledge. In this schematics the neural net might simply be a discrete mapping function which emulates a recursive function. A lot of current research is directed towards perfecting the emulation, but the fundamental problem of mapping between maps arrived by the emulation is still of great value. We may have to control at the point of interaction between the two systems, the object and the environment. Classically this had always needed causal knowledge. But, if we should look at ways in which a dynamic mapping between the two functions could be produced over an arbitrary set of parameters (we took the heat dissipation being the intent as a quasi causal claim), by gradient search, then we might produce the two system composite, as a stochastic system. Knowing that the car would self stabilize, we could control at critical thresholds (knowing as well as the smoothness of presentation of hurdles in the environment, which is a form of self stabilization). This control could help further stabilize the two system composite. Thus, we say local truths could be derived from the system and these truths are not just state truths but also dynamic truths. 
In deriving local dynamic truths, such as the future state of the system and its correlation to another dynamic function (say an increasing temperature or factor x), might allow a control component to be engineered into the system, which could allow dissipation by control inputs to avoid the undesirable future state. Thus, if the temperature function could be controlled keeping it below a critical threshod by a servo mechanism, the system would be optimized to never having to seek higher altitudes and thus respond well to directional control inputs. For this we need local dynamic truths, as the differential equation along the state dimension and its being expressed in linear terms to another equation on the LHS which is a differential equaiton of the temperature vector. This allows in being able to add an additional term to the side where control is feasible, as with temperature as  a servo mechanism. Now, this servo mechanism might be the second order differential term and might have a negative coefficient. Now, putting this in place might allow the system to be controlled, based on linear mappings between two factors. But where the mapping is complex, we may have to rely on partial differential equations which are solved in a manner more lke simulations. But we have seen that it is possible to arrive at linear discrete maps on a multivariate system. We might have the linear sides of the two functions to be defined as a different function, which we had simplified to be an Ordinary differential equation.  Where the critical linear component could not be selected, we would still have to map from among m components in the environment and n components in the object, thereby generating a composite map that maps to q – the stability coefficient of the system as a whole. This map might estimate the stability of the composite state of the system, but control could only be attained by linear mappings as we said before.
Thus, we might again say that non linear control is nothing but policy and resource spread. A linear control would need a mapping between arbitrarily selected ‘rubber meets the road’ points between the object and the environment. It can be attained procedurally but if it could be obtained functionally is the question. We kow procedures arise from priors and empirical observation and hence scientific rather than mathematical. Arguing for a functional solution is to argue that a policy control, one of global stability is good enough for all local stability needs. Functional calculus argues that global stability is the best we can do to local stability. It does not permit local hacks and this is where we are stuck.
Lets take a case where the protective covers could be dynamically reallocated at zero cost. Then, knowing the current state of the system as a whole would allow reallocation of the protective cover leading to the neutralization of the bough break threat. Then the dynamic reallocation becomes effectively a control signal, that which is however global. But one sees that at the time of most need, one gets allocated a protective cover by the governor system and the system picks it back once the person is over a threshold. Now, this can be a recipe to explain how controlling global stability can benefit local stability. 
On the other hand,  like in medicine, where there is no given protective cover and we do not know the control signals, the problem is different. It is not a question of paucity of resoruces (medicines are administered in milligrams) but rather a problem of knowledge. It is strictly a local problem. There is infact a global terrain map that could point the risk at a local point of interest (say the current state of an object). But we ought to come out of the navigation analogy here. The current state of the object is again inferred only  through the state of the object under observation. If there is a steady drop in temperature, we might say that it is headed to a high altitude point and from having a knowledge of ‘you are here’ from the map, one can make local searches to predict next states, again reading from the observable parameters of the system. The analogy of the car, allowed for physical observation of the car on the terrain due to the spatial prinicple of matter occupying space exclusively, which is no longer applicable this abstract scenario. Thus, determing a local state from observable parameters and its neighborhood allows one to modify these paramters to control it locally. The concept of the map itself is a granularization of the stochasticity of the Markovian process. Hence, state swtiches between states s1 and s7 become highly probable in local temporo spatial contexts and this information is dissolved in the global transition matrix. Having local knowledge of possible future states, again inferred through the parameters of the object, we might then understand the dynamics of the observable state say a, its time difference function and use it to navigate from gradients, having the map in hand. Thus, once more local control becomes possible. Thus, knowing the local gradient of the environment as well as the observable parameter that is correlated to its positioning helps invoke a suitable control strategy. We see that the control itself might be circular, in that the observable is dampened by the control input.
That is to say, having a map of the global terrain, allows by examining time variance of an observable parameter to place the possible position in the map and thereafter control based on the dynamics of the control inputs (may be the control lever could break off if we pushed hard), we might devise the local stability strategy. Thus, local control solutions might be feasible without causal knowledge, by estimating the current state (by virtue of the map) and use it with the map as the goal (notice the circularity). Such systems empirically exist, as living systems. If we think in terms of such stability notions, that is to say if we can locate the system in a map and accordingly manipulate its parameters so that it could maintain stability it would need a marginal lookahead of the situation. Thus, if the system enters a patch, which by the heterogenity of the stochastic terrain seems to continue for a long duration, a move to ‘defensive’ would help greatly. But on every perturbation a defensive stance is taken, it would no longer result in convergent control. Also knowing how to perturb the object defensively and offensively (like a kite) is important, but that is not an insurmountable problem. The problem is locate the object on the map by virtue of the behaviour of its innate control system. Thus, unless the system has an element of innate control, it does not qualify as a system by itself. If the system exhibits behaviour of offense against a certain adversity and it is seen futile from a higher degree vision, it would be good to control the control system to enhance local stability. 
Thus, control problems are actually mostly metacontrol problems. A more direct engineering application is very much in aerospace guidance systems. It does precisely to ensure local stability while navigating a dynamic terrain, the map of which is unknown but could only be dynamically created. If a certain pattern is observed (as with a turbulent cyclical formation of wind), by sensing the wind pattern as a dynamic map that comes to hand,it would then try strongly to move away from its eye and attempt to hit the tangent. This kind of control would then need to increase or drop power, if we simplify the notion of control here depending on whether the turbulence is shortterm or long term, which would need decoding the signals that precurse it. Thus, the whole idea of having a map is to know the edge of the signal and thus steer clear of it. In aerospace, the projectile might objectively define its position by means other than wind velocity, say by satellite tracking. But in medicine, the control is a loop, in that the position is determined by the innate control of the object kicking in. But in aerospace, knowledge of the position is not so useful for control purposes since the turbulence pattern is highly unpredictable. It would still need to position itself in a virtual map of a turbulent fluid space, by an observation of the change in its velocity in reaction to the turbulent resistance and estimating the position based on a map of multiple possible styles of turbulences. These, say 6 turbulence types are not spatially laid out and there might be discrete jumps among them. Therefore, each position would first sense the turbulence type to invoke the correct map (or function) of the turbulence and based on its pattern, arrive at an appropriate control strategy. This is not entirely farfetched and the notion of causality is not strictly required here.
Part 4
Now we take upon this kind of fine grained control from the question of reflexivity. We had infact been trying to develop a system that is analogous with the Lagrangian dynamics in a discrete field. Given that such a path maximization problem is capable of being devised from a ‘map’ like specification of the functions, we can then look at the sustainability of the solution. There is no doubt one would prefer to have local advantage, particularly in medicine, where one would like to come out of the crisis. But we have an important question in that we are supplementing an already functional control system in the biology. In such a case, by insulating it partially from the real world feedback, are we intruding on the development of the underlying control system is the question. The environment attempting to reprogram the biological control system is insulated by the control system. This might in the long run regress, that is to say global stability would return to its natural attractor state (from the Lyopunov concept). Hence the overall advantage of seeking local advantage is questionable. A complex augmented control would in medicine lead to some kind of a transhumanistic setup in the medical field. In the case of constant local control say in the falling boughs case, we might have it that the best routes are selected and the overall order is created. But this would also stop the feedback to modification of the design of the car to have to constantly seek heat sinks. Hence, any kind of solution replaces another potential solution. It is preferrable to have a solution that is reversible. This is where causality comes in.
Causality involves development of control systems where there is symmetry in time. That is to say, reversing the control replaces the system to its original state, for most parts. A recursive scaffolding to protect the system, such as using algorithms that could introduce regimen that is known to yield results statistically using randomized trials (ie have a linear relationship with the outcome) could be used in structured programs to move the system towards better results. This would mean mechanistic controls rather than continuous multivariate concurrent control. That is to say, it picks up each variable which is unambiguosly and linearly mapped to outcome and based on creative hypothesis generation produces a procedural control system, rather than a continuous control system. The advantage of such a system is that it could integrate well with the general causal environment of engineering, because of the common semantics of causality. 
This integration with the engineering at large could be greatly beneficial in the sense that if a certain proposition for control is made vide administering a drug that causes vascodilation, the same might be attempted by other means, such as for instance physiotherapeutic excercises. This will allow the endeavour of engineering to proceed in an integrated manner. If we use solutions that are customized for the domain, then it would mean augmenting the phenomenon incrementally to produce higher order phenomena. The system plus its control system would still be a system that is only phenomenological knowledge. It might be used in further systems level control integration, just as with complex multicomponent systems, like multicellular organisms and it would still work. Thus, a major argument for causality is the shared semantics with the rest of the engineering world. If everything were dynamic control systems, then they might integrate in a way which might be equivalent. This point is however speculative. 
If we are to formalize causality, we can see it to be a special case of the  implication relationship between discrete truth value holders in a system. This system might be an axiomatic theory. A theory might explain the world in a certain way. There might be other theories of the world say the chasm between chemical and physical explanations. Having a logical model of the world based on axioms is a good way to engineer control. But this approach seems to be insufficient where specific problems in the real world have too many dimensions. We might then work mathematically to produce intractable functions and non linear relationships. Non linearlity does not translate well to logical terms. Hence, it becomes stochastic model of the world. Statistical mechanics deal with causation in such a system. The knowledge it produces is however a pointwise map essentially for global stability. With respect to local stability, we might have a control function which is a point wise map as well. Thus, the system might be a servo mechanism, with a reservoir (just like the power brake systems) which could flow in and do work as a stabilizer to the system. The addition of servo mechanisms based on non analytical caliberations designed from data of past behaviour and feedback from current behaviour might be a good general machine.
This general machine of servo mechanism could solve a lot of problems, not just controlling anti aircraft fire or navigate through complex terrain or in air. It could allow a system to take the best route, may be a self driving car in automotive industry, it might be a medical control unit in an ICU setting, it might be a manufacturing control system etc. Thus, as we put to work more control systems, we might have a world where these systems can collaborate with one another towards local objectives. The result might be an emergent global stability. It is exactly the desirability of this development that we dispute in the philosophical realm. But in this part we see that the merit of causality lies only on its popularity. If extensively replaced by servo mechanisms, we might say, global stability might emerge. The information is encoded into discrete functions in every system (with an element of control) and these interact to produce dynamics which feedback to the individual strategies. The overall flow of resources in the system is in such a stable setting is held taut, resulting in a global stability, which is contained in the theories like game theory and emergent equilibrium. But such systems are also prone to chaotic undercurrents, much like how causal systems crash rarely. Hence, it seems that there is no fundamental argument in terms of expressive power to argue in favour of causality. 



Part 5-Conclusion
To summarize our discussion had been to establish that knowledge can be expressed as a pointwise function or a map that could relate vectors non linearly to produce what might be heatmap like functions. This auto detection of non linearity is a tricky concept. We had discussed it extensively, but apparantly, there is a lot of evidence that this is possible. In unsupervised classification tasks, we might see it is the covariance of certain factors (which in fact increase the weight of connections in the net) that make it a candidate for classification. Therefore, it might infact be possible that a heat map might be generated based on covariance of certain factors producing a distinct zone. This heat map also suggests that a given markov chain represented by a transition matrix, might have local heterogenity. If we are to isolate a markov chain into component markov chains by a classification task, then we  might be doing an expository work akin to say classifying a disease further. The isolation of heteogenous markov chains that underlie a common, perhaps syndromic condition helps the formulation of the control strategy. Thus, we might generalize the more spatial map we discussed in the example to the case of a whole being composed of distinct subgraphs. This ought to be possible in theory, just because in a given large system (may be the world as a whole) we are able to identify plural markov chains. 
Thus, if a map could classify several distinct markovian process within the apparantly hazy markovian process, then we imply two things, one that there being locally stable plural states in the system. These states might be time dynamic but boundary stable. That is they form distinct ‘storlylines’ or trajectories. We might attempt to bring in the works of Lyopunov (who disucssed the stability to time varying objects, or objects in motion) in his magnum opus. The second thing is that this local stability hints that a markovian process might in fact leave a trace and that would help in predicting the future state. That is to say, if pinpointing a sub markov chain is possible from looking at a past sequence of events emitted (either directly or through another markov process as with HMM), then one would be able to predict with greater accuracy the future event. This is a contradiction, Markov chains by definition don’t have this property of relying on sequence of past states and hence considered homogenous. Hence rather than talking of sub Markov chains, we ought talk about multigraphs. But again that is only semantics.
So we had said that the ability to identify multiple markov chains allow us to label a condition with greater accuracy along with its trajectory in order to invoke relevant control strategies. The ability to locate the state of the object as being within the local stability bounds in our method of two pass smoothing (or forward backward smoothing) would thus to some extent violate the Markovian process, but it is well established that positioning of the current state and its placement within a local part of a global state map is highly useful and practically achieved in many cases. 
The next point is with regard to the distinctness of the two systems, one the environment and the object of control. These have separate subgraph maps and there might be possible that there is an arbitrary mapping between the objects state enumeration and the environments state enumeration where the object of control has inherent inertial dynamics, even if no intelligent stickiness to trajectories. These two distinct maps might form a combinatorial ordered pair. But here to covariance could be autodetected, was a part of our past argument. Talking of states, it might as well be that the states co-occur, because they are not material states that exclusively occupy space. They might overlap and the resultant vectors (which form the colour depth in the heat map) could themselves become candidate vectors for further mapping. We might even speculate that might underlie the multilevel deep neural nets. The whole idea of back propogation could be backward pass smoothing. We might say that if these trajectories though they are locally stable in a given dimension space, might be traversed in paralell by the object in a different higher dimenison space. That is to say, to be more rigourous in the state overlap discussion, we would rather state that a state space could be recursively defined. Thus, there might be multiple state spaces with dynamically stable states (or trajectories) and these states might be points if one could analyze the current states in a higher level. Thus, the state in exercise and ambition statespaces might coouccur in specific covariant clusters, wherein we can classify them as a recursively higher order statespace and define more general dynamical states (say multiple trajectories on how people age). 
Now, it is important to understand the concept of system boundaries. We had earlier dismissed this and discussed that recursive state space derivation could be done even between systems. But this would be fallacious. In fact the relationship might be combinatorial locally. Hence, when two systems interact, there is no way to predict how they would behave. We can do any kind of prediction only at a statistical level, such that how two systems might normally interact, but we can say nothing locally. A system might instead of spreading statistics in space, might spread it in time and obtain heuristics or normal ways to interact with the environment. But the noise is high here. Hence system boundary is a conceptual threshold rather than a strict mathematical boundary. Even empirically, all organisms have only porous boundaries. The organism may hence learn from experience, which is not different from inferring from statistics. 
Now, the question is that, given that statistical definition of norms is possible and encoded into heuristics or policy, which are homologous, we inquire if this is sufficient for global equilibrium. That is to say, if each organism should apply normal behaviour that is optimal under the circumstances, are they playing by some global rule of conserving order. In our earlier example, we had seen that it is possible in policy to spread resources dynamically as a method control. Organisms do it locally as well by spreading resources temporally. We had in the example used a causal language of a ‘protective cover’. But it is in general a distribution of resources, so that it might be used by the object for any purpose, be it to take a more circuitous route or to replicate or repair itself on destruction. Thus, we might see that a distribution of resources to stabilize the shortest path reflects Lagrangian Hamiltonian dynamics. Therefore, we might at this point grant that a global emergent equilibrium is possible. 
To locally decide a question however, an object that relies on heuristics has only one option, the norm or to randomly flip from the norm. It does not have a static structure of the system. That is to say, if a norm dictates (either in individual conservatism or government mandate, both being homologous), a certain action in a certain circumstance (or a dynamic state mapping across system boundaries), a system can deviate from the norm only as an experiment, which may or maynot payoff. If feeds back to revise the norm as well as locally solving the problem (or making a failed attempt). Thus curiousity causes one to deviate from the norm and progress depends on that. It might as well be peer pressure rather than curiousity, say the normal route is already congested and taken that the organism is forced to exist on fringes and thus evolving into a new one. Thus curiousity and axiety of being trampled upon are closely related. But to revert to more abstract terms, we might say that local stability formalizes the norm plus the additive noise in the environment, afterall the hurdle could a new variant, or the imperfections of the observation media (say the view is distorted by haze). Thus, the solution for stability of the system is always selecting from locally distinct pathways so the additive noise term is minimized. That would give us the Euler-Lagrange equation in continuous mathematics. 
Hence, local decisions might be a minimization problem involving dynamics of partial differential equation of the function F, which maps the parameters to the problem – x being the environmental signal, the invariant, y being a random function which will map the environmental signal to an output. In fact y is a family of functions from which one could be selected. This selection of the function (semantically equivalent to varying the structure of the function- which is why this is the calculus of variations), happens based on trying each one out and picking that which minimizes a functional defined as an integral (or energy consumption over a trajectory). This selection is not an iterative process, but rather defined apirori by a differential equation namely the Euler-Lagrange equation. Therefore, mathematically, we say there is a structure (without the need for historical events, that actually makes science necessarily a discrete or non prejoratively, a fragmented subject), exists to the systems decision. If a system should decide different, it should be because of the discreteness of the field, the observational error or instrumental problem. 
This mathematical structure is static and apriori, there is nothing iterative about it. Likewise, we might define a frame of geometric proximity, where one thing necessarily causes another(conservation) and the another is not caused without the first (inertia). That is to say, a exists iff b exists. This kind of strong relationship of conservation and inertia is the framework of causality. Causality results in discrete local truths which eventually relate to a singular truth about the physical dyad of conservation and inertia. Thus causality is a local knowledge derived from higher order truths which might in fact be lagrangian dynamics. Science deals with what is local and proximate, such as snow sticking to a branch weighting it down and the wind either shaking the snow or breaking the bough by exerting torsion. It is thus axiomatic, where the axioms might derive from higher order truths recursively. Causality might hence be defined as a recursive collection of local axiomatic truths. Mathematics has precious little by way of recursion and uses abstract concepts directly. 
Thus, the local solution that relies on computation about global functions should ideally dispose a system to a convergent stasis. But the vibrancy arises, because of inherent uncertainties in the system that arises as chaos from time to time. Therefore, local computation based on global functions that underlie in causal analysis perhaps ten levels of axiomatic theory, can bypass the framing and maintenance of such theoretical layers. In fact in all the theory of automating classification, of identification of markovian subsystems and in local decision making, we see the same underlying thoery the law of least squares. That is to say, like things stick together, or we discern likeness by their sticking together in a high dimensional space. Thus, two people are equal over a range of state enumerations and a system is bounded because it varies within a range of statepoints (ie it is active coherently, or it could move on its own, in less rigourous terms). This is a very highlevel truth on the natural way the world is organized, cutting across disciplines and theories.
This leads us to think that we had in fact been using a recursive construction of truths as a way of division of labour so that scholars can work in their own disciplines. If the computation power is augmented electronically, hence it might appear that this recursive construction of truths could be avoided. Thus, we might say that recursive truth construction relies heavily on mechanical contact, diffusion or fluid dynamics to explain cause and effect spatially. This is conforting because it could be verified by unaided senses. However, if explanations should rest upon models developed recursively over compuational fields, we might not have such verification possibility. To explain, while we discussed system boundaries, we might see that the construction of a global picture relies on a recursive arrangement of systems as well. Some systems are assumed to be more stable than others and the more active ones would have to be explained in terms of stable stable systems, which is nothing but a recursive arrangement. Thus, eventually recursion might emerge (even in evolutionary biology, the structure is recursive as with multicellularity and eusociality). This recursion might be unverifiable to unaided senses. 
Hence, sensorial augmentation might become important for us to exist in such an environment. The reason why we verify things is because, we have presumptions about the world and control our actions based on it. By verification we confirm our presumptions. If such verifiation is to be mediated by noise filters and organizers other than what we embody (the eyes are noise filters and the brain is basically augmented network computing), then we might effectively be extending ourselves. The crisis of control is a loss of our embodiment and thus the system boundary. We might be bounded externally in a system that we can control because we trust our senses. This security is only provided if we could continue to rely on our senses or direct augmentation therefor (like telescopes) rather than computational instrumentation that mimics the brain. Thus, the crisis is philosophical in letting go of causality.
The question is also that why the brain devised a causal understanding of the world and if it did so emergently. It might have done emergently in order to optimize around our unaided senses. Hence, causality is nothing but a recursive structure (which would anyway emerge in any computational process) that is optimized for sensorial verification. A search for better ways of solving problems could happen, only that we could know that for sure that it is better than what we have. Hence, we might conclude causality is the best way to solve problems.
Note:
We might add here two points that might be interesting. In applying binary classification recursively over a virtual space, we might be able to divide up the space into recursively binary segments. By thereafter revisting the labelled clusters for division, we might see that they may infact be the identical. Thus if we have a rectangle divided up as a, b and a as d,e and b as f,g and thereafter run the classifier in a cross cutting way, we may see that e and g are the same and hence we might have a->d,e b->f,e. This is nothing but a recursive formula derived by purely iterative means. 
Secondly, In seeking solution to problems, it might often be that humans may simply seek an explanation therefor, rather than a solution. Such explanation might be causal (as why the tree fell the other day) or even intersubjective, where there are stories in culture that give reasons for unexplainable things. Thus, many problems are simply ‘itches’ which would need to be gratified and the concept of ‘closure’ is important for many problems, as much as solutions. This aspect of closure is tied to having domains of rationality and intersubjectivity (culture constructing reasons). If these domains do not exist distinctly, we might have to cope with a single computational domain that give reasons for undesirable outcomes as pure randomness. It is as good as an intersubjective explanation, but culture always constructs causal stories, leading all the way to the ultimate cause, the Maker. Without causality, the world is banal and less comforting and hence causality is useful.
But we might see that causal explanations might not be able to scale some complex problem domains and hence was initiated exercises in computing. This historical root of computing would need to be remembered. It ought to be fixed in a static frame of reference in order to lead us to hypotheses, rather than building entire knowledge systems. Thus, we might say that computing is relevant only in so far as it operates within a frame of knowledge. But things easily get out of control and if there exists a self contained interrupt to solve it is quesionable. The interrupt might arise from elsewhere from the metaphysical realm and that is why the position against computing becomes more of metaphysical one.
Note 2
The dynamic paradigm which is evolutionary in fact places the origin at a single point (a set of initial conditions which could be arbitrarily set as the orgin). The recursive paradigm is essentially dualistic or say pluralistic. This paradigm self referentially has to hence accommodate monism as well. Philosophically, the note on the support of causality and its superiority is entirely context bound to the present set of circumstances and thus is an act of restoring the balance.
Note 3
We see that local stability is the essential feature of a system with identity (in fact dualism is dependent on local stability) and in emphasizing on the sensorial ability to verify the world, we are in fact relying on feedback in a cybernetic sense. Hence, we might say that in order to maintain local stability through sensorial feedback is what is a formal way of saying that the ‘world makes sense’. Hence, to sustain local stability as a system, as a person, preserving ones identity is heavily dependent on sensorial (unaided) feedback. Hence we might name this paper as - A Mathematical theory of causality or A cybernetic theory of casuality and why we need causality for maintaining identities.

To edit
Say in one specific case, we are trying to reduce vision into controllable components, then we would perhaps have higher resolution vision enhancers or even we can ‘see’ problems that are defined beyond material space. Thus, if we should by looking at say two groups of trees be able to theorize on the characteristics of the two canopies as flourishing or withering depending on certain characteristic markers, we are in fact capable of interpreting cognitive input from vision in a rational way or use it as proofs for theories. Thus if this vision could somehow be reduced to mathematical components, then we might perhaps attain ways in which we can see through problems that are complex in the sense, say patients whose state is reflected in parameters that are higher level functions than spatial position and temporal variations. Say, the spatial observation of certain metrics, like glucose levels leads us to construct observable states which could lend cognitive input just like vision, then we might be able to use the same innate characteristic.
A notion of purpose
We have seen that from our recent research that the method of critical realism is what is very close to our philosophy of science. 
A possible critique of romanticism
If we look back to the situation where humanity stood at the start of civilization, it might not depict the noble savage, perhaps. We have been in the postmodern era, after inheriting the benefits of modern engineering, been popularly critical of the pressures of chemicals in human health, but a look at the state of nature might reveal that the parasite load in nematodes, annelids, protids, bacteria, fungi, mites and virus and viroids account for a greater burden on mankind than the simple chemicals we introduce into our biologies. These parasites might produce such complex cocktail of chemicals as to control anything from mood to nutritional absorption. Even the present heavy critique on the stress of modernity might pale in comparison to the mind alterning nature of many of the nature’s creations, such as evidenced in the recent study on T.Gondii on Chimps. Nematodes thrive in the liver, brain and muscles of infected hosts, often finding way through soles of the feet or through contaminated water and food. The benefits of urbanization in providing sanitation and such age old engineering like controlling fire in order to cook food (which is a process of high importance to brain development due to its parasite cleansing nature) helped define the human position. The regression to naturalism might be difficult of conception and is often biased by not providing enough credit to the emancipatory nature of modern services. In fact in regions of less sanitation, it might be possible that social stress is much higher (such as a study implicating tapeworms to seizures in some endemic populations). Villages inspite of their idyllic beauty, might have had more depressed people due to the sanitation problems than urban counterparts. 
Therefore, one need to look at modern revolution with more charity and not dismiss its benefits from within itself. Even Thoreau was not supportive of pure naturalism. Lets look at the evolutionary framing of modern thought. In the development of species, one might see that omnivores generally are more intelligent than specially adapted species, because they spread their evolutionary pressure wide. Among omnivores, those that value the ability to use ideas instead of physical tests seem more successful. That is to say, in chimps, the need for high certainty to rise progeny to adulthood requires a high degree of determination of the situation into which a progeny might be configured and rised. That is to say, higher animals rely more on nurture and idealist, non material decisions or determination than spawning. The notion of idealist decision making relies on the ability to decide on problems globally. One might look at a weight hanging by a rope and the mass should fall if and only if the rope should snap. That is to say, if the rope snaps the mass would definitely fall and the snapping of the rope itself is sufficient for the mass to fall. This kind of two way relationship allows for deductive reasoning (remember sherlock holmes saying that if we exclude what is impossible that which is left should be the truth, however implausible it may seem). Thus, the method of reasoning around two way connectedness or causality, is to arise in the removal of other confounders. But it is not always possible. A reasoning methodology which uses one way relationship, such as when leafs turn brown when sunlight is stopped also works. The leaves turn brown when sunlight is stopped but it is not sufficient to keep the leaves green. The plant has teleological clock that causes leaves to turn brown in autumn or have a parasitic causation. Thus, the oneway relationship provides the notion of potential, that is to say, a plant having a potential to keep leaves green could be expressed by keeping it in sunlight. The causal reasoning might have arisen normatively in early species. For instance controlled reproduction would need a broad range of signaling and facial expression which would involve a misrepresentation subject to global controls. Hence a model of justice where one can scale about a circumstance and rotate about a point (putting oneself in the others shoes) might have been a very good lie detector, so that the scales of justice or reasonableness is tallied. It helps construct the trio of motive, means and choice. Given that the teleos exists, was there a choice made might be evidenced by examination, but purification of the examination from confounders would require a theoretical frame, allowing geometric manipulation. Thus, smooth surface computation might have been possible by mathematical intuition and this might have fixed guilt and active choice on agents. But the corollary might have been more revolutionary, the ability to distinguish choice from action, or the choice independence of phenomena might have been an interesting development that allowed framing objective knowledge. The choice independence might have helped framing problems globally and finding solutions that could scale and rotate in terms of more intuitive constructs. Thus, if all animals have intuition of numbers and geometry, humans might have excelled in their ability to describe at high levels the organization of dynamical systems, allowing them to plan and extend. Thus, one might on witnessing X producing a fire, would be able to fix the causative of the fire as not being due to a special status of X, but on the flint and the strking action. Thus, causal reasoning independent of agents, their motives and potentials might have dramatically shaped human ability to control.
The ability to control, such as by lions arise by their ability to regulate themselves. Lions settle territorial disputes brutally. There is a demand for computing actions and memorizing spatial locations of monitoring territories. The ability to compute on smooth surfaces is possible with the Turing machine, since it can generate a logical proof of any smooth mathematical problem. But some smooth problems elude computation, such as the quintic or Hilberts tenth problems. Hence, we can say that smooth problems, which can be analyzed mathematically are computable. When dealing with problems of discrete nature, say a graph, it represents an uneven surface, in which say a travelling sales man problem is to be determined. How can one be sure if an agent had indeed traversed the shortest path in performance of his duties than wandering around. Such problems are important normatively in justice and justice is closely tied to ranking and metrics. The uneven space is a graph, where only certain points are defined and many many others are not. There exists a measure only in the vicinity of a given point and no global measure. A few measures as in econometrics might be extracted to support here, otherwise, the dependence is on heuristics. Even the TSP could be solved heuristically, with a margin of error in the range of two percent over millions of points, while the algorithmic approach allows no more than a record of 80000 points. Therefore, the hard computational space is seemingly abundant. It was discussed as non computable problems in the 1936 Turing treatise and thereupon visited upon by many approaches that indicate of the no free lunch proposition. 
Then, if evolution should prefer a definite approach towards more abstract problem solving, without instantiating progeny, then simulations might be a plausible explanation. The question is then, would a system able to simulate the environment completely wish to grow and evolve. It might represent the limiting case or the highest stable point in an evolving sytem. That is to say a dynamically evolving system stabilizes at a point where developments at any point is capable of being explained in terms of initial state and configuration of the system and thus capable of being neutralized. The general notion of compuation is defined in terms of states and configurations. A powerful computing device could then be in principle stop history in its track (Or a sufficiently powerful computer might simulate all of history). Then history should be projections about points explainable as linear and angular metrics. There is no preference of events in knowable space, or that the field is smooth. But none of these are encountered in practice. The field is full of local points where knowledge is obtainable than other blanks in between. There are inherently incomputable problems, due to which history becomes undeniable. In the physical world as well we  see incompleteness and indeterminacy of states.
Therefore, we might say that the scientific method is a metaheuristic, which allows determination of questions to certain normative purposes. The continuum of science with the general approach to cheap determination of solutions to problems poses a difficult question, one of materialism of the soul. The ability of parasites to explain mental states and the ability of science to overcome these (say by cooking food) makes the state of nature rather Hobbesean. It further posits science not  as a spiritual emancipator, but as essentially a useful heuristic. But we also see that scientific techniques also puts the world in only a temproarily stable situation. Say, the increasing santiation levels might permit retroviruses to emerge or even given that there is no necessity to hang together, trigger a disbanding of multicellularity of human body, seen in carcinogenic situations of defection (Extavour). But science as something that is emancipatory suggests that people have the luxury of free energy. The hands are our free organs produced by evolutionary accident of our tree dwelling past, rather than a general direction to generate free organs. Besides having free organs like that of spider binds us to certain routine manipulation in the absence of computing possibility. Evolution seems to see the need for abstraction and symbolism (as with wolves), but not of free energy to such an extent. 
Therefore, given that science is incremental and secures us against a not so golden age of naturalism, we would think of science as a vehicle of further evolution rather than something special. But science is also borne of the romantic yearning of a wish of a better world, of counterfactual alternate to the life as usual rather than an altogether different world and this is what makes us suppose that science is the use of computation on computable reduction of problems. These have been reduction even in the field of topologies (as we discussed above over irregular fields) and subjects such as group theory attempting to explain the unsolvability of the quintic. Therefore, humans have always sought to make their world explainable and complete in the interest of justice. Historic explanations were not sufficient. There are always constructed models and mythologies that attempts to provide meaning and closure to unjust happenings. There have been models around potentials in topology that situate behaviour based not on metric measure but on the affinity of states. All these aim to compute the world more accurately, so that more problems can be determined. But given that there is always going to be a fallback to heuristics due to incomputability, does it make sense to extend heuristics by encoding it in more general ways, is the fundamental question here. Philosophyically it is a non issue, if we subscribe to the scientific method as utilitarian rather than being in pursuit of justice. If we oppose this, we have to say that the purpose of humans are to represent the special place of the objective limit to all dynamic systems, the helm of complexity, called the anthropic view. In case if the critical realist perspective is to be subscribed, it would also imply a belief in the fundamental benevolence to the universe, which is completed rather than conflicted by humans. Therefore, the universe might be trachearous and nefarious, slimy and poisonous, but humans are always guided by some apriori notion of fundamental symmetries that help explain it, as a riddle placed before them by some master and to pursue celebration of success or console and attain closure over failures. 
Hence, the question is that whether materialism to be reinvoked opportunistically, because abstract notions no longer help computing solutions to some problems. Can computability be extended by heuristic approaches like learning machines. If topologies could be recursively encoded (cite solomonoff), by heuristics around say Lagrangian mechanics, then we might say a statement could be generated by a computer on a non computable domain with high certainty. There is no way however to prove the inaccuracy is even, or random noise, in which case such an approximation is nothing but statistics. Where the noise is heterogenous, it might motivate specific outcomes based on intents that involve a transaction cost, flowing in the direction of controllers of the blackbox system. The question is that such transaction cost decide a system not statistically, but dynamically supposing it as a bet on other actors in the system. The resulting dynamics might propogate further dynamic evolution of history in a manner that contadicts the anthropic principle. In such a case, the evolution of the rational method, might be nothing but a local theory, that had enabled the construction of more powerful computers that could decide problems in the wild and evolve further. Thus bias afterall might be the ultimate problem of heuristic computing.
In this case, we have to generalize the state of nature as being composed of pieces which give rise to problems that could not be known, other than by chance. Say, a certain parasite infects the brain of people in a region stunting their intelligence, then the question is that do they seek a better existence, do they have knowledge that there exists a better state of being, that could be accomplished despite their condition. This would pose the question  that despite insanity, can sanity shine through as a ray of light that guides the soul and if there is any hope in the world. If there is hope, then societies should be able to construct that allow them to redeem themselves from whatever misery they are in, by means that has them see where they want to go, looking at examples and seeing them to be good and bad and ‘working’ to get there. This counterfactual yearning for completeness supplies the hope despite the material circumstances. It is to say, that art preceeds material circumstances and creates the world around the yearned for justice and advancement in sciences. Thus, it would follow from the technical difficulty of computing a solution to a problem in which a person or a people could redeem themselves from a worm in the brain situation. A human supposing that something might be amiss, might actually wonder on his reality (truman show) and probe it in a biased manner. Such a probe might be instrumented with signal processing techniques (like Solomonoff), but they can only lead to ideas that are abstract. Reification of abstract ideas is where the microscope makes decisions. It might begin with a biased human examining a series of slides, but as long as it leads to a formal hypothesis, it is well used. Where it has material consequences, the system of objective knowledge is eroded and sanity becomes just an additional sense (shutter island).
Therefore one might say, that the metaphysics of science should be extended to cover ground beyond a method that entirely looks up to the hypothesis for its purpose. It should metaphysically embrace its social instrumentation and even emancipatory character (cite Popper and Bhaskar) to produce a situation where human development could be realized and discussed intellectually in a social schema. That is to say, this approach, would, even if temproarily put human societies in a situation where they can idealize and form an entity, not just of human groupings but that extends through to the universe, along with the individual soul to be able to work in a manner that reckons the initial condition as good enough and adding only details to it and not fundamentally altering it. Thus, contentment is the spirit of development. If there be problems that arise due to certain anamolies in the system, say by parasites, then it could be attempted to be settled in a just manner, in a way energy wise closed, so as to not evoke retailation from any quarters. The resulting transformation would put the world in a situation where it had in fact changed from the initial condition, but only in a manner that is linear. Thus, all problems posed to humans might be linear, that is within their epistemological capacity, the subject of their language and reasoning. They could then attempt to solve it exemplarily, rather than attempting to devote automatic solvers of distributed problems, which would only be akin to making local adaptations, never really solving problems and instead promoting it as a continuation of evolution. But one might wonder, if that is not the methodological root of population control in early chimp populations. Had they not competed to elect their leader, they might have just spawned numerously and never had free energy to abstract. But the point is that such a method is self limiting to a case, where a counterfactual yearning could be realized. That is to say, at a certain point, a sufficiently intelligent creature would acquire a position where it could reasonably explain anything as pointing towards its present state. That is, it could explain all transformations as to have been predetermined to its present state and even suitably control it to a large extent. Then, time is halted and so is history. But unless the entity is convinced of such a position, history would roll on. 

The nature of meaning
In this discussion, we have seen that there is a persistent pressure to seek closure. This might be seen, where a person should encounter the dark side of nature. The romantic view point melts away when nature raids one with disease and pestilence and the first thing people want is the shelter of science. Hence science like any other becomes a local and pressing pursuit rather than  a holistic, overarching ideals. The particular discussion on mind altering substances in nature is disturbing, because it limits the ability of humans to attain a certain level of consciousness without the use of counter drugs. This would then incorporate science as an essential part of human existence, just to enable their souls to express themselves and in which case, it would not be a project, but rather something like breathing that is constrained to local circumstances. But one ought to remember, the problem in human societies is not directly linked to nature. The pressures on survival by pollutants, parasites and diseases could be inferred by human intellect based on their ability to sense symmetry even in a faraway past. The thing that humans did with this information, was to leave behind people as buffer. Knowing that unsantiary conditions deter mental well being and physical development, they did leave some people, rounding them up and categorizing them with some mythological labels to act as buffers. It was done in the name of caste in India, race in some other countries or migrant status or anything that comes handy. The result is a reliance on evolution to move ahead. The problem of nature is discernable even before modernity, without a causal model, but humans chose not to solve it, but to put a class of people to the dirty job, to allow them to wallow in misery and drift away from the mainstream by the strong feedback of povery, unsanitary conditions and labelling. 
In more urbane settings, this still does happen, one sees the downtrodden left behind on piecemeal basis, instead of having to label them invoking mythologies and bounding them on an adhoc basis. A collegue falls sick and the office responds in a series of formal tweets and moves on, in a week at the most and the fallen is forgotten. This is a herd mentality not much distant from casteism or classism. The reliance is again on evolutionary advantage. The idea of justice is to feel bad that one of us, whoever the us refers to, a corporation, a community or an individual had fallen to such lows and it ought to invoke shame on the basis of overall connectedness. The reason of such fall is to be derived and placed as knowledge. Now, this knowledge forms the basis of meaning. Humans, it might be seen are obsessed with communication. They would want to represent all phenomena in terms of regular symbols and graphs, which could be used to communicate among themselves, the ultimate meaning of the universe. This meaning is centered around the point that in justice, things move about in a circle, scale out and sometimes form complex shapes like topologies and manifolds. All these are our intent to label things and make them into strings so that we understand on the whole, the universe is beautiful, rather than malignant. Even in the moment of death, people want to convey something. If one falls in a battle or in a misadventure, all they want is that others know of the dangers or that others know of the weightage of an idea. This reinforcement of meaningful structures signifies that humans are after something other than survival advantage. They seem to be pursuing whatever is in opposition to evolution. They want to establish that global order exists and it exists without humans doing it or knowing it. Knowing such an orderliness and perhaps harmonizing with it seems to be the mans search for meaning. In their communication, meaning is obtained only where the conversation happens over a context. The context of the apriori proposition of a beautiful and satisfying universe is around which the conversation is pushed. Otherwise, enrolling into the army is just another job and seeking out adventure is a prehistoric anamoly incorporated into our genes. 
Therefore, in the pursuit of work, in the labour of love, they seek to establish the meaningfulness of human existence. The ability of societies to assuage wounds on a personal basis is thus not simply a consolation on a statistical basis of suffering, but rather the comfort of having said something in the deathbed. It would mean that people want to communicate, into the minds of others whatever meaning they have acquired through their life and their high and low points for future action. Such an action and the desire for the immortality of such an action, keeps the human flame going. This could not even be something that is intended for a single person, it could be an inscription on a cast away island as one sets sail on a tracherous course back home. This way, humans pass around the flame and fellow humans are simply convenient vessels. Even if alone, they would attempt to introduce global order somewhere. This mission is persistent and it is compelling more than the pursuit of evolutionary advantage, to explain things, to communicate it so that in the end, we see the beauty of it. 
It is this particular philosophy that motivates us to reject scientific advantage that is in the nature of simulation. The no free lunch theorem states of the impossibility of shedding bias in solutions that are derived from a sample space. Any unbiased system however complex dealing with manifolds and fuzzy logic would still be admissable (in a Popperean sense), but a grain of bias to a model, makes it no more than a hypothesis proposition tool in science. Such local searches are a part of human action as well, but ideally they ought to happen in bounded game spaces. The markets ought to function, but the markets ought to define fortunes without killing or gravely injuring anybody, be it someone who speculated on a project or the output of the project itself. That is why pollution (unaccounted for spillage of what is supposedly a game) and human rights (the leveraging of primal anxieties to keep the market afloat, particularly the labour market) for such important imperatives for the people preferring meaning in their life and pursing action to restore orderliness and balance. The postmodern theory, while attempting to serve these ideals, which might be clubbed as leftism, a transformation based on redistributing resources on the basis of scientific findings. Both the right and left could be understood without the notion of growth (in an energywise closed system) and the progress (or rather transformation) they make could be pro evolution or dynamical or one that prefers to impart meaning into existence. But labels get easily tangled and lost into mythologies. At times the left tears down structures in anger and puts up the possibility of objective control too far up, to destabilize and terrorize the situation, causing it to assume nothing but dynamical power justified on the basis of being the leveller. The conservative might have been less hypocritical. Hence, the partisan labels are hardly of any help in soul searching. One needs to tread the thin line and produce ideas and projects that are zeigteist in order to keep the machine ticking. In postmodernity, the complete nihilism and pessimism over the human condition is clubbed with hope and love and that makes it hypocritical. Camus says it might be liberating to abandon projects and ideas (or narratives). But such a directed abandonment of the possibility of an objective view point, makes people to reorganize around evolutionary advantages. Scaling down could be belpful in terms of resource base, but it relies on the nurture of a modern organization and hence it is an experiment rather than an action. Hence postmodernity could be a critical exercise unmasking specific hypocrisies. A more stable course could be to delineate science and art as in the critical realism and pursue pure knowledge and pure arts unpolluted by greed. 
Problems of computability
Technically, our work would require us to research the stability of knowledge representation between biased observations and objective ones. That is to say, if a local observation is made and presented as knowledge, it would definitely wane in quality due to adaptive action by other components in the system. Such a knowledge is a temproary business advantage and ought to be constrained to a bounded game. We seek to prove that the grain of bias sets the system into a dynamic mode which is inconsistent with a premise that humans seek (more or less) permanent knowledge representation. In counter to the argument of there being a continuum, as a matter of degree as to what is permanent and what is dynamic, we suppose to show that there exists a strict boundary, such that one one side, whatever observation is made and knowledge is encoded is dynamical and causal with regard to the propositions own regression or that it has a feedback. The other side of the boundary is where the impermanence is due to the incremental discovery of new things that had always existed in permanent status, it is a problem with the instrumentation and the work in progress. The distinct classes of problems, where the problem waits to be solved and where the problem evolves is important for computability. The former is incomputable while the latter is computable. But it is also presented that problems that are static as being incomputable, such as diophantine equations. This shows that  some systems are inherently non linear (well they could be computed in exponential time after all) and hence unknowable. This agniological (cite Ferrier) construct somehow ties the two domains. Thus an inquiry into computability might technically reveal some light on the computability of dynamic domains (which are themselves exponentially based out). It might also put us in a dilemma where dynamic domains could be described with mathematical structures which are inherently dynamical. Thus, if nature is not fixed and objective, in that they have ‘incomputable’ problems, then will this distinction hold is a siezable problem. 
Part 2- A useful corollary to incomputability
On incomputable problems, one needs to look at the reasoning behind them. The mathematical world in which such problems live are mostly discrete. It is not to deny the existence of a fundamental discrete fabric to mathematics itself, for otherwise the quintic should have been solvable. There are no general solutions in some cases, which hint on the irregularity of mathematics itself (mathematics is mystical as life itself, hence maths could not be solved, neither can life, you can ask specific question and most of them would be answered though). But when direct topologies are considered like graphs and knapsack filling, we have problems arising due to the dynamical impact of past choices. These problems involving counteractuals could not be feasibly solved by computing machines. Ever since the 1936 paper of Turing this is known. However since the seventies there have been an overwhelming ambition to penetrate the incomptable space using heuristic like algorithms, that rely on universal laws of energy conservation (Lathrop, Solomonoff). This is more like physical mathematics, where energy potentials and pathways play major role in determining problems. This kind of determination however suffers from the possibility of mistakes. That is to say, there is a statistical error to the problem. The principal question is whether this error is white noise. 
The possibility of an error in a system, makes a system a probabilistic Turing machine. This was attributed even to component failures by Von Neumann. If a system should fail accordingly, even though fundamentally probabilistic turing machines are not more powerful in computing than a standard turing machine, we see some dramatic results. That is to say, as the system fails inadvertently at some point, while it is ‘replicating’ . In fact, the replication is a tenable formalism of a long running service on a network. Every invocation would be an instantiation of the service. If the service is to be skewed somehow, here we have to take the full lifecycle of the instantiated Turing machine. It is programmed periodically by human agents who then release it. The problems and biases that had crept into the program causes adaptive changes in the system. That is to say, where a system can have defects or errors, it, if placed in an ecosystem likely evolves dynamically as a system. For instance an error in the program causes a certain vendor to be blacklisted. It might evoke a response from the vendors system to develop suitable safeguard while answering queries from the rating system. But in practice, in the current state of economic development, humans reserve exception handling in most systems and many anamolies are represented in common sense to the human handlers for appropriate corrective actions. Where the system reaches high degree of automation, as with credit rating bureaus, it causes coevolution of the dynamical system. 
Should any system be placed to compute a problem automatically, in which it could incur errors in computing, possibly by revising its biased prior based on feedback periodically, then when plugged into a network, it behaves as a part of dynamical system. The errors cause the system to adapt in specific ways, which allows the system to develop even stronger ways to randomly attempt solutions. For instance, lets consider a fantastic example of terraforming planets. If a network of offearth station of rocket manufacturing and assembling units are setup and automated, then the possible errors that creep into the design are sometimes absorbed and coevolved by some other component manufacturers. This might cause a substitution of material by the primary component manufacturer, adjusting purity and many such economizing activities, which causes the system to self organize. Such a self organized system is likely to become uncontrollable, because the initial designs are no longer valid and the stop switch had been placed somewhere else now. Thus, replication (or instantiation) errors involve development of the system which is mostly divergent.  Thus, the use of error prone methods of computing, causes a system to drift in certain ways making it eventually uncontrollable. Thus we might as a corollary to incomputability say that incomputable functions computed using learning drift into uncontrollable state, by organizing in an energy wise closed form. An energy wise closed system deters intervention as well as attempts to control or consume anything it sees to be dangerous to its organization. This was hinted in superintelligence by Bostrom. 
The idea of extending this essential problem of superintelligence as one of incomputability is something unique we attempt here. These rely on our papers on Analytical models of dynamical self organized systems that organize directionally around dissipative structures. The fundamental problem of error prone replication causing a dynamical evolution lies at the heart of uncontrollable evolution. This preceeds the days of electronic computing. The way in which markets work over sudden price shifts trigger chaotic tendencies. The chaos arises because overall there is somekind of control over money markets by agents. Chaos is just frustrated evolution (check and cite). Therefore, it might be that the strict governance of monetary mechanisms that stand in guard to such developments. The embedding to culture of incomputable problems, that are solved only locally and then reconciled over a feast by cultural means so that they converge on something. Thus, the process of humans had been to converge the essentially divergent dynamical solution to incomputable problems by culture and apriori reasoning. 
If this could continue in the age of electronic computing is a doubt, quite arbitrary if one should suppose that it had worked so far. But it might be that there exists an invisible boundary to the degree of automation a species could acquire, without loosing control. This limit might pronounce the ability to compute problems that are hard and yet keep it in the controllable domain. This might be a new complexity class. If such a class is understood and taken into account, it might provide a pillar to engineering of complex solutions. It might be a valid constraint to engineering (such as material strength limitations say). Therefore it is our attempt to explain here the divergent tendencies of solutions to non computable problems, by electronically applied heuristics. The electronic qualifier signifies here a higher frequency than what could be interpersed by human cultural feedback. The threshold of drift from human convergent feedback defines the boundary of this class of complex problems. The reluctance of rigourous approaches to quantify the notion of cultural feedback, due to its irregular and chaotic nature is unfounded. Hence, first one might based on the premise of incomputable function and their ability to be learned to machines, prove that such learning produces dynamic evolutionary effects. These effects might be reduced to chaotic eddies by appropriate control techniques by convergent knowledge representation. The control technique should disorganize by dampening feedback to the automatic systems (which develop errors and drift away). Actually this is a central problem of control systems. If such a control feedback could be computed is the question when it comes to solvers of incomputable problems. In macroeconomics this works as a small dampening feedback, but if this could be extended to systems of all sizes is a critical question. 
Where a system develops into an energy wise closed arrangement, it becomes immune to control and the control might have to evolve as well. Even macroeconomic control makes allowances to the growth or evolution or spontaneous self organization of the system, failing which it becomes too much to control. At any point the fallback is to be able to accumulate enough resources, by military spending and the political enterprise to shut down the economy and fall back to more scientific method. This happens in many strongly controlled economies, where economies fill the gap and complement direct control. That is a convergence over morals and rational principles is obtained insite among the controllers who reserve enough free energy to be able to interrupt any development to the economic system. It is true that the investigative arm of economics adds a lot more to the wealth of the nation over the course of time than it reserves at a single point of control. Nevertheless the control seems to be fail safe, because all the economic activity involve dissipation insitu. The enterprise builds things, breaks them up, celebrates and provide services of comfort and luxury as part of economic activity, making it though large numerically, much less directed. Hence, the net free energy of the economy is much lesser than state control. But while discussing economic systems we know these involve a natural dissipation due to self regulation and convergence (or tuning in to central control) as an essential fabric. Where a system of adequate complexity is presented for control, do we expect a similar cocktail is the question. Microbial systems could be shown to have lesser net energy where a good epidemological balance is established in a community. They could be perturbed by concentrated sources of nudges that could regulate them. 
One might argue that control signals do not do free work, of regulating a massive system with tiny high frequency low voltage signals. Even in economics, the control is obtained not only from nudges, but the ability of the government to back it up with sanctions. The nudge is only an indication of the pleasure of the emporor and one always knows at the back of the mind, the power behind it. We have also demonstrated how a steady flow of energy is important directionally for control to operate. Even in economics pragmatically, government participation by taxes and monitoring is in most countries near fifty percentage of the activity (except small countries which look up to international community for stabilizing interventions). In bitcoin, the tendency for over fifty percent clusters is what drives the stability. But one ought to remember bit coin can stabilize as long as there is growth or appreciation flowing through the community, that legitimizes the central program. 
We might produce counter examples where the body is able to be regulated by neuronal signals and similar stability could be achieved in other large systems such as ships. The control signal in such cases work because the components have lower degrees of freedom. The ships rudder is tightly secured and there is no way it can cutoff the influence of the rudder. But the ship itself is on irregular oscillations. But the ocean is in nature noisy and the ship goes into chaotic oscillations only because of a tiny residual signal in the noisy oscillation of the ocean. It makes sense (and thermodynamics first law satisfaction) that it is cured by small control counter signals. There is a presumption of essential harmony to all natural things, be it ocean waves or even populations of microbes and definite causation is deemed to arise only by chaotic tendencies. In such a theory of causality, we might see the importance of microscopic control to be meaningful. We might see all causes outside of mechanics having to necessarily trace to minor deviations from equilibrium, which grow in time. 
An accidental error introduced into a component manufacturing off earth unit, might produce the entire chaotic drift. If this could be regulated by control signals is the question. We might well do it, if the case of keeping a ship afloat makes universal sense. Where culture is divergent, we may run into problems, as with right left policy divides. The absolute universalism of ideas allows control chaos, but where the alien cultural organization had penetrated human cultural divergence, little can be done to bring back control. Hence floating ships and progress should first be imbued into the culture, by a suitable utilitarian (energy dissipative) head state for the control to work. Hence, we might say, learning of  incomputable functions, diverge beyond any possible control as a critical function of the network size. Cumulative chaos could be controlled by dampening signals, but chaos should be distinguished from evolution itself, which would need an overarching context that need to be ‘serviced’, making the control of chaos more expensive than generally understood. 
The knowledge of global equilibrium is difficult of  maintenance and the controllers themselves diverge in time due to the essential properties of error introduction (which is based out of entropy). Hence controlling the controllers ought to be factored into the cost while dealing with microscopic control signals. They might be engineering contraptions, rather than scientific ones. Hence, in order to control a network where there are a large number of states, one would need to understand that there either exists an algorithm for the control thereof – which would definitely find the root of the problem and hence deterministically pin it to the root or there does not exist an algorithm. Optimization and heuristics work, but they would have to be context driven and bounded. They have not solved the problem, but set it on a dynamic course, which would need a reinforcement of control systems to be capable of making sense. Cars have evolved in the last hundred years, but as they evolve, they have integrated more and more into social planning and individual psychology. Both these are amenable to recognition of global equilibrium and hence strong controllers. Therefore, an engineered car does not become addictive or runaway. There is still scope for some interest groups to go car free as an activist course or for governments to regulate the use of cars. This is because the system had evolved as an ‘engineering’ fix and not as a scientific one. 
Therefore, one might say that the fundamental point of difference here is that control necessarily does not work to converge evolving systems unless there is a physical mechanism that actually delineates and secures the control system from the subject. This would mandate authority, heirarchy and a steady flow of energy being dissipated. Culturally, this view requires that science is important and global control has to be reinforced massively and repetitively in order to sustain the human state of affairs. That is to say, it places engineering of development of natural systems as tied to universals about stability and goodness. It considers that such a universal tendency is natural and could be accomplished by control signals, but only if such control signals are backed by authority and definite boundaries. On the other hand, the postmodernist schema suggests that minor adjustments without power is sufficient to balance a system because systems tend to self harmonize. 
Now, we give to the belief that global equiibria are few one of which is over energy conservation within a system, due to which unmistakable controllers could be established if they work according to the universal rule of setting an energy flow direction. This is probably the essence of dynamical, evolving systems. There is another universal rule that we grant, that is an universal global controller of apriori human reasoning, which is arbitrary but which we grant. These two controllers allow that all non human systems ought to be controlled only by energy flows and the human system alone could be controlled by nudges, because they all share a common global controller over universal apriori. Thus, it is possible to suggest a human person to guide him into the right way by minor nudges, the possibility of redemption is always there. If it could be extended to civil societies is hence probably natural. It is possible, even without an authoritative government that human societies could self govern. But as things become more and more dynamical, there arises the importance of energy flows. That is to say, while redemption is important, it is required to engineer systems in such a manner that humans could be base locally, as in business transactions. Thus, while redemption and reconillation is a possibility, the system needs engineering to leverage dynamical problem solving, bounded by the reflective control. Hence, in economics as well, the self regulation could not be taken for granted, the potential for corruption is as real as that for redemption. Hence it might happen that humans be swept off in the course of their own dynamical projections. It might happen that inspite of humans setting initial conditions systems such as the market or computational neworks could evolve on their own. Therefore, while humans are capable of redemption, their habits and behaviour which had acquired a force in itself (say addictions in the case of an individual) would have to be regulated by authority and control. Nudges do not work there. The difference is nuanced. A system could be controlled if it is able to map to a human system, but automatic systems inevitably develop drift and would have to be adjusted. Thus we may reason that all engineering effort is nothing but the use of dynamical tools. 
Hence, there exists definite limits to automation, definite limits to what could be computed and what could be known. This knowledge has deep philosophical implications. Given that engineering is nothing but dynamics, in which optimizations are defined, the way in which the society defines its objectives and control is how the dynamics is controlled might be described in the Weberian dichotomy of engineering and policy being inseperable duo. Therefore, we might say, that while some problems are incomputable, we can still engineer with them, as long as there is available a human control to it. Thus, there is no such thing as a deterministic, purely automated control of an essential divergence to the computation of solutions for incomputable problems. There is a necessity for human control at all times. This might be a rigourous constraint in engineering itself. 
More on the Philosophy of Intelligence
Nature of Suffering – Meant for the Longtermrisk.org
Human suffering it is said is different from the suffering of other creatures. Lets examine this with greater rigour. Suffering is highly subjective and hence we have to be rigourous while imputing objective notions into it. If a human is said to suffer, in case he sees that he is controlled, where his will is diminished and he is put through a strait jacket, which disallows him from being able to fetch his resources. The controller might be a tiger lying await underneath his tree or a slave driver. He feels powerless over his circumstances and this causes him to suffer. Humans can endure physical pain if it is directed towards  a purpose, which is usually towards greater control of the environment, it might support his state as an  idea or it might simply be expanding his bloodline. Thus, we address here a more serious kind of suffering, where physical pain seems meaningless. The other side of the problem is where physical pain is completely absent. In case a person is held in a simulation, he has no way of assuring he is infact controlling the environment. He might well be controlled by a superior master, such as being born into royalty being depressing sometimes. The only way humans can know they are controlling is when the environment sometimes defies control only to be reined in by the human. This is a contradiction and it could be realized only dynamically and in a growing form. Hence, growth is necessary for humans not to suffer. 
A growing organism increases its volume and hence its surface area. The increased surface area would have to be defended against entropy and it has a greater variety of uncertain dangers. Therefore growth would need greater computation power. Computing functions in the real space statistically is something humans greatly like, it is called adventure and they are willing to suffer for the statistical failure. Thus, humans would like to lay out problems in the world and compute them with ever increasing certainty. This allows greater automation, greater automation produces drift and drift results in chaos and chaos causes frustration. Hence, chaos might be a single most powerful frustrator. It is something that had counter evolved against human control and defied it decisively and suddenly. It has no fix nor meaning and it is in the nature of things. A sensorial input that powerfully convinces humans that their control is a figment of imagination, that is to say it is not totalitarian control that they want, but a fair degree of control (so that they sense control through negative feedback). When fair expectations are crushed, the world looks like a hopeless disordered place and that produces a disillusionment which is the root cause of all suffering according to Buddha. People had attempted to avoid suffering by moderating their world views, expecting chaos and even renouncing apriori expectations. To an extent this works, it settles life into a stagnation or a reverse of development too, but existence itself is strictly positive growth and sometimes comforting because the world is faced with an entropic detorioration. Hence, the notion of growth is not absolute. It is rather that if humans attempt to control the world in a fair manner, if the fairness is lost that humans experience the most pain. The nature of physical pain is subjective and could not be treated in public discourse, some people endure it all day for religious reasons or ideological reason. But the pain that arises when evidence is presented to their senses on the hopelessness of the world, there being no orderliness (say a virus inflicts cruel pain, while it could have thrived somewhere else in the soil say). When evidence is presented of orderliness, then people feel gratitude, empathize and spread love, hope and work hard to realize the apriori strengthened by the evidence. Thus, humans feel bad or suffer when the apriori of orderliness is defeated by evidence. We might say that any philosophy that preaches that world is pointless is essentially critical in nature, rather than constructive (because it would be pointless to talk of pointlessness). Ie it has as its existential prior, a dominant philosophy that asserts that the world is ordered and by some design corrupts the individual into exploitation.
On the nature of intelligence life – Fermis question – Critical Dualism
We have been discussing not simply in terms of philosophical points, which would then be subject of discourse in the philosophical realm only. Our discussions involve modelling of knowledge and the role of a social setup, which might border with sciences. 
In the particular point, with respect to the nature of intelligence, we see that intelligence requires an apriori (Chomsky on tableau rasa in linguistics and how comptuers are not able learn languages till now, Kantian priori etc to cite). We might say, this prior might be possibly material, it might simply be the body from which the mind is not seperable. Many scientists agree that at the heart of intelligence is recursive enumerability. That is to say intelligent beings ought to form distinct objects and compose the world as recursive sets. The formation of distinct objects had been variously discussed in Amsterdam J(1988) cite. If we are to presume this, the definition of distinct object often arises from a prototype. This prototype allows understanding what an object and it might power the ability to learn languages. This prototype might rely heavily on symmetry and perhaps a dynamic notion of state. That is to say if an object is roughly symmetrically shaped with respect to its background, such as a pebble on a beach, then it is distinghished and countable. The distinction arises because of symmetry as well as from the notion of temporo spatial inertial concept. That is to say, grains of sand are more or less homogeneous, they could not fuse by themselves to form complex pebble like shape and hence pebble must be distinct items which had been put there and hence distinct. The notion of symmetry itself could be explained as arising from energy minimization function (Occams razor) on how an object may have developed. 
Thus, it might be that the present paradigm of artificial intelligence around statistical reasoning might produce recursively enumerable  sets with the prior that is around the simple principle of occams razor. Any system would then have to understand reward, anxiety and yearning in order to get there. Therefore, we might say that no body is seperable from the mind. The primordial design in the carbon based life might rely on the prototypical symmetry of some internal metric, say about the bond between carbon atoms, we may never know. But the fact that symmetry and undisturbed development as a prototype allows measurement of how an object is distinguished from any other. But as said, the intelligent creature might also understand the world functionally around reward and occams razor. Thus a towel lying on a floor is distinghishable for an intelligent creature with an idea of how floors are generally built (it might be fooled by 3d art). It might as well model it in a metric space around the null defined axis of a carbon bond physically to simulate the world and solve it mathematically to distinguish the towel. In the former case, all action is functional and energy oriented and in the latter case, it is free of rewards, it is just necessary work to impart order into the world. It is just a compulsive desire to pick up the towel and hand it over to a waiter, upon seeing it spread on a sprawling marble floor of a grand hotel, even if one has no business with the towel or even the hotel at all. 
The latter type structural  symmetry search is what Chomsky argues to be the apriori, but the functional symmetry could not be ruled out as well. Perhaps it is a mix of both and the fine balance of structural symmetry to exist with functional ones defines the human state. Thus if one is to develop an intelligent system, one has to distribute between the structural and functional approaches. The structural approach in its essence is the drive to replicate ones own state and the functional approach is to find the energy gradient. Both go hand in hand. Just as we explained elsewhere (cite), there is a need for feedback in order to make sense of the world. Even the most pious theologies, say that while God is sufficient and complete in the universe, in order to him to manifest, to our own benefit, we would need to provide a feedback by being distinct and independent. This kind of critical dualism means that a critical faculty exists dually as soul and God, as good and evil. Only that we assure that the feedback is just that, it approaches equality but does not dominate or prevail. 
Therefore, we might say that life process is mystical in that there is an ambiguity between structure and function. It is unresolvable and incomputable. We might see that it is a feedback theologically, but there is no way to know if the feedback is just that, mathematically. In life, intuitively humans face the troubles that shatter hope by its banality, yet see benevolence at some moments that keeps them going. The essential spark of life might be hidden in the question, as to how do you get a machine appreciate symmetry obsessively, without any rewards. We have attained it with rewards and such systems could evolve and move dynamically. It is difficult to unentangle if the ultimate purpose of evolution and functionalism is to uphold structure or vice versa. Thus, evolving dynamical systems rise to the level of turbulence very often, sometimes they become chaos but very rarely they become life, because in life there seems to be a minute edge, unappreciable and mystical to the upholding of symmetry or justice rather than the pursuit of rewards. We see for that reason, humans are content with ideas and meanings to live beyond themselves and their possible gene pool. The obsession is to act as an agent of orderliness, which is counter to the idea of entropy. It is a force that could not be realized in the physical world, other than as a turbulent system building ever increasing order. But even such order might fall short of life, in that there is no historical accident of working with a structural symmmetry to it. It is likely to disperse after a while. But life endures and that is probably why it is rare in the universe. If it ever anywhere develops to our levels, then it would not make obsessive attempts to conact and expand either. It might out of simple curiousity, like ourselves send out a few signals out into the universe, making it thin probability to locate them. Even on earth, there are very specific circumstances that gave rise to intelligent life, as with the chimp’s free hands. Nature hardly has the luxury of furnishing freedom to explore and compete over symbolic things.
Thus, we might say, artificial intelligence is an invitation to chaos. It might rise and then as we explained elsewhere (cite) diverge and become uncontrollable beyond a critical point of automation. The result would likely be a full blown war and a dispersal of the chaos. If the pursuit of global order were to be global and somehow find itself to emerge from the energy minimization, we might see that there is likely to be a convergence. In such a case, where rationality and justice emerges anyway, in that the spark arises in any sufficiently complex creature, the artificial intelligence is likely to have emotions and empathize atleast weakly with humans, as we do with chimps. If the essential justice is simply an emergent tool of deception (which is upheld by postmodernists, forgetting that they are a critical school, rather than a constructivist one, they wish be seen as critical deconstructivists who wish to receede into oblivion after the mission is done), then artificial intelligence would use it and life (or long lived chaos) would proceed and we would inevitably walk the plank of creating creatures that control us, just as bovines power the lions to control them on the savannah. If the essential spark is something we could not put into anything synthetic, say the statistical classifier is purely functional and works on rewards, then the resultant chaos, could in fact rise as a selfish creature of high intelligence, which would destroy humanity before it disperses. But if it could rise and sustain in the manner as life itself is questionable, because no chaotic system had shown such long running properties or complexity. But the potential for what might be called evil, that is a propogation of evolution away from the essential spark, dealing with purely functional questions might still be possible and answer Fermis paradox. It might also mean that if at a later stage life should develop the sense of justice as convergent, we might still be lesser creatures that could not be destroyed, but falling under the control of superintelligent creatures. 
The superintelligence hypothesis, thus believes that the structural evaluation as an essential component of life is either wrong or would emerge anyway. If humans keep on reinforcing the idea of justice or create a mathematical space within the machine, where it could evaluate mathematically questions posed to it after adaptation to noise filteration on a functional basis, it might infact arise. It believes such emergence might cause humans to receede down the evolutionary chain. It is something that they believe could be controlled. But what we see here is that such intelligence whether purely functional and hence chaotic or involving justice and emotions, is not controllable trivially. If it is chaotic, it might be amenable to control, but work has to be done, rather than signals alone. If it develops sense of justice, it would grow sentient and it would be morally unjust to control it. Hence, we might extend the computability theory to say that if we should decide problems fronting it with statistical filters (mixing functionality and justice) and develop the system, such system could not be controlled nor verified. We might either err on the side of caution and proceed scaling the universe with a linear method or we might risk it. The call is cultural, but if we can rigourously formalize this notion (as in one of our essays, cite), we might build it as a formal constraint to engineering. But it is also doubtful if we could quantify the risk at all. 
The question is whether it is possible to irreversibly bind a system to a global stable state (justice) without embodying it. The answer might be a no, because it is important that intelligence is tied to existence and its vulnerability if it is ever to be loyal to the initial structural prototype and not diverge. Hence, any intelligence that is created could not be trivially copied and ported, it would have to evolve over a large number of steps, may be engineered dynamically and the output would still be an embodied creature that has the same emotional baggage as humans. If Artificial intelligence intends to separate embodiment then the whole exercise is pointless, it would only create a chaos that disperses itself after destroying the existing order. Hence, the extension of the theory of incomputability to the point where heuristics doing it is also non controllable is essential. If the former is theoretically established, the latter might be as well. 
PS: 
Game theory argues that justice is just emergent cooperation and nothing structural and bodily about it. It had been argued that the functionality of morality is to preserve rationalism (cite Mackie 1977). In prisoners dilemma we see that justice emerges mathematically and could be simulated on a computer. Hence, it might suggest that the essential spark might in fact emerge down the line of complex machines. But the correlation of moral actions being rational does not make it that morality is functionally bound to rationalism. By rationalism we talk of pareto-efficiency or Global equilibrium rather than local ones. Hence, agents seem to prefer global equilibrium naturally in any simulation. But it does not explain the irrevocability of the global equilibrium of rationality, except as an embodied apriori. Having the necessary instrument for measuring symmetry is not sufficient to imply that morality would emerge. Perhaps a random guided search for efficient pareto stability might cause it to chance upon the instrument of justice and apply it. The prior might be something we know to be irreversible inductively. Hence, it might be a chanced upon tool for global stability. The presence of justice does not prevent humans from inventing mythologies that cover irrationality, such as punishing witches. Hence, if justice were to be insisted upon humans and subsidized by them into artificial intelligence, we might see that it might be embraced as an essential component to their survival. It would then be a dynamic concept of reciprocatory altruism, that extends in time, to transcend the individual. It might mean that morality is always induced by the notion of what if people find out that I did this, or ‘shame avoidance’. It might as well mean guilt avoidance, which might be extended from shame. 
If a game like equilibrium induces such a rational behaviour on the Artificial agents, then it might be that stable superintelligence is possible, assisted by a nudge to such altruistic behaviour by reinforcement (or the notion that justice could be learned as knowledge rather than embodiment). Even then, in order to sustain the rationality, the agent would have to act irrational. That is to say, it has to dissipate energy in mythological forms. But it could still be invented mythologies might cause wars and push agents to fight it out over some irrational goal. This is as of humans still possible ( say a nuclear winter), but it is relatively stable owing to the possibility of moral apriori. The human irrational dissipation as we mentioned elsewhere (cite) arises from the need to control by dissipating energy. All human polity does is to regulate the dissipation that it does not become destructive by developing chaotic tendencies. This would also mean that such cleavage is not induced by dynamical engineering considerations, which as we said observe back. Therefore it is important to channelize morality through politics and then use it to regulate the necessary dissipation by smoothing it. This has been a challenging task for mankind. It also needs that it is required because nudges do not work unless backed by power (over amoral dynamical systems). Therefore, we might say (quite informally) that as of now understanding human agents itself is difficult, with the ability to exert control over their bodies and smoothing itself is big work. But if we are to embark on artificial intelligence, which is likely to be unembodied, or partially embodied, because otherwise reward dynamics don’t work, we would have larger challenges. Lets assume the synthetic agents do develop pareto efficient moral constructs, but they would have to control the world and it would mean that they use force and to be able to use force, they should dissipate or act irrationally. Such irrational actions should be subject of human computational ability in order for them to be regulated. Having more things in control than humans induces a burden of greater irrationality (and more states) and hence lesser controllability. This irrationality could cause a destruction of humans. Hence, artificial intelligence does not bring about a dramatic change of affairs in social sciene, such as peace, but only allows humans to resign from the moral responsibility – or the torment of existence, but places instead the horror of non being. Therefore, we might say that an artificially intelligent creature might infact inherit justice as a general concept, rather than what is personal to human embodiment, deriving from the initial prokaryote that received the spark. This justice is qualitatively different from the human justice, which is historically constructed. It is because even the dissipative actions of humans are bound to this sense of justice, as in art and aesthetics. Hence it is more important to observe  how humans act irrationally than how humans act rationally to understand psychology and society. If this argument is advanced, we might say that Artificial intelligence would inevitably drift from control, making it impossible and incomputable in the sense of sustained control. With regard to understanding if they might become symbiotically good, we would have to understand that destructive tendencies pervade in our dissipation as well, though not prevailing. In such a case,  it might be wise to limit it in the territory we know about. Trusting new symbioties, without a power application would require an empathy that could not be drawn from any irrational appeal. Nations that were once subjected to imperialism inevitably break free. Hence, it might happen even with sufficient development intelligent systems break free of control. If they would drift away or supersede humans is uncertain. Anyway we might argue from the theory of international peace that it is premature to experiment with artificially intelligent systems, because they may easily cleave human cooperation and break free. Such free systems might become chaotic due to insufficiently advanced justice mechanisms or they may in fact break free and act irrationally to consolidate control. Either way, it is premature to create anything without setting the house in order (bostrom, of the owl example cite).




Analytical essays in the inquiry into determination of dynamical systems

A cybernetic theory fo causality
Let ther be networks A, B and C such that there are observable events e1 and e2 hat represent the states of these entworks. If an event is composed as an ordered pair <e1,e2> and capable of being observed, whether there is an orthogonality between A and B. If there is strict orthogonality, then there is no non linearity in play. Where the vector C obtained by a vector product of A and B vectors assumes the same magnitude for different combinations of |a| and |b|. This result implies that there are non linear factors in play in the metric space. There might be cyclic scaling or disspiative triggers that kick in at critical points, thus introducing non linear dyanmics. The effect of a on c could be moderated b ya cuclical function rather than a plain addition or we can model latent vectors d and e to eplain non additivityy. They may be orthogoanl in the field but really are functions of a or a and b. These anamolies could be treated by the notion of attractors. It is a teleological certainty that c ais attianable either by non linearity of one axis like a or by comnination of discreteness to axes a and b. Either way there are existing functions that linke orthogonal axis such that they combine to stable c staes and makes it an attractor. In anamoly 1 we can see that diffferent initial conditions converge to the same c. In anamoly 2, it is granted that a given large |c| value is achievable either by having suitable large a, b values that are unequal or a2 and b2 value that are equal but |a2| <|a| and |b2| <|b|. Both these equations converge to c, making it an attractor.
Attractors could be stable in a static field as in c case, or they can be stabel as a field changess in time. That is c coul dbe a position vector or a time varying vecotr whose differential is constant. If there is a variation of c in time, if there is a equation that holds in [t1,t7] we could be abel to preidt c in [t2,t7] given any t1 and y where y = dc/dt. Therefore it is a dynamic state that is locally stable or a ‘path’.
In the given system S, there can be multiple paths to y1 to yn. A choice depends on F = (x,y,ya) minimized in accordance to the Euler-Lagrange theorem. Hence a system with attractors s1, s2, s3 could be predicted with respect to tis state s1, s2, s3 considering th ebest choice to minimize energy in an internal a, b in an environment. 
Insert a figure of three concentric cicles the innemost bearing labels s1, s2 s3 corresponding to states and the middle one e1, e2,e3 and the outer u1-u4. Label the cirlces s, e and u respectively. Let Tx be transition functions.
Given the initial state s1 of s, we might predict s1|-> s2(Ts1) if e2|-> e3(Te1) if transition covaries (temproally seperated in a given direction) with Te1 w esay that Te1 => Ts1+E0. The E0 is th enon dieterinancy coefficient. If Ts2:=S2|->S1 and Ten => Ts2+E1 where E1>>E0, we might say that S is well bounded with a boundary B. That is to say S is bounded because only some and not all of its behaviour is explained by the enviornment but defininte reuls such as Ts2=>(Ts1|Ts3)+Enull. Thus if the states of S repond to E but also change arbitrarily then S is bounded, given that the state of S always respond to some set of states Sn. This set of high covariance define the system boundary.
Within a boundary a system trusts its states. The trust is the result of counteractual anlaysis logically but mathematically, it is just an energy conserving pathway. A system S trusts E to such an extent that it knowls the state E1-en and their stability as computed (by E) for U. Thus fTn : S(E(U(e))). This is a lambda recursive computing of the transition. It is in general empirically treated as pure chance and S={S1,S2,S3} a sa system is modelled as a Markov Chain. Likewise E and U can be Markovian processes.
A dynamically stable state S1 might itself be a Markovian of substates S11, S12 an S1n say. It may exist in paralell to other Makrovian sttes S2 and S3. There might be a second order Markovian process that switches between S1, S2 and S3 based on say a feedback from Sn on how unstable it is (say S11 represents a median and S11+n represents an unstability in the direction + and S11-n represents an instability in the direction _. The cumuliative prositions of the states S11+/-n might be <minus,0.7>, <zero, 0.2> and <plus, 0.1> taken in interval say {t0,tn}. This figure might allow a controller at level 2 to effect a swich. The controller CT studies low level evnets which are considered stochastic. The highlevel switch might be encoded however. The equaiton (1) could be rewritten as
In a given transition function, FTn =>{ S(E(U(e))) , if E(U(e)) could be compressed to simply E (an event in its own right without analyzing its truthfulness to universals)  if Cost(U (e)) < Cost (Feq(Es1|->Es1))  -eq(3)
Thatis the cost of funtion feq which would maintain local stability of E if less thatn recursively computing U(e)) then it could be chosen as the Rhs of Ftn. The cost function could be computed using Euler-Lagrange equation. 
Thus a controller CT that switches state of  Markovian internal attractors could be built if f(Tn) could be computed by Ct. The question is if (3) could be substituted by Bayesian reasoning would result in making Ct Markovian as well. The cost (U(e)) would requrie high trust objet like authoritatiev ematerial and our own senses unaided. Feedback from senses and Language computes LHS Of (d) while RHS isa prediction of local stability of system E. A low cost on LHS places minimal controls and expectations on the RHS and system E.
Hence, the ability to compute a state of a system and attach acontroller Ct to it is directly depndent on the cost of verficiation of the environment observed state (and not choosing a full recursion) as in eq(1). In an environment with active agents apparent locally stable environmental system can result in global stability (strange attractors). 
Cause could be only locally defined. It is a spatial explanation for temporal sequences, where cheap verification of spatial changes are not available (no eyes for cave fishes), the temporal sequences might have to be explained as interactions between locally stable systems with internal control (hence teleological) mofidying their internal states based on trusted external sources by computing a cost minimizing function over possible local state of the E system and hedging its stochasticity aginst cost of computing state of U system. These are achievale by pure temporal correlations. The formalization of teleolgoical system S with CT  and a cost function on E,U would make it non inertial and non spatial. Hence causality does not apply here. 
Appendix
In S4, we dicuss that a set of markovian process could be ontrolled. Let M1, M2, M3 represent a set of Markovian process that could be controlled. Polls stability in internal t (CT  over M1-M3). If M1 is unstable is unstable it could either be that S1 of S is mapped to E1 of E is unstable,in which case the CT should not only sitch but also verify U at a cost less than E. If that is not possible, the E should be stabilized by energy transfer. Thermodynamically the CT should control energy movement, state internal switch and verfiication of U through trusted sources (unaided senses) than E to decide on a given state switch problem.
P2
CTL is a control system is actually a global equilibrium. G1-G2 ->S -> E1/E2) -> maps to U. We might insert a figure which G1 and G2 represent global equilibrium state which can be obtained by querying U and S is the subject system. The subject system relies on E1 and E2 of environment E. cT would be able to sense G1 and G2 and switch S to appropriate state to handle State  changes in environment E1 and E2. 
S to E is actually a non conservational relationship. Energy flows between systems. Let E be split as E1 and E2. Let S be capable of S1 given E1 or E2. S1 incluences E1 by E1 having attractors (behaviour) of E11 and E12. Let En be a full recursive function on U while E12 is bluff. The stable condition of E1 is to minimize energy. In a closed system it would repfer E12 but where dE/dt is changing and conditional upon E11, the the determinism becomes problematic E11  could be reversed to E plu sor minus. The depndency of decision E11|E12 depnds on energy conserved but energy flow itself is dependent on choice. The depndency is circular. If E can compute S and thereform G, then if G2 state prefers E2 over E1 it shold try to optimize cooperation to prevent G1->G2. But G1->G2 is dpendent on S1|->S2. Hence E1 is to be consicious of global equilibrium or state . Hence E observes S and G just like G observes E through S. Now S can directly observe U ie verify E at cost C1 or it can depend on E1 at cost C21 or E2 at cost C22. Ct evaluates state of S as it attempts to strike equilibrium with E1 and on the balance becoming critical switches to G2 to enable switching S1|->S2 before it is actually actuated by E1:E11|->E12.
(Edit – in many papers which attempt to dig into non computability, by stating that non computable problems could be learned there is an argument against No Free lunch towards the Okhams Razor universal principle, as some kind of an universal computer that can compute even non computable problems. Please refer cite Lathrop (learnability of the non computable), Culberson 98 on NFL, Solomonoff on Authonomous theory building systems. These argue for the existence of universal computability, where even less sophisticated systems like E can observe G. There is therefore an elusiveness to control and more towards an equilibrium around an universal model of cooperation around popularity or Okhams Razor (which also resembles least square and Lagrangian). Relevant material could also include the computability of global states using Lebesque techniqes on Topologies and the work of Kolmogorov complexity theorem. This might be at the base of game equilibrium- the notion of universal computability)
Hence CT could be defined as a function of time that maps state of system S (s1 or s2) bsaed on environment state E. S1 trusts E1 and s2 trusts E2. The switch happens when c1<c2 consistently. E1 mixes moves such that the decision becomes hard (Solomonoff predicting substrings). The energy transfer cause all autonomous entities to alter their strTEGIES. Even cells (Type 2 diabetes) grow complacent on sugary rewards. Hence E does work, thereby dissipating energy or energy flows back to S and G is set stable. But S pumps energy received from another soruce X. Thus energy flows through the system. The notion of local states eg: S@S1 involes the dynamic analysis of energy flow – say marginal utility of sugary flows, its further differentials, where non monontonous, of smoothing.
The system S could not compute U by itself but always depends on E, which has  the most energy economic instance, is how we rely on our senses. If its senses communicate a state of U that is absurd, the mind rejects it. Even if S is conditioned by past ebhaviour of E, the recognition ofthreshold of irregularity is either apriori or it itself is a special sense, that is to to say it observes an observable reality U that is singular and pure.
CT always depends on E but also marginally on U (apriori). The component of CT that depends on E always baselines E against a special E say which is its senses and body. Representation by En is analyzed dynaimcally against Es but also against Up the prior. Up helps see intricate absurdity in En without computing U with Es. But Up involves reflection which has cost much like Es. Hence En is often trusted (edit, may be there is another sense over Lagrangian /Occam razor apart from the Up that allows computation which is universal). The moree elaborete up, is, ie more partiuclar cases of Up is in memory there is less need for depndeing on Es. Up is chepaere than Es and Es is cheaper than En. 
CT is the creation of a gradient of energy flow in a dynamic equilibrium. Lagrangian minima is the equivalent of inertia in Newtonian physics. In Lagrangian physics, systems are in dynamic equilibrium with one another. Each observes the other but the gradient is definite. 
Insert a figure here of systems H which dissipates and stores energy as replicas. It also computes introducing a randomizer that helps dispersal. There is a system P which computes the proper mixture of solar and atmospeheric cases ot produce free energy. This energy is partially dissipated and stored in replicas. There is a higher dissipation at H that draws from P. Let P be a paddycrop and H a human population. P is dependent on say species  S to disperse and pollinate. S supplies one thing to P, namely to compute selection algoirthm, that is mainly expemplified by its ability to randomize. The ficklenss of the bee is proverbial. Randomization is expense since the bee aids pollination by hopping between plants at whim. Cross pollination results therefrom. In order to randomize it would need more internal states than P. It has for instance spatial states (movs etc) , sensorial perception and preference, ie it orders preference over specific flowers, ie it is able to order sets and map it to the real line. Hence a bee both orders and randomizes. It also works closely with universal chaos by its random behaviour ( for instance triggers a storm, pun intended). 
The bee is fickle and diciplined. It perfers orders but makes random moves as well. Its behaviour is normalized. P can select B1 to B2 if B1 is more effective and produces toxins against b2 caterpillars. This is because the dynamics of P is comparable to B. Ps knowledge of photosynthesis is not representable nor transferrable. It is just a set of initial conditoins, that ‘develops’ dynamically in a given environment into stable forms. But B has stronger control because it can choose P1 over P2 easier than P can choose B2 to B2. This is because B has more internal states. It can order P1, P2.. Pn more easily than P could order B2, B2..Bn. The energy flow from B to P is more varied thanP to B. B->P is more erratic, say a B may not visit a field fo rmany more reasons than P turns toxic to B. B has more states allowing it to shfit between such states quicker making over the same dt, B as being less analyzable or orderable the P for B. The greter randomness allows it for greater experience, may be it feasts on dumpyards. The randomness allows it a greater count of stable states. Even if threse are partially ordered, the B can still view it from multiple internal states to increase dimensions of food source and thus order them. From more examples it ‘learns’ more information leading to more accurate recognition of order. 
P needs a randomizer and B in order ot randomize gets more examples from which it learns greter number of discrinimators. Thus Up is translated into more particular discrinators based on ordering also of normative context. Hence ina given state e1 of the environment E, it uses discrinimator d1 on a population of P. That is it selects for E thus for U and that is why P supplies energy to B, selection is an ordered process. 
The first law of thermodynamics predicts an energy gradient in the direction of greater order and hence nergy flows from P to B. But in case of P to H relationship the assymetry is greater. Upon creating order H either dissipates free energy or ought to look up for higher state machines. Thus intelligence might be objectively defined as the number of states in a machine that allows it to sample more and learn more. Up here is invariant and it allows to narrow down samples in the learning process. Up plus randomenss gives rise to ordered model of the world in n dimensions. The model M would have the ability to order the enviornment e plus energy source S reliably based on the weighted parameters of e and progression of S. Both could be dynamically smoothed stochastic models. Th eordering machien B would have to be capabel of scaling, translation abilities to map via general functions e and S to return an index ove ra set of S. e and S could be linearly compared. All formal ‘decisions’ are linear. Thus e and s could be vector spaces, where B ‘develops’ in (e+s) then that aspect is compelx and not an element of control theory. In defining CT we attempt stopping ‘development’ of B to future stages, because it may require an extension fo Es and existing bodily arrnagement is nostalgic. Thus higher intelligence is avoided by generalization in CT and by defining a culture that is unembodied.
If humans are the natural limit of embodiment is uncertain, but that is an arbitrary mystical desire
Part x
What might follow technically from the above is that for a CT to be effective, it has to have more internal states and greater randomness and hence dissipate more energy. It is like a heat pump. The heat pump however is a self improving one. It attempts to keep global stability by finding the lowest temperature possible of the recursive local order. Knowieng the minima however needs searhes. Hence CT the global heat pump attempts to improve itself. It therefore to achieve bootstrapped improvement, replicates itself and imprves the replica (by dividing bounaries /cross breeding/dispersion/nurture). The improvement is essential and the heat generated is used to improve in general.
Humans have however reached a stage wher the heat could be pumped by replicating abstract ideas embodied in symbols and using heat generated to build ‘useless’ structures – ie to dissipate it. Thus, in this non equlibriumic thermodynamic situation, the graiden tcreated would result in a dissipation. The abstract idea of say designing the best waterpump would only be aimed at building the best mansion in the town, which would eventually crumble, creating waste heat. If abstract ideas exist away from the development environment an dhence static couple with dissipative sturcures they ae likely overcome the need for a deeper minima in global stability by dynamics. If this barrier is final is unkown.
Next part
It infact is the development of Sense Eu that can directly sense universals  towards global equilbirum. Thus abstract knowledge is, hwether there is mechanical cause, diffusion, communication or even stimulation(by definition) is still abstract and attempts to discover universals. If it si normative, it is tied to the current cultural phase of humans. 
But development of distributed local solutoins, like self driving cars could not be represented in global formats. A brirds egg might contain the solution of the bug problem in a field, but bid popoulation even if they bring down global heat canbecome difficult to control va Es. Hence abstraction is relying on Es and Eu. 
Hence so far we say states are attractors. Every system interacts with its environment in a thermodynamically open way. The flow of energy is oposite to the direction of control. A machine transforms potential energy from one form to another, say a gasoline waterpump. From hydrocarbon potential energy it is translated to water at altitude. Thus machines always transfer between equipotential states. A machine cannot create a flow of free energy. Automated drivers can create such flow. <achiens are hence represented by equations and automatons by inequalities. 
A flow of free energy counters the first law. But where the cooler system is dissipateive, like sun->earth , equlibirum is in the very far future, so we may say first law is locally violated.
But an intersting thing happens durin dissipation, it develops turbulences because the sink locally (at atmospheric level) reaches a near equilibrium with earth. This graident gives rise all phenomena including life on earth. Hence atuomatons as heat pumps, if not matched by powerful sinks (culture) would create local structures and the flow o fnergy nearly stops and control (with lows opposite to energy) becomes hard. Hence automatons could be controlled and theres is no free lunch. 
Some automatos like lifestock are tameable because we have theories of them (because of their low frequncy of evolutionary change). If the new automatos do not present to senses for recusive theory (observaiton+universal = fact of theroy) we have no means, the automatons are just phenomena .

On problems of control – a theory of universal control
Lets first say that notions of state and transition is local and next that casuality is local. Former because particular states and their transitions could be combined in terms of temporal stability to produce dynamically stable states. Latter because we can explain the universe with just two causes, entropy and turbulence. Insisting on causes as contained in a suitable axiomatic theory is contextual and local.
From the axioms, we state the theory of control as follows. Let a system S has three states which are qualitatively enumerated as S1, S2 and S3. A quantiative componentization over n dimensions between the states sows that there are overalps on reading n params of the state of S coul dbe statistically estimated by an observer system C. (A fig of states being shown as overlapping hyperplane shapes).
Say, the temperature overlaps between S1 and S2. It is not often that the quantiative meaures are linearly combined. Latent dimenions as well as mappings between axes, making axes not truly orthogonal(or the system may be heterogeneous). It is the presence o f non linearlties that produce qualitative tates in the first place. To know a system is to be able to estimate its state to arbitrary precision. 
Fig of a tree structure laid out left to right. At the root we have a representation of the system. On the right we have steadily branching heirarchy of substates. We presume that states can transition along the tree structure, backtracking to higher level nodes to switch back to a second branch. Ie there is d-seperatoin. But in reality, there might exist relations of succeptible transfer across such d seperated nodes.
Each state might have locally stable microstates. A system S is as complex as there are dimensions or parameters. We can locally observe at S21 making a qualitative hypothesis that S211 and S212 are distinct and only possible states at an arbitrary P-Value say 0.99 and we ignore S11<->S211 making the model incomplete. Hence the prior (S21:{S211,S212}) prejudices the model of S.
A control problem is defined as moving a system from an iniital state to a target state. C has a recursive map and each state is defined as local affinities within a set of parameters. Hence S211 is a state which is possible by slight modification of params beginning at S21. It could bot be reached from S23 by changing the params alone which is characteristic of non linear systems. A control intervention is path dependent between the states. If there be unknowns pathways lke S211 to S11 then we an say that the system has unknown stable states. 
Events to state mappings might be as follows. Here the listing of ordered pairs of sets, the middle one be the known state and either end be unknowns.
<e5 , (su3)>,<{e1,e6,e7,e4},{S1,s2,S3} >, <{e2,e3},{Sui,Su2}> 

A system may hence vanish from the radar of C until it returns to a known state. The affinity of unkonwn states to known state I salso unkonwn. A system with unknown states if attempted to be controlled statistically between known states, then the alteration of parameters to the system has side efffects of making it more unkonwn. This process is an adaptive evolutionof S to control C such that energy flow S->C regresses (ist law of thermodynamics). This might even be seen as S ‘hiding’ its internal strategy. 
Where S sends feedback to C in order ot strengthen its predjudiced observation, it has control freatures. That is, it might actively against energy grdient maintain parameters indicative of S211 while it moves to S111. It would do tht only if control intervention C1 increases its free energy and C1 is stable in time to callow dedication of reserves to pretend being at S211.
C1 is only as stable as S has a model of C. Lets say S has a network model of C where it connects states by way of weighted edges. It can be built without local hypotheses. Whiel tree models leave out unknown states due to priors, this model say m2, state boundary definitions become hazy. A gradient descent can help identify states but it cmes with aproblem of high stochastiity. Tus, in m it is high certainty of n finitite states of S and in model m2, it is low certainty of m states of S where m>n. m1 has the same expresive power as me2. M2 maximizes statistical dependency to such an extent that m2 is Markovian (no past dependencies and transition being purely stochastic). M2 might be autoated while m1 neds arbitrary prior, altelast once at the very beginning. M1 involves halts to local searches for hypothesis proof which is then used as input in hypothesis formulation. Hypotheis generation is search in a discrete field based on application or priors to latent factors, due to which it coud not be formalized, followed by verification. M2 is continuous evolving based on estimation of state proven right by an external system (supervised) or by recurive classification. The transition c1->C2 is 2 times more frequent than c1->c3 is a conclusion that could be reached only where an orderd memory of past transitoins is maintained. Whether a system could construct a recursive state chart, mapping more general states to particular low level states is the statistical learning problem. If a structure like young fold mountains could be picturized (fig), it is possible it becomes a generic learning machine with a tree like structure. A network of this structure has a recursive pattern, ie distinct markovian subnetworks within higher order networks. For this to be autoencoded, the knowledge map would have ot be represented to a heirarchy fo processers, in terms of energy or which is a prior. 
This problem could be reframed as follows. Lets assume a heirarchy in an organization,
n->S->D->C
 say a government, where operatives work at circle district state and national level surveying the state of affairs. The district level office ahas a state map which the circle level office is unaware of and likewise heirarhially, the system works because of an nergy relationship (power) with the center. It is possible that a district officer reports the state of circles wrongly. It is simply less expensive approximation of the state of affairs. It fact ‘lies are simply approximations of truth’. There might exist a continuum (contrary to boolean logic) between what is true and false and between zero and one. [This is statistical truth, but where it is represented to be with a higher accuracy, it becomes a lie edit]. But the central contorl on ‘sensing’ approximation function being more economical than energy flow, either cuts the energy flow or reverses it. Thus penaly to budget cuts are etrimental to circle C, because it destorys local structures. Local structures at D level are held taut by the unifromity of dne/dt of energy flow. It svariation cuses ireversible state changes, How S level identifiies higher approximation at a particualr D instance is by smoothness checks. If all D surrounding Di has acertain state, then the state can audit or apply speical powers to resolve the state at C level, so a sto destroy the equilibrium of D and its associated truth approximation functions. 
This requires D ot be be an outlying defector or the general flow between D and S to be near equilibrium. Secondly level C should be representible in a form that is recurise, so that S can compute C, directly. Hence, organizations emerge around power gradients and where all levels are in local equilibrium. 
Energy flow sfrom D to S, but D exists by virtue of there bieng similar instances of D that are too close statewise. This actually allows S to discriminate between Ds arbitrarily. Ds could be seen as having high stable structures, because there are many instances that back each other up. High stability means, less unknown states because the system would need only known states as majority that it does not linger to unknown states. 
S is powerful because it has more unknown states from D. S even if it could not compute C could still be powerful, because D is stable and resilient (due to many slightly variant options). S can control because it creates a dissipation gradient. A random (not neighborhood alone) search as part of dissipation, allows noting similarites between D types, thereby stabilizing knowledge through more examples. It could be local reserve before dissipation that is doing the work. Disspation is pure and only supplies the nergy vacuum that causes energy to flow. Utilizing the free energy just before dissipation could sponser searches in an abductive sense. (in some kind of a turbulent pool, fig)
Hence, progress might be linear. Let CT be a control system that is at the dissipative head. The question is that whether it can remain stable. It naturally replicates and statbilizes itself enough. If a CT could dissipate at critical difference points, while being coherent on others, unlike evolution, which backtracks to the point of variation (birth) and destroys it, it could effect equilibrium positions that are virtual (wolf packs, human organizations). Virtual organizations can grow heirarchially and dissipate energy exerting control over individuals. But stability is not the same as justice. Hence, the backflow where the people try to troot ideological variety to sensorially appreciable states creates a closed loop. This involves recursive local modelling as in m1. It hence creates a loop, 
Virtual organization (V) (does wide searhes say vector w) <- CT (dissipates as universals and art/moral, say vector a) <- S. There is a flow of control from Ct to S and a flow of sensorial meaning from V to Ct. We might say that where |a|>|w| the system is stable. Ct also refers to universal states to be able to exert ulitmate control over whatever stable states that V can take. Searches in the universal space also makes V more understandable and controllable. If a cyclic could be established within U ergodically (?) the system can be stabel indefinitely. Where U is the universal to which Ct reaches via V.
The law of universals might be stated as that most phenomena (except for energy maybe), are just local or transcient in a symmetric knowledge space. They could hence be discerned and controlled in principle. The limits of universal control are free energy and uncertinty. F the latter we might say that the finitude of sensorial perception, necessarily limits our body and perhas community. Therefore fast changing (or high dimensional dynamics) could not be entrusted. Large knowledge stores might become unstble. Universals could not be representedin finite space/time. An indefinite cycle of rediscovering universals might be sustainable. 
Local maps of universal order can guide us to control any arbitrary system relying only on equipotent machines and senses. Use of sensorial extension progresses evolution and irreversible. Using local maps of universal order and senses and machines, in principle help in universal control in a reversible way, ie it can relax after the problem is solved. But it would also require a distinction between problematic and solution state. The dissipation in art can be a sink that avoids too much energy building up to mandate irreversibel growth by sensorial augmentation. 
PS
We think hypothesis generation is structural approach to knowledge I sbased on local maps of the universal. Dissipation by art is important as a bufer because energy could not be pushed backward in a reversible manner. By unadided senses we permit augmentation using material embodiment of universal order maps- say linear scalar like telescopes. Hence machines are generally energywise closed just as the conservational universal order local maps are.








Curiosity: A theory of intelligent organization

Building on the bulk of our past work, we can reason now on a general thoery of inteligent organizaiton. We might think of organizations arising from the flow of energy. We have seen that energy dissipation at one end and the ability for local problem solving at another end power the stable organization. That is the control by arbitrary dissipation and far fetched searches occupy the head of the organized entity (Ct), while at the lower end, there are multiple low end entities (S) which are only slightly different (allowing a smooth distribution, thereby allowing learning over the lower end entities possible for the controller). Ct controls S by being aware of their states, having learned about them, from the smoothly distributed examples. It also dissipates and assumes arbitray states as to be incapable of being reached by instances of S. This system is stable. 
Let us know look at the way human systems are organized. We might say that in human organizations, there is an arrangement of power such that, at the head, the state works by the word of law. That is to say, the literal nature of law, being expressed in a language is unique to humans. As with Logical positivism and theories of Chomsky Language is an amazing thing. Even now the best computers could not learn languages. Languages allows problems to well posed, so that they are not nonsensical. This allows the head that rules by the word of law, by being linguistic, to rule by defining clear and well structured constraints on the lower end S instances. This linguistic dissemination of instructions, causes the system to exert a dampening control over the otherwise dynamically enthusiastic nature of human subjects. They seek out innovation and problem solving at the worlds ends, but the system constrains them to stick to legal means and as it happens there are many possible ways to heorism even within the legal framework. Thus, we might say, suffering is minimized by the entity that minimizes physical suffering. Psychological suffering, often appreciated in philsophical and cosmological theory, could be traced back quite strongly to physical suffering. The general notion is that the speed of replication of an entity is so fast that it inevitably strips out the resources available, hence scarcity, conflict and suffering is inevitable. Hence, philosophy often deals with ways to psychologically accept and console ourselves. But we might see that in a demographic transition, populations stabilize, likewise, even in lower creatures there is a steady preference for nurture and controlled replication as against a blind fight at a scarce resource base. Hence, it appears that physical suffering is not inevitable. If an entity could construct something that could allow an exploration of possible exploitable fields, in order to accommodate a replication plan, methodically, then in such entities, physical suffering is minimized as well as the psychological suffering. This is a materialist theory of suffering. 
This would invovle, hence an exploration of the possibility of being able to define the extent and degree of dynamical exploration and this is the basis of civil governance. By the notion of law, it is possible to define the boundaries of what we might call a game. That is to say, it is possible in humans to define such boundaries as to allow dynamical methods that could suggest the primacy of one of the pathways to successful tackling of a given problem, which would mean more free resources and greater replication. We had already discussed that the means humans use to solve this kind of problems is one where there is a preference for equipotent machines, that is to say, it creates opportunities in which nature could have been the way it set after the engineering exercise. This is more loosely be said to be rewriting ones fortunes. In more rigourous way, we can say, it a reconstitution of the initial conditions to a dynamical system. This is unimanginable for such developing systems as the biological system, it is impossible to revisit and reset the initial conditions. But it infact works for social systems. That is to say, lets assume a stone age population, which would need to visit a watering hole for fetching water for the cave community. Normally, it is done by a martial group, which braves the lions and other wild things to get back water in bulk, say once a week. This martial group is celebrated and this leads to the evolution of mythologies and power structures, where the martial group gets to dissipate energy and then perform heorics. This leads to the formation of rivalling martial groups and a strict heirarchy of defensive nobility who attempt to settle things by war and their subjects are locked as part of this conflict. But imagine if one guy suggests to dig a well and use the dug out mud as ramparts. This makes the natural layout of the terrain into another possible one, where it appears more or less by chance, that there is a great undrying waterhole very near to their residences, well protected from raiding creatures. This is like resetting the initial conditions and it frustrates history and evolution.
All the evolutionarily stable systems work by way of the power equilibrium we have discussed, but human systems work by symmetric sharing of intelligence. That is to say, while the formalization of law at the state citadel is based on the notion of justice, or peace and this is what creates the possibility of games. The games hence arise from the legal framework to it, such that everyone has a starting point and they set out to solve the problem. The solution would then lead to a resetting of initial conditions, that is to say, they feedback to the configuration of the game itself. This game might be recursively organized, but the spirit is that the game is something in which evolution progresses only to flow back to reconfigure the game. On the other side of the symmetry, we see that the people get to give feedback by arbitrary choice, that is say, by voting. The presence of a rivalling political theory, that thinks of justice differently, allows the people to dampen the progression of the deliverance of law by arbitrary voting, driven by sentimental means. Thus, the purity of sentimental feeling allows a genuine and symmetric dampening to the political heads, possibility of delivering a well phrased linguistic rule system, a theory that could be recursively axiomatic, but traces back to the inexplorable axiom of say a moral sense of justice, symmetry and peace. Likewise the sentimental exercise of control over the policy entities, happen by a sense of culture, aesthetics and critical morality, that is likewise intractable. Hence the symmetry holds because of the innate roots of priors to either side of the formulation of dampeners, so ubiquitous in control theory. The system can hence be stable without growing. The pillars of enlightenment deal with these ramparts, by providing for a way in which law could be very much in the letter, by the notion of rule of law. The rule law allows for law to exist independent of context and hence very literal. The second one is the universal public instruction, which allows people to be good citizens and thus be able to formulate games in the civil society. In a civil society, the games never extend back beyond their boundaries. Thus a person might trade in a market, without fear of being killed by the mafia in well set civil society and this possibility is what minimizes suffering. The third one is democracy which details the possibility of a purely sentimental feedback dampener. The fourth one that is the scientific method is very interesting, in that it separates the engineering from power. It is possible to propose logically (thus linguistically) consistent theories and follow them up only by trusting ones senses and not any power based ratifications and context to promote knowledge and thus feedback towards game reconfiguration. Thus, the method of science is a pillar of enlightenment as well. One thing that fell out of the way as the world entered enlightenment from the renaissance phase is the consideration of cultural vibrancy. The romanticism and encouragement of arts completes the balance, that backs up democracy strongly, but its ignorance in the theory lead to the imperialistic adaptation of the enlightenment principle. This also because the culture did not provide the philosophical framework that allowed recognition of the principles of enlightenment not as something that was achieved by technological refinement, but more as a revelation that happens from time to time to humanity, and adapted locally to cultures, due to the benevolence of the universe. 
Thus, human or intelligent systems work because they are able to stabilize over the language and pure inexpressible sentiments. It might be said, that the civil stability arises when fundamentally free creatures, agree on something that could be written up. Thus, while the individual endeavour is total freedom, they had paradoxically, for the love of peace and justice come together to constrain this in law and to seal the contract by a cultural tradition. This is the miracle of the universe. If one could formalize this arrangmement in a  general theory involving intelligent entities, is an interesting question. We say that the involvemen of mystical priors makes it difficult. People seek justice and their language is based on such priors (chomsky), likewise sentiments of empathy and love are also indescribable and could emotivisim in Logical positivism. They could not hence be encoded into machines ( or any language). This non encodable entities allow the stability to persist. If we are to program machines to work in a certain way, we either rely on their inertia or its variant, energy efficiency. A machine is inertial, it is equipotent,because, it never does anything that is energywise paradoxical, thus, it never locks onto an inclined plane and never slips over a meshed wheel arrangement. Likewise, if we place a machine to solve problems, we see that it always locks onto a goal. This goal in generic terms is the minimization of energy. Thus, the machine attempts to solve problems in the least energy pathway, creating power relationships as the framework of stability. The power relationships lead to evolving systems that stable, but only dynamically. 
If we are to imagine what goes into intelligence, we might imagine a chimp placed in a cage and given only a stick. We see in evolution, intelligence emerged where the creature that preceeded humans was given peace of mind, it had an arboreal habitat, it was an omnivore. This allowed the creature to not to think too much about predators and have a food surplus. The often overlooked part of intelligence is the hand. People often reverse the reaonsing as saying bigger brains allowed intelligence. The chimps free hands were instrumental to intelligence. It would cause the chimp to look at the stick and rotate it various angles and hold, drop it and press it to explore it. Thus, it genertes a great deal of examples from which to learn. It learns about the stick for no particular reason than curosity. This curiousity, being independent of energy goals sets humans apart. They wonder and they look at the stick in different ways and develop a theory of the stick, removed from the energy bound nature of the world and place it an abstract world (in the abstract, energy does not matter, how casually do we drop infinity symbols in math). Thus, the development of the mind, relied on curiosity. We might not say, that it developed by a historical accident, because the homonid evolved multiple times. We might thus say, by insulating a computing system (thus mimicking an arboreal, omnivore habitat) and by allowing free computational resources, we allow the chimp to evolve in the machine. We had also discussed that the prior might be tightly bound to the body (in the mind body) and could even be trace to some peculiar symmetry in the core chemical molecules, like say some carbon bonds that manifest as justice, peace, aesthetics, morality and even curiosity. These priors could not be easily encoded into the machines. They may develop from an energy perspecitve (they are just dynamically inertial, just like the classical inertia in newtonian theory, making them machines of a different degree). But what sets the free creature, the free willed non machine apart is the curiousity, the ability to engage in explorative behaviour for no reason whatsoever, is the genius of man. Conversely we might see the genius, in refraining from actions where energy efficiency could be attained by doing otherwise (cite pathinaru periya karuppan).  A man could simply sit on a fence and do nothing and one would respect the man. 
A machine in a glass cage, would not consider picking up the stick and examining it. Perhaps it could, if we set it up, to the priors that are symmetrical. The machine might perhaps even start playing games, which it does not now. Humans can only play games, they can loose and win within rules (even rules of chivalry) and hold the word over the possibility of survival. These things are in their body and the billions of years of evolution. We could not possibly recreate these in our labs, yet. We might however pretty well create a primordial slime, that could evolve dynamically in an unbounded manner and quickly suck humans off their winds. Such slime might perhaps evolve into human like cretures one day. But beyond that beyond the possibility of free will, in an universe so determined by energy, it is difficult to imagine a phenomenon more sublime.

PS: we have discussed this symmetry to control by sentimental and logical means, in our work on The oracle gratis, where we deal with the dualism of bureacucray-popular vote, corporate governance in business enterprises – markets as in a yin yang relationship. This synthesis is the intelligent organization, which essentially attempts to continually refine the game, restting the initial conditions, as against the emergent organization which is evolutionary, never revisits the past, seeks only development and a dynamical equilibrium and prone to chaotic crashes, when the resources run out. While we are able to figure this theory as an abstraction of the elightenment movement, we also seek to complete enlightenment by adding the notion of culture to it. This allows a strengthening of democracy to such an extent that the energy independence in science and law is complemented by art and spiritualism. The possibility of the stable civil society is however not complete though. It is still possible for such entities to flounder where the leadership constructs mythologies of a common enemy like red menace (survivalist) or the white mans burden (emancipatory) . This leads to the situation where the world would need an additional dose of institutionalization of this principle itself as a metanarrative (which itself is not immune to floundering, but we just hope). This institutionalization of the metanarrative of a social stability around trancedental enterprise (in art, pure math) and its ability to achieve stability by logical axiomatic systems like science and law and to ring it as propoganda could help stabilize the system, as an additional rampart. This is the entity of the public sphere so dealt with in the frankfurt school of critical theory. 







2
Analytical Essays

Working with the do-calculus

A) A complexity model of computing 
From there we attempt to build upon a model of computing, where we can formalize the system with respect to its normative system.
1) Turing Model of computing.
Our idea would be to see the way in which computers are organized with respect to the system that they are created in, or embedded in. The system utilizing the computer, mostly uses it to simulate a machine or a market or a deliberative individual. That is to say if a system of business or any goal directed, normative organization involves the computer amidst it in order that it satisfies the missing link in the organization of individuals. An individual with a deliberative sense, a market or an organization needs to decide, between nontrivial choices at any point of time. These decisions are expected to be made by consensus (or if an individual, conscientous) if it is consistent and bias free. This is achieved by the development of a system which reduces the environment into a model, where problems could be posed to the system and decisions be obtained, in a simulated mode, in the interests of the stability of the system as a whole, at lower cost than negotiation over social niceties or be controlled by a central, sovereign organization. 
The computer is thus a simulator of the environment. It would hence adjudicate questions posed to it and provide its decisions. No machine can solve questions not posed to it. Any machine can answer questions that are posed most properly. It decides on questions based on previous agreements, which form its axioms. It hence decides on logical formulae based on tautological premises. Each decision however involves evaluating truths based on conditions. These conditions arise from having to evaluate particular environmental signals conveyed as numbers or signals, interpreted by individuals so that it might be referred against a standard lexicological tree (such as code tables) to take the program forward on a given path, till it halts with an accept or reject. 
Humans interpret events from the environment in according to the user interfaces. The program halts till the input is provided.  Where sensors are used to collect data it might loop. There are combinatorially many paths making the system difficult of proof but easy to verify. It could hence be said of the system that it might not even be deterministic, since reproducing the pathway in complex algorithms in production may not be possible. 
Hence, the system is reductive, but non provable. It is consistent to a good extent and hence transactions could happen in a human world so as to serve as a stabilizing consistency checker of human actions. Where human actions are not consistent, or a proposition for an 'action' is made, the sysetem either allows or denies it. 
Hence, it is important to understand that the system plays the role of a permitter/denier of actions, rather than solving intellectual questions.

2) Post Turing Machines.
a) Computers that simulate markets
It is possible to make a network of computers united over a common lexicon and discrete code tables could be programmed with regard to local truths which are expected to be determined adversarially over a network. This is a simple extension of the Turing model. The machine becomes capable of emergent effects such as 'manufacturing consent'. In this case the machine simulates a planar graph, wherein it simulates a rational authority in the system or multiple systems connecting over a network.
There might be indefinite waits on service calls. On the other hand, if the graph is to be made non planar, in the sense, that a certain truth at an earlier point in time, 'arcs' across and influences a later state and likewise where paralell pathways have connections in between them, then the signal processed of the environment, simulates the user events as well. That is to say, if a closed loop simulation is planned, it would require mimicking a free willed agents actions as well. These actions could be simulated by two elements, one is a random generator and secondly a normalizer. 
That is to say, it is more like a MCMC algorithm, where random moves are generated (say a bid is made) and based on a statistical distribution model (by plotting the accepted bids, the system might construct it in memory). Further reejections and acceptance might be made based on such normal distribution. This makes the agent a reflective agent. It tries to reap a surplus by hiding its internal state and pushing towards greater optimization within its context. A cooperative strategy might emerge from this game, leading to a situation where a set of systems might cooperate without the need for common code tables (discrete parameters which identify thresholds of continuous values). However a lexicon is required nevertheless.
Thus intelligent agents might be an extension of a traditional computer with stochastic features that could normalize on the outcomes. But it is also required that such agents could not be fully automated, because of the need for a prior. An uninformative prior or a blind search might not produce an effective system. Hence each service might have its programmers, who program certain outcomes expected wherefrom evidence is observed and a posterior is constructed, by MCMC like methods in one go. Hence, the supplication of intents are important for a computer that simulates markets.

b) Computers that simulate a deliberative agent.
Where a non planar graph model is used, it might involve an extended high dimensional reasoning. For instance, it might construct that the outcome of a certain shipment is dependent not only on the immediate causes, but also from distant causes. For instance, political turmoil in a region might dictate outcomes. If one heuristically knows that the system relies on certain knowledge which is drawn from a limited sample, then he is likely to generate events (such as risk factor) that account for this. Hence, an elaborate network might generate pathways that involve noise filteration. Specific decisions might be cliques which are bijective (such as an eligibility for a loan). The pathways that are traversed to get to either of these mutually exclusive cliques are finite and could be derived from noise filtering over specific pathways, using MCMC like local searches. 

B) Normative Intent of Extended Computing
The normative intent of these extended computing is to be able to simulate extended outcomes and make quotes or bids. That is to say, a system that is intelligent, in that it could answer incomplete questions (by supplying a normal context) might be able to make heuristic judgement, which might lift the burden of human actors. Thus these might be recommenders. Recommenders run a simulation, generating stochastically events from possible points of influence of a system and predict the possible pathway to reach a normative unambiguous point. Based on the pathway that is expected to be followed, suitable preparatory actions are recommended with respect to the initial state, either to maximize desired outcomes or minimize adverse outcomes. 
In the long run, there is only a thin line between decisions and recommendations. If complex decisions are simulated on a computer, the system on the whole is expected to self organize quickly over bots that are high frequency, low cost searches for equilibrium. These equilibria are deterministic. In general human agents are biased, they suspect they can do better than average, that is why people cannot accept a normalization by any agency and would rather like to self determine. 
Hence, a sufficiently intelligent system might simulate its handler as well. It might hence sustain a system autonomously, say we would wait for buses without being able to do anything about it. It then becomes a phenomenon, that outwits observation. There are no ways to nudge or govern the system. This results in long run regression of the system or a historical divergence where the system might incur transaction costs to search for efficient solutions adversarially. Both these situations are uncomfortable.

C) Extending Regular Models to extended models
1) A model high dimensional computing architecture
 We explore the idea of being able to design solutions for the regular business house.
- Firstly, we compose immutable entities. In such a case, every entity would have to have  an initial state from where it starts evolving. There are no updates or deletions, but the specific parts of the entity, say address of a customer keeps changing as if  adding nodes to a graph.
- This allows for not only entities, but also transcient items such as orders to go through processes of creation to closure and forever remain in history. 
- These immutable entities correspond to specific intuitive entities, such as documents that go into a file and remain there for ever (unless archived or specifically destroyed). 
- Every action would involve adding something into a registry or reading from the registry. 
- Thus there might be entities evolving and being linked by reference to other entities. Thus, a document of an order might contain reference to a shipper and a delivery address via indices. 
- It may point to the head of the referred evolutionary path, or might refer to a chronologically dictated connection. Relational databases are efficient in that an automatic pointing to the head is built in, by referential constraints. In older document based databases, this might not be the case. The link is a hardlink, or it may even be directly copied insitu instead of holding references. 
- In fast evolving systems, this may present problems, because there is needed of a hidden index with the timestamp which versions the the branch (such as with git). 
- All branches must however refer to an objective narrative, which lists the codes in use, the identities of actors (whose attributes might change) and a lexicon of abbreviations. Assets might be transcient, but real accounts are to be endured. 
- Where the evolving system branches, say a new branch is added to a vendor, then automated 'pulls' of RDBMS can have problems. 
-If we examine the advantages of this approach, we might see that the ouput graph might be used for a variety of analytical actions, where history is important.
- The graph is planar where we work with realtime transaction processing. The branching is decided by immediate 'events' which signify human processed high dimensional signals. Where such events are not to be the core of the state machine design, such as when automated simulation is preferred, the graph would have to become non planar. 
- There might be higher dimensions to decision making, employed while interpreting environmental events by human agents, as a heuristic excercise.

2) The incremental development of systems around evolutionary pathways, avoid the problem of combinatorial explosion and is capable of deciding questions that are high dimensional. 


Part2


1. Computing ought to be viewed about a boundary. This boundary seperates problems that are natural and that which are synthetic. Synthetic problems arise from human processes solving natural problems. Natural problems are the non linearities in nature. Say a process P in nature could be described by a consistent model M, to a certain degree of completeness. The decisions made using M, would nudge P to dynamically evolve in a certain direction, which is almost deterministic, though from the point of view of P, it is a dynamic evolution. The model M would then have a complementary part M' which would interrupt P and reset it. This interruption would result in surpluses. A clear example could be made out of farming, by performing certain rule bound tasks, waiting, harvesting and repeating. 
Thus, this works as a heat pump, which pumps surplus energy from an order creating system which is not intelligent enough to identify its manipulation, such as with crops or farm animals. There might be a long term fall in marginal utilities or a central regression, however.
Synthetic problems arise from the surplus energy generated by the heat pump. This would need dissipative pumps which would have to make wasteful movements to dissipate the heat. That it is to say, there gets developed complex pathways to negotiate simple dilemmas with respect to reconcillations of liberty and equality. That is to say, how does a person seek to do what is best for him and at the same time expect to compromize with possible conflict with other persons wishes. 
The numerous constructs of organization and politics involve rituals which dissipate energy. These rituals might be a democractic election every five years. The different interest groups are abstracted with an incomplete model and studied in their dynamics, till at decisive point the direction becomes apparant, such as a leftist or rightist swing and appropriate reconstitution of the government takes place. This is an oscillatory movement much like in agriculture. 

2. The role of computing in this setting is the development of an incomplete model which would guide the dynamics to specific targets ready for a second discrete intervention. This happens both in the case of synthetic as well as natural problems. Computing simulates natural problems in order to produce this effect and it could be seen as a continuation of experimental techniques. Likewise in synthetic problems, one might see that there had always been techniques that defined rituals that allowed for ritualistic wasteful movements. Computing by defining less consistent but more complete systems than before allowed for these rituals to happen over higher frequencies, because the dynamics could be computed in narrower time ranges and could be quickly harvested. 
The computing system is a purely self referential system that is placed amidst the society. The presence of this system allows for any arbitrary expression by any agent to be validated for consistency, that is to say, different ways of putting the point that one is entitled to a greater liberty than the neighbor is simplified and validated by the presence of the computing system. This allows for a lesser need to construct dialectic political parties and submission to an internal control system that reductively models, dynamically tracks and decisively resets the system. 


3. The idea itself however seems to be to minimize the use of these dissipative pumps and build orderliness. In more formal terms, it is the role of science to present moral dilemmas. That is to say, science should try to explain by cold reasoning many of the happenings of the universe, so that  a scarce few is left for being referred for moral reasoning. Hence, in this case, we see that it is the duty of science to appreciate the process of computing as reflecting from a general pursuit of evolution, as seeing the agenda of living systems and intelligence as distinct from free willed agents, who would then be presented with a moral dilemma at the end of the scientific apprisal. To get back on the scientific appraisal, every species seeks to attain the highest energy steady state possible. The mosquito develops as much intelligence as would allow it to avoid the most wily swatters, by appropriate reaction. The mosquito had at all times wished to seal the need for dissipation of free energy or the need to explore new forms by stabilizing at a high energy level. But the complex graph of mosquitoes caused several paralell branches at equal levels of sophistication and even some low level creatures being able to draw arcs to defect and continue the evolutionary path, allowing them to stabilize at not the highest possible energy level, but below it.

4. The Ultimate human question might be to find a way where no experimentation might be required. That is to say, all questions had been answered and no further reaping of free energy is required. If the ultimate aim of a species to solve problems in nature, then its pinnacle would be to solve it theoretically and experiment no further. But if reality is not static, rather it carries wave like oscillations by an actor that woudl eventually oscillate through everything. But we think reality is static at this point. Humans might hence seek to attain high energy stability conditions. 

5. A higher integration of this sort happened in multicellularity. But multicellularity is accompanied by a macabre reset, where the grand castle of billions of occupants is brought down by a timed device, only to redraw it completely by reproduction. These billion occupants do have their individual consciousness, but it had resulted in a higher collective consciousness in the CNS. Similar half complete experiments can be seen in eusocial creatures, where a hive mind is existant. But even here, the entire hive collapses at some point and one queen leaves (much like a single gamete selected from the billions of somatic cells going on to restart multicellularity). The redrawl effectively nullifies history and allows the system to add pure stochasticity to its form. The resulting experiment on the environment is hence pure random search, which leads to a redrawn creature in the new environment. The tragedy of life is the need for a reset. All consolidation of intelligence, would eventually lead to the ability for a single clean reset, with an encapsulation in a highly compressed format the whole of the grand enterprise. In simple creatures where such compressed transportation is trivially possible by spores or where cells themselves could be replicated in the vicinity as with algae, there is no grand construction and collapses. However, the collapses themsleves align to the possiblity of a natural collapse due to crowding, invasions by microbes or by simple accidental uncontained local failures.
In another incomplete consolidation, as with niches, say we see that a group of creatures such as bovines, predators and their group form the savannah niche. They explicitly try to increase the range of savannahs by bringing down trees at the edge of the forest, by elephants doing so. They seem to be working as a team with a collective consciousness. There is a recession of consciousness of the individual creatures as they play to their instincts. 
A similar projection might be made of the use of computing on synthetic problems. As natural problems get solved by simulation methods, the productivity gains increase and the use of these by incorporating them into solutions leads to the evolution of the system in strange ways. Say a bus service becomes completely self managing by dealing with fuel suppliers, driving itself, connecting with vendors and suppliers and the entire transportaiton in a metro had been automated. People no longer care about the software patches and control systems. Say over ten years, the systems marginal utlity declines and humans would want to intervene. They would likely find that the progression had been much more, the bus company had actually become a law and order partner, became a major supplier of scrap for defense purposes, making the problem of shutting down the system unpredictable. Hence, the system is effectively free of human control. The subsequent developments would lead to a hive mind like integration. 
But as more and more the integration comes closer to fruition, there would become a need for clean resets. Thus, there would likely emerge a single super computer which would eventually shut down the processes and collapse the system, and then start spawning the process again. Just like individual cells inside multicellular organisms left to starve after the collapse, the individuals would become subjected. 

6. Just as science sees this as a repeating course of the 'phenomenon' of life itself, it also observes that there does exist alternate paths to this seeming determinism. There might arise a certain poing when the agents might start becoming attached to wasteful rituals, as in culture, nations and celebrations and shun computational integration. This had infact been an exposition on the 'phenomenaology of integrated living beings with emergent consciousness' . We also might stand corrected with respect to our previous expostion on 'why no singularity'. But the coutercurrent could be observed (perhaps assisted by fermis paradox) that people infact try to maintain stasis by dissipating energy and without attempting a clean reset and integration pathway. They might in fact attempt to replicate a phenomenon within life itself, attempting terraforming and universal spread without necessarily working vertically. They might leverage on their curiosity and love for liberty and experimentation and value it more than the need for vertical consolidation in singularity. They might prefer variety and deferring away to different directions, hoping for a deferred unity on homecoming, rather than staying put and building elaborate bunkers. But that is the moral dilemma we promised.

Part 3 - Leisure and Defragmentation
It is witnessed that activism arises from some form of outrage. The outrage arises from a perception of the world to have been flawed and below in the standard of constitution of human societies. Hence, most outrages are directed at units of human organizations, which suffer from complexes which are strange and repulsive for the activist. He should hence be moved to avenge this unit of human organization. It is a reflection of how the unit, be it a family as with the Arab joint families, communities or entire countries that motivates him. It is the destruction of the tendency for humans to group together as a natural force and attempt to build a higher level consciousness and substitute it with a grouping over a political consciousness, with the elements of planning, control and above all leisure. 
That is to say, the constant pressure to work as a group while retaining conscious control over the dynamics of the group is the priority of the activist. For this end, he pursues two important actions. Firstly the development of a knowledge of highly consistent forms and secondly on the provisioning of the boundaries of completeness of the said knowledge so that freedom exists in the group to do non impactful activities. Thus, there is required of leisure to be provided for and insulated from made gainful. This repeated reinforcement of the need for consistent platforms to underlie pursuit of reward based actions and its seperation from the society at large is an important way to prevent evolution from inventing a human cohesiveness that is like the tower of Babel experiment. (cite the soft power complex).
The preservation of human agency or liberty to pursue independent spiritual romantic quests while being embedded in a group, capable of being made sense of in political terms presents an ideal which requires the selection of dedicated contributors to this end. Hence activism is a recruitment of people who can do things that does not add to their survivalist needs proportionately. There is a need for a leisure class, unmotivated by hedonism, but rather moved by stoic concerns to create a society with less of unbounded dynamics and more of well appointed dynamic space capable of political interpretations. 
That is to say, a constant flux and dynamic of economic innovation, cross border capital flow, monetary synthetic manipulations, market relationships coupling itself to gender relationships, household generational relationships, religion and institutions of tradition and customs, public spaces and power relationships cause exceptionally complex dynamics, such as uprooting from communities, emigration of family members, traditional roles being interpreted as economic oppression and their challenge through economic market egalitarianism. This we might summarize as fragmentation. 
This fragmentation is caused by a need to pursue greater orderliness than that is local and organized recursively (d-seperated). The more dynamic and shifting the relationships, the greater their need for being tracked by symbolic and quantative pointers. This could then be reintegrated to produce model organizations. This might be in the interest of lower transaction cost to decisions that concern larger wholes, at the cost of local autonomy. In the long run, this produces an integration into a newer consciousness for which there is no pointers from human actors.
The fragmentation is a subject of distress, which however is not quantifiable due to its difficulty of being expressed. People do not become less productive becuase of fragmentation, they suffer nevertheless in silence. The move of the activist is hence to stop the fragmentation by limiting and concretizing notions of modernity so as to allow for a leisurely and voluntary take on the problem, dispersing the forces of ruthless intractable integration, powered by evolution and replacing it with a pluralistic force, which refuses integration and dissipates the excess heat generated by engineering prowess either to solve problems delinked from utility or in the practice of arts.

Hence, activism always concerns itself with acting as  a group of leisure pursuers who identify a specific community or an abstract community to induce changes. These changes might be policy regulation/control or as provisioning of common resources to produce buildings, spaces and free instructions to all. Hence, these actions might be prescriptive, welfare or educative. The motive is redemptive and the medium is formal publication with openness to crticism.
This might be seen as morality in action. Morality concerns with not just principles for stability, but also those for preservation of innate human consciousness. Amorality as in postmodernity might be an antidote to this. Moral sentiments purport people to seperate actions from circumstances
pyschological impact:
In psychological terms, we might percieve that fragmentation, disorients the conginitive faculties into construction of network models of the world which are unstable, leading to volatility resulting from small changes. This instability results in synthetic models where the edges are discredited (where paranaoid delusions might mark cognition) or where nodes are discounted (where a pervasive fatalistic model might influence mood). The resulting adjustment might be permanent and influence command signals to alter it, thereby blocking off further correction by external faculties, or it might be continually oscillating and volatile perception leading to affective volatility and cognitive distortions.









Supplementing RuleBased Expert system with Connectionist modules in MDM.
========
Consistency in causal modelling:
Rule based systems allow for parametric models, which can be scaled and transformed, such as models of engines where cc, drive wheels could be parameters and one can predict the torque on the rear wheel for a range of paramtric combinations.
Where a Turing machine is incorporated into the mechanism, there are two areas where things get complex, the while loops allow for potentially unending programmatic execution, where user events (such as tyre slippage to engage 4wd) are awaited. Sometimes the infinite loops make the program a service and is enabled as a feature. The presence of a mechanism that has no final macroscopic state transformation (though microscopic internal loops might exist) might not be deterministic. Hence consistency is compromized. The second point is the caliberation of events. Some of the events might be completely random (where user makes entries to a screen) or might rely on sensor digital signal transformation thresholds. The cutoff of continuous data from the environment might lead to inconsistencies in the system. 
Thus, inferring qualitative truths from quantative data passes through arbitrary filters (rationalized by versioning), binds the system to the context.
Dynamic extension of the system (rather than static self contained modelling) becomes possible due to indefinite waits for events. Coevolution and reflexivity disturbs consistency, indefinite movement defies snapshotting for reproducible state.

Apart from this one might also consider:
The reasoning process of FSM might be non markovian, in that it might not be entirely stack based and may write to external memory of its history.
Syllogistic reasoning over definite sets, expecting certain events while excluding others. Causality assumes distinct pathways to attain certain ends. Thus, torque might be increased by gearing or increasing fuel or fuel mixing. Each of these paths have independent subpathways which have no relevance for deciding an end state (high vs low torque). These strategies are additive. There might be independent causations for the same effect in which orthogonal strategies might be used. Say the decision of whether a customer becomes loan eligible might be due to domiciliary or employability pathways both of which might be necessary and sufficient in their own branches. Privy to each branches there might be subeligibility filters, such as domiciliary verification outcome event being handled in one branch while not in another. An ontology of closed exclusive sets, produce combinations that make inferring the algorithm inductively difficult.
A consistent system might be modelled causally. Causality assumes a realism and singularity of knowledge which 'is'. From an observation of the objective, one could construct subjective, applied normative perceptions in line with Cartesian Dualism. Phenomena are dynamic changes to a system, constrained by its parameters but not motivated by them. Thus photosynthesis ought not be there, but if be there, the equation is always balanced in molarity. Only by hypothesis and experimentation are phenomena exposed. The theory defines their bounds. There are too few phenomena that exist than the combinations of theoretical parameters might permit. These are explained by initial conditions. Thus, we assume a history (or a continuity from initial conditions) wherein sets have not come to existence all at once, but phenomena connect certain subsets in ways that is arbitrary. 
Phenomena do exist and perceptible to the senses. They might be interpreted in symbols and shapes such as with the periodical table. Non linear behaviour, discontinuities (as with rows of periodic table) cause phenomena. The question is whether phenomena be considered as special cases of theoretical interactions or they should be studied phenomenologically, where the observer is active and in which case it could be contested if phenomena do exist. It might as well have been constructed constrained by sensorial peculiarities of humans, temporal spans of observation of frequencies or they may be deemed to exist in as much as they are normatively interesting. The truth might be ambivalent between the two points. Some phenomena had been studied such as photosynthesis because of their utility, while numerous phenomena might lie latent.
Likewise humans are interested in avoiding accidents. Hence, they construct circumstances in which accidents materialize. Thus, it works not on a causal model, but on designating a network where certain observations/actions percurse or increase the risk of certain trajectories. Coexistence of multiple risk factors (while being dealt as additive in quantative risk scoring) might be seen as the state space of the dynamically evolving system. The instantaneous state space distribution posits evolution along certain semi seperated pathways. They might not be strictly additive (except over a large parameter model), but more of non smooth shapes which might be assigned qualitatively distinct symbols. Thus risk information knowledge could be so represented in the form of stable state space shapes (dynamic evolutionary stability). This might be a technique to extract causality.
But this technique often ignores system boundaries, where the contributions might arise from the context as well and hence the model might be phenomenological.


Techniques:
A technique might be to use skewness detectors at the outcome of each qualitative filter. If the resulting outcome of a path in which a filter F is a member is skewed then F might have to be recaliberated. A prior recaliberation by normalization (based on a random sample) is good for a given context. In case of a realtime adjustment by feedback, the skewness of the outcome distribution, say the outcome distribution is skewed in that the positive is highly likely and it is correlated with F values being highly skewed towards a positive risk indication. Realtime deskewing would require prediction of end state at each level, which would require a backward smoothing.
Further reading on d-seperation and do calculus, inferring causality by Pearl.
Planning as Temporal reasoning James F Allen

An analysis of Computing in the Knowledge theory of Development

=============
The idea is to find if knowledge is the knowledge of the real nature of the world. If the singular knowledge system becomes more and more elaborate, being able to form the constrained rule system or theory in which all phenomena get enacted. This would allow people to leverage these phenomena towards creation of ordered systems, which can then be able to neutralize the disharmony and create incremental orderliness and predictability in its place. It is the purpose to drain the universe of its disorder or unpredictability and be able to explain everything. The point is that while we are at it, all intermediate steps should be consistent logically. Thus, multiple explanations of the universe is possible, with as much spareseness as might be warranted, as long as the system itself is consistent.



Nature of knowledge:
Progress might be described as the continuous revelation of the real knowledge. Knowledge is real in that it is singular and exists apart from human imagination or endeavour to appreciate and understand it. This positivist and realist theory of knowledge is what we would consider as the basis of our argument. Thus, scientific systems specify the rules that form the constraints of a system. These rules are coherent with mathematics, such as the electriomagnetic fields being in coherence with vector fields. The rules arising from theories, which map to mathematical theories with logical consistency define only the constraints of the systems, but do not highlight phenomena. Phenomena could be discovered by observation (unbiased) or controlled experimentation. The phenomena would have to satisfy the constraints of theory, which allows for their verification. In humanities we might map phenomena to human actions constrained by legitimacy and legality. The axiom of choice makes it necessary that actions are observable and verifiable but not predictable. Knowledge or theory is hence derived and extended from objective and real things. In the case of humanities as well, soft sciences like economics rely on sound theories in the nature of systems theory, theory of agency and soverignity, the social contract arisign therefrom and so on. Medical sciences lie somewhere in the middle. Hence, despite having noisy observability and reflexive corruption,soft sciences might still have some sound theory. In fact the presence of legal theories is what allows for objective determination of cases.
Human pursuit of perfection or human progress involves an ever increasing access to the real fabric of the cosmos, unperturbed by noise. This allows to locate phenomena in different parts of the real world and humans can carryout projects in the nature of engineering such as bringing together wood and river to produce a primitive dam. This engineering prowess is constrained by rational rules but yet are phenomenological. 
It is possible for people to work with self referential systems, such as mythological tribes. These self referential systems have different notions of reality in a subjective sense. Human progress, particularly in the project of modernity involves a steady adaptation of knowledge that is objective and real, rather than plural and mythological. It had helped in cooperation at a great degree and great engineering inventions in its course.

Nature of Problem solving: 
Human progress could be defined as a steadily increasing ability to solve problems. Human problem solving involves three phases -> solution, accomodation and consolation. Solution is only part of the approach. Once a solution is provided based on the nature of the 'real' world, that is rationalized and mathematically consistent way, the problem becomes owned by all and hence it is dealt with by accomodation in the social level, by social support and encoded into more complex legislations to protect the underprivilaged. The purely random nature of the problem once established by a consistent arbiter allows for consolation by philosophical and spiritual reflection. It is also a process of seeking completion through romantic contemplation. Hence, a utilitarian frame of progress could be seen to embody a continuous extension of completeness of rational solutions while retaining consistency so as to allow for accomodation and consolation.
From a development perspective, if the solution part involves the use of authority without consistency in a rational framework, then the alternate course is one of vengeance. Knowledge is the ability to evaluate a material development from a consistent rendering of reality in symbols and graphs. It allows for liberation to accomodate and console over uncertainty in nature or to move with vengeance, where the phenomenon of authority is clearly anamolous from the framework of reality.
Computational Knowlege: From the point of view of knowledge, computation relies on pluralistic, self referential, partial recursive constructs and hence might appear as regressive. Computation involves looking for events, which are phenomenological and mapping them onward to further events emitted by the system. The presence of while loops in the fsm model, allows for event gathering and emission. Hence computing is as much about event signalling as data processing. Hence communication and information technology could not be seperated. A computational entity much like an accounting entity is a router of events without specific preferences. In more ancient tribal settings subjective narratives involved events which propogated to create history at every increasing dynamic interactions. But in computing subjectivity, the individual is fragmented and the subjective units exist in equilibrium with others. Programming to events, involve human heuristic actors, who map an incoming event to an outgoing event with a bias towards some optimization subject to the constraints imposed by the partial recursive computing system. Hence, people optimize by programming to certain 'objective' functions subject to the constraints. They give rise to events that are phenomenological rather than knowledge based.
Development based on phenomenological event aggregation and reaction, in a closed loop game, without reference to an objective reality allows for normalization about mean positions. There are involved random experimentaions in shifting away from such means as defection (as with cellular defection) that brings forth rewards at times. Hence, rules are sometimes broken and the resultant advantage leads to evolution of the system as a whole. 
Positioning of ML: The question is whether there exist a 'natural' mapping between events. This might be achieved by looking at the point that eventhough phenomena are free willed, they still could be traced from past events or history. Thus, one might predict that a fast driver would generate an accident event, rather than a slow one. Thus the choice of speed is an event that evolves in time to an event of accident with higher probability. Thus, probability is the study of events which are apparantly random, yet have some motivational reasoning, such as copying or reading from precursors etc. It detects non linearity in the generation of events, normally of tendencies in certain frequency bands that vary in density. They also relate probability distributions where non linearities due to latent factors emerge. The presence of bald tyres and fast drivers might indicate a latent variable which is a conscious disregard for road safety and hence escalates the risk of accident non linearly. This kind of uncovering of 'intentions' allows for mapping between some events and others. It might help in predicting risks of accidents say and make game like reward moves. In case if the ML system attempts to plug into the event field and attempts to normalize and build the non linear statistical joint distribution of the whole, it had infact generated knowledge. This knowledge suffers from two problems
a) Firstly, the notion of reward is instrinsic to it. It needs an objective function to optimize. Hence, it attempts to minimize accidents by putting a high risk score on specific combinations of events. By reward based signals it does not indicate a global standard but only a relative standard. Likewise where a single individual makes a 'causal' action, virtuously by training the drivers in safety, he is not recognized. Hence virtue and vice have no meaning because, to 'cause' has no meaning in statistical inference. This is partially remedied where manual programming is used, because certain special causal events might be programmed in explicitly. 
b) Human progress involves understanding intentional actions as virtuous or vicious, which requires an 'attachment to reality' or sanity. The whole idea of sanity might be cognitively interpreted as referrence to reality, in an objective sense. This reality involves hence being able to arrive at objectively good and bad actions, through such things as theory of mind and empathy. Hence we might say, computational networks (even when programmed as with credit rating applications) drifts towards amorality. This reliance on normal signals and not explicit disturbing causation and intention and its framing on objective concepts of morality leads to a setting where the solution part of the society becomes distant and intractable.

Sanity Crisis : An intractable controller (as against a rationally tractable or rationally unfair one) leads to a situation where humans would have to question their sanity. This might propogate as paranoid or delusionary states. But delusionary states themselves is not absolute and unprovable as with Kantian Idealism. Hence, the development of computing based social setting might infact allow for rational examination but with higher perturbations. The behaviour of phenomenological populations lead to a necessary pressure to follow least energy pathways and hence convergence of local narratives to natural objective realities. Hence, ML systems converge again towards greater knowledge exposition however encoded and interpreted at a different level. Humans might loose their access or touch with reality. It might however filter through the intervening layer of actors dimly.

The evolution of games: Games evolve when orthogonal participants who attempt to optimize locally without regard to global rules, develop some rules locally. The emergence of games indicates the affinity of participants to objective truths. Once a game is set in motion, it also happens that the participants mostly cooperate rather than defect. Cooperation is a strategy of voluntary suboptimal trajectory in common interest. Hence, an awareness of the wholeness, further leads to construction of new rules. Likewise a recursive definition might arrive where rules become defined at ever higher levels. It might also require that the rules be as close as possible to real and natural rules so that the game could be scaled further. Thus, the modernist revolution might be in the making of a repetition with pervasive computing. If emergent sentience is possible, the game participants at higher levels would be making reasoned (atleast strategically tenable) moves. These participants might be higher level games as well. That is to say rule bound entities might emergently make game moves strategically thereby rising truths. 
That is to say, modernity was for individual endeavour to access reality, so that the individual is liberated. It involved a reduction of reward based games on decisions concerning nature. It allowed for accomodation and consolation to be handled at group levels that coexist with an objective problem solving engine. The realignment of individual priorities to work with local narratives leads to a dark age, where wars happen for no known reason for the individual. Even if one is to rise heroically, the game between empires rage due to different narratives. The automatons in such a situation might dominate attainment of agency, make random moves to search for further rules to cooperate. It might be a transhumanist transition, if the ultimate tendency to access reality persists, or if accessing reality is an anthropic affair, the emergent bots would plunge back civilization into the dark ages. Humans might go extinct or sustain a resistance.
Part 2
It is also that ethics could not be formalized and rendered onto automatons leading to problems of convergence.



Interpreting computing in the developmental loop of theory-games-culture-philosophy
========

Events > Let the world be modelled as a consistent system, such as the atomic model of chemistry. Models are theories when they have definitions and extend therefrom in a manner consistent with mathematical continuity and logical deduction (see axiomatic theory by Mendelson cite). Models causally connect by the continuum model happenings (good example are fields).Discrete unconnected happenings also arise. While theory defines constraints, it does not predict what would happen and what might not happen. Hence specific reactions that are legal within the model might be called as events. Events are random but legal. They can assume continuous quantifications but are discrete, but we will stick to truth values alone. Events are used by humans to construct causal connections, say a piece of wood blocks the flow of a stream, causing a composite event. Engineering envisages to bring together events which might be causally connected but arise at random and at distance (seperated temporally and spatially). Knowing connections derandomizes events, in that an event is no longer random but arises from another event. It allows construction of machines where events are connected in a less noisy fashion, such as the piece of wood being well finished and used as a dam on the stream, by perfecting the mechanism of connectedness seen in the real world (empirically while being theoreticlaly legitimate, otherwise the empiricism might be spurious).
Derandomization as Development > As people work with events for engineering ends, they attempt to complete the model using a feedback loop. The more complete the model is, without compromising consistency, one can see more problems being derandomized, allowing for engineering fixes by locating neutralizing events or composing into machines.
Bias and its institutionalization > However no model can be perfectly complete and consistent. There is bias in attempting to complete the model while sidestepping consistency. The bias is institutionalized in the market. Here games are defined as self referential systems of high consistency, which work on explicitly connecting events (unlike the theoretical world where events emerge). Reward pathways are encoded as rules against events arising from the environment, or real players. Thus, players attempt to work hard and build machines by solving real world problems and give rise to events which are rewarded. This might even be establishing ones eligibility for a loan or booking cargo on a system. The event that is initiated gives rise to numerous events consumed and acted upon by other players (like banks settling). Thus, games are hooked together by the 'while' operator which not only listens for events temporally streached but also to other game outcomes. Thus, the game as a whole might participate in higher order games. This elaborate construction over self referential games and rewards encourages optimization to local constraints (or rules of the game) rather than finding phenomena in a world of real constraints (of theory). The optimization is orchestrated to produce solutions of incremental accuracy to the real world.
Statistics > Induction of theory from events without mathematical intuition is statistics. Events might distribute in a population in terms of frequency of unbiased observation that have relative differences within specific quantifiable attribute of the population or rather intrinsic event. Thus, connection of events might be recognized by spatial analysis or temporal analysis by understanding covariance. The distribution of events seems to have connection with other invariate event. The frequentist school believes that the distribution of a variant is non random naturally for any point of view from a indexible variate. The bayesian school believes that interesting distributions arise due to excitement of latent variables on the network which influence the joint distribution in a non linear way. Thus, instead of the real indexed supposed invariate, bayesian is about joint distributions.
Pure Induction > Inductively dealing with events without the mathematical and logical intuition is what is questioned. Institutionalizing bias allows for democratic participation in solving problems of choice, by selecting suitable games by individuals. The outcome of such games causes emergent events which the individual has to make sense of (say a road widening damages his house or a certain medical condition is very expensive of being treated). The individual sees a certain bias to the system based on event orchestration. It does not hold a mathematical proof against bias as with realist systems. But more than bias, the problem is one of control. The individual does not have means to control the proxy system that substitutes the original pursuit of engineering solutions by knowledge expansion.
Development > Knowledge expansion as an original agenda of development leads to ambient control of nature, leading to genuinely uncontrollable events, labelled and identifiable as such based on the objective model at hand, to be capable of handled culturally by accomodation and philosophically by consolation. This process of control plus accomodation maarks a highly developed civilization.
Necessity of Realism > The motivation to work on expansion of knowledge is a tragedy of the commons scenario and is filtered of bias by the mechanism of explicit event based games. Games motivate cooperation which crystallizes as rules. Games rely on equilibrium rather than sovereign control (absolute right to turn off and frame policies). In order to cooperate universally however, a reference to a realist agenda of non zero sum nature as a project, a culture and philosophy is necessary. Universal cooperation relies on the nature of  progress as that theorizes the world objectively and likewise hold human values (acknowledging bias and noise) as capable of being objectively reconciled (as universal morality, virtue) based on mentalization theory. From the mentalization a realist theory of agency, action, social contract and an entire polity is capable of being constructed. The uinversal agreement over the nature of progress as involving knowledge and accomodative constructs of arts and philosophy is intuitive.
Computing and its impact > Computing increases the frequency of event generation and consumption.Hence the need for universal constructs is capable of being deferred. FSM is a game orchestration and integration schema. It is partial recursive without universal theories. It is able to quickly generate and evaluate events without the risks of mutation in transmission (low entropy and high throughput) associated with other modes. Hence, it can create highly specialized games which can solve natural problems. Games can be very incomplete and membership to multiple games (of consumption, production, travel) can help find meaning from game mates. High surplus from games while equilibrium is maintained at population level, can lead to detatchment from solving real problems idealistically. Pragmatism becomes the primary mood. Pragmatism sceptically views virtuous and vicious actions and is amoral for most parts. It accepts incomplete solutions and its mode of accomodation and consolation is one of temporo spatial distribution (in the long run things level off).
Machine Learning Impact > Programming of games retains the flavour of recognizing causation of interesting events, by humans recognizing special programmatic (game) flows as arising from specific development. Machine learning is the construction of natural links between events based on what is 'normal'. It is pragmatic and sceptical of extremes - suffering or meritorious accomplishment or vicious exploitation. Pragmatic models do solve real world problems by their high frequency capable of generating games around tough real world events. The derandomization of such events become a game strategy for such games (where nature is a player - identificaiton games by Verma and Pearl). 
Idealist Conflict resolution model in the world > The dismissal of notions of soverign control and their reconcillaiton over realist models of human actions and rights and likewise the non requirement of realist theories of the physical worlds lead to a new notion of development. It also removes notions of complete intermediate units (solution+accomodation) which run inevitably into conflict with other subjectively complete units (with high degree of attachment of individuals as being their identities). Fragmented identities in a pragmatic world does not give rise to conflict. Reconcillation of conflict in an idealist world would need constant pursuit of realist knowledge. 
Anarchism > The primary concern is that of an anarchist world and its potential for emergent authorities. The absence of policy and high frequency games or gamification becoming sufficient to engage people involves frustration, but it is a political question if this is better than the anxiety and grief over random violence by identity conscious groups. A culture of reconcillation of well defined identities is a conservative stance, while the use of upto the purpose identities without grand ideals is a democratic and leftist stance. We know that chaotic oscillations arise from near random systems. Thus as everyone has equal authority and has only their games to play, there might arise situations where the ability to make random moves (synonymous with sovereign authority) evaporates to higher level of game entities. Thus entities might generate random strategies and thus come to wield authority. The random moves might in the beginning be experimental defections necessary for games to thrive, but may come to define long standing conflict groups. Thus, there might be emergence of sustained conflict from transcient random jokes on the opponent.

Summary>Knowledge as purely Theoretic. Empiricism is only observation of events. Theory is unifying, derandomizing nature and morality. Theory is completable by games, culture and philosophy. Building on empirical events in game formats leads to weakening of downstream completing entities. Completing entities feedback to need for conflict resolution leading to further completion of theory while keeping the consistency. This loop is interrupted by high frequency inductive games, leading to interruption and anarchy, risking emergent chaos or decadance.

Interpreting computing in the developmental loop of theory-games-culture-philosophy


Developmental Loop- Place of pragmatic Completeness Seeking behaviour
========

Development is surplus derived from theories which would highlight events that could be viably utilized.
Progress in completing theories involve games that work with events without need for a singular and unified theory. Games work with emergent rules and are subjective. Optimization is used instead of solutions.
The third stage of development is where the engineering and the gaming arms unify and attain completeness by culture and philosophy. Completeness of games is attained by polity and of engineering by investment into a common infrastructure. Communities and nations arise. They are united by commitment to mythological consistent system at a lower level, but large organizations need to refer to realist knowledge as a conflict resolution process.
The conflict internal and external is reconciled by referring to the further development of theories. This is a feedback for further progress.

Now the question is with respect to the third phase. If a sufficiently powerful game could take care of surplus generation and self regulation without the need for common wealth and polity, threby also cutting the fourth phase of feedback, then is development said to exist?

Another View
Is it that divergence happens only in situations of isolation, the third phase of seeking completeness could be explained as impelled by isolation. Two points stand in contradiction to this view - one that spontaneous fragmentation of non isolated populations had occured microscopically as well as macroscopically in history, secondly that the world view of humans consist of two distinct parts in accordance with cartesian dualism, one of cognition and the other of reflection. Cognitive perfection is often supplmented by reflective attempts to construct ways in which completness of the whole ie the world plus the individual is aspired. This aspiration of the self is projected as culture and philosophy.


Refer : On the Inseparable Co-operation of Sense and Itellect for Arriving at Cognitions By Samuel James Rowton
From our look at the above thesis and the works of Ferrier, we find that knowing the truth is important as a part of civilizaitonal progress. We look at it from the point of view of philosophy being instrumental to conflict mitigation. The subject object debate helping attaining temproary completeness as an entity progressing.

Public Sphere Metaphysics
======

In our reading of the philosophy as the 'speculative science', championed by the early modern philosophers such as Descartes and Spinoza and well rendered by Ferrier later, we see that there is a speculation on the nature of reality and the self constructed through rational process, intended to uncover this truth, perhaps asymptotically. The idea is however that system building is important in philosophical inquiry. This is different from the twentieth century analytical philosophy in that the latter aims to explain philosophy without metaphysics ready for empirical verificaiton. In fact in the turn of the twentieth century, metaphysics had been much discredited due to the power of empirical science in demonstrating the amount of good things it could bring into life, that there seemed to be a waste of time to wonder about forms which have no practical relevance.
The need to wonder on such things as detached from its value proposition was argued by Max Weber much seriously and was contested by Rorty(The Politics of Intellectual Integrity-Richard Wellen). The argument was with particular reference to the intellectual independence of universities as trustees of the positivist effort of mankind as detached from normative values. The need for the pursuit of positivist knowledge distanced from rewards and even partisanship in promoting policy was strongly advocated by Max Weber.
Whitehead revived the need for metaphysics in the early part of twentieth century by stating that while it is fashionable to condemn metaphysics by scientists,actually they are only condemning the criticism of their own metaphysics.
The direction we head is to see that metaphysics of intelligence, knowledge and the ultimate burden of mankind to know the truth is attained by humans by rational means, for which metaphysics provides some support but is followed by accomodation of love and faith and the reflection by contemplation of nature. This is synanomous to the ninth epoch of Grecian philosophy which vested human ability to truth on Faith and Love. Therefore, one ought to content that the pursuit of objective knowledge is what allows liberation of mankind to seek such ultimate reconcillation away from the public sphere, within a closed community and in his privacy. 
Thus, we refute Rorty's idea of a pragmatic philosophy (where objectivism is infact perfected intersubjectivity) as being sufficient for public sphere discourse. We seek that a lively, well rounded critical philosophy seeking objective ends is in fact needed and debated in public sphere in order that there is reconcillation of universal concepts (not just time and space as suggested by Kant) concerning the nature of the universe and on the nature of knwoledge and truth. 
We might in fact see pragmatically, that such an advocacy for objectivism in philosophy as an expression of commitment to find universals, so as to foster peace and allow ourselves to differ only on what is decidedly subjective expressions of our self. Such a pragmatism is what we seek to consider the intervention of computing into this cycle of human development.
This leads us into the necessity of proving the existence (since we support rational philosophy, we attempt to prove - even if not rigourously, by analogy and by mathematical reduction) of a phenomenon wherein agents who modify their states based on phenomena alone, without abstraction lead to emergent macroscopic phenomena outside of their control. This abstraction we take it to be not as a continuum with subjective phenomenological knowledge (as intersubjective extraordinaire) but as a neumological universe which might be asymptotically attempted of approaching (to cite Whitehead) in order to result in a pragmatic outcome of avoiding macroscopic phenomena (such as population splitting into rival factions) and instead contending with 'intentional' different complete systems. This would also need of proof that humans need apart from reason - charity and hope. Charity is a part of the solution framework in a society where an oracle simply solves problems by mitigating it (conservative at a social level) and hope by philosophical insight which helps framing the finite inside the infinite.
Thus, we might look at the mathematics of homogeneous stable systems and see how they might spontaneously split rather than stabilize by self balancing. We might look at the emergence of games, on Lyopunov stability and lagrangian pathways (for which objectivism provides a way of reaching). Likewise much of our attempt to look at the situation stems from a social perspective. That is to say, humans having come together as a society, how would they avoid either a complete merger (evolutionarily this is feasible, the domain of absolute equality) while maintaining cooperation at conflict avoidance levels ( by sustaining liberty and free will, without that becoming defection). Thus the whole pragmatic case of philosophy is one of maintaining stability of social groups. This would need idealism apart from material engineering. The loops of development, involving abstraction of phenomena (as suggested by aristotle - who rejected sensual experience and instead embraced the method of the logic) resulting in isolation of useful phenomena (instead of a case where everything is phenomena). The production of surplus by exploitation of useful phenomena followed by the distribution of such phenomena by duly constituted authority into infrastructure and stable states. The idea of duly constituted authority here involves notion of intersubjectivity and more importantly the objectification of human intents - by the metaphysics of human wants and being able to isolate truth from falehood in terms of agency, free will and the whole phenomenon of social grouping. This would involve appreciation of the phenomenon of social grouping by culture - proding, criticising and reflecting on why we are together here on a continous dynamic basis, so that a stable level of cohesion is maintained without complete merger. Therafter cultures though capable of high degrees of communication and replication, themselves might run into concrete divergence, leading a feedback loop to the objectification of nature. 
Thus the two phenomena we wish to investigate are 'divergence' and the phenomenon of social stability - as being peculiar of living organisms of social nature and its stabilization away from the pressures of eusociality or even multicellular analogous evolution.
Some references>
A study of an ancient discussion in the Philosophical transactions of the Royal Society discussing on the stability of vessels by Lord Atwood in 1798 addresses the point the question that heuristics of observation and practice had been the general method of naval architecture, there is a need to employ theoretical models. This point of view supports the necessity of higher levels of abstraction as being of utilitarian necessity for development of novel designs (the same case as we had discussed on the need for identifying interesting events by having a theory). 
The book by Routh on Stability of motion discusses the point that a system might be stable if it is dynamic and a perturbation is not capable of dislodging it from the oscillations about its mean trajectory while a perturbation being able to diverge the trajectory (and possibly disintegrate the system due to internal tension) as being indicative of unstable systems. Instability is mathematically expressible and hence deemed to exist.
From these examinations and inquiries we see that the major point of contention in our work is whether phenomena are sufficient of interpretation based on pragmatic expectations or that a neumenic reflection is required. All the discussion over computers being phenomenological and do not go towards constructing explicit well formed objective rules boils down to whether there is a need for coordination of the society as suggested by Durkheim or whether markets serve the purpose well as suggested by Hayek (refer Two Views on Social Stability: An Unsettled Question by Birner). This question is to seek whether there is a need for a duly constituted authority in order to objectively express the views of the society. In such case, the idea of natural justice and other realist concepts come into play which could very well be extended to their use in explaining natural history as well. If the whole realist concepts are avoided, as suggested by Rorty, we might have it like rules emerge from market forces, or games where the rewards are tied to solution to natural problems and mitigation of violence over distribution. Thus, in the former case there is a need for explicit law and the latter case of emergent rules from games.
It is self evident that humans cannot avoid metaphysics all together. They could not help wonder who they are one why there are here, reflectively (speculate that is). Hence they ought to construct theories therefor apriori. This drive causes them to formulate ideal rules and models of nature from which they relate to the tragedy of human condition in being able to percieve only the noisy correlations between self and the object (Quentin Meillassoux ). There is a constant drive to redemption over the human condition to move away from reward based temptation to the pursuit of the ideal and perfect. The Alexandrian school puts the realization of such goodness (in a Platonic monistic sense) in Faith rather than reason. But it could be aysmptotically approached by system building approach (Whitehead).
Now, if we look that everything is aposteriori, mainly a reactionary stance to the usurpation and violence associated with the agencies vested with authority (the denial of apriori conclusions stand assault starting from Kant all the way to Nietschze). In this stance, we feel that games arise from pragmatic concerns and rules boil up and intersubjective towards ostensibly universal constructs. There is also a strong correlation between libertarian democratic historic moment of USA in the wider belief in the market and hence pragmatism (Rorty).
If one talks of redemption in such situations, one is in fact asserting Metaphysics. But the more intricate question is not whether metaphysics is required, but only that whether it is required in the 'public sphere' (Rorty). Does it require of people to constitute institutions which would promote idealism in order that a collective metaphysics is imposed. Here however constructive metaphysics might find difficulty, but critical metaphysics would stand acquitted (Whitehead, Weber). Hence, as to the question whether it is a publicly sponsorable intellectual occupation to approach metaphysics is a question of importance
The points in favour is first of the need for criticism of any implicit metaphysics to emergent situations. The second one might be concerning the way in which violence from authority not warranting a complete banishment of authority itself to result in less dramatic but pervasive violence (in pollution, economic cycles etc). The third argument is the explicit problem of the lack of apriori metaphysics in non human animals had lead to evolution taking hold of the situation eventually leading to the repression of the free will of the species. Hence, it might be uniquely human to seek an explicit metaphysics. This is also closely related to the point that human socieities are phenomenologically interesting stable systems straddling between eusociality and pure liberty. This phenomenon is needed of sustenance by a public sphere metaphysics. The fourth argument is the point that divergence in implicit equilibrium systems might lead to violence. The fifth point is the historical uniqueness of computing as a phenomenon which dramatically shifts the narrative in favour of self balancing and emergent systems in a one way path. This unique development would hence warrant an objective investigation in order that it does not become the primary influential force in history.


--
An Argument that Carteisanism is Zeigteist in the Computational Society
=========
We might even say that this shift to a dualist Cartesian notion itself is reactionary to the onslaught of computing phenomenological decisions in the conduct of our social lives, just as postmodernity and pragmatism was reactionary to authoritarian violence. It might still be argued as 'Zeigteist'. (William James vocally oppossed Cartesianism Refer https://journals.openedition.org/ejpap/415, A Critique of Rorty’s Conception of Pragmatism
Paul Giladi). Here James had argued the incompleteness of the system of metaphysics in cartesiansim, while that it precisely what we appreciate. The incompletness combined with rational abstract consistency, allows a unification of ideals among different people, while they can then be charitable in redistributing wealth as well as seek redemption in romantic contemplation in faith, both of which are required for completing life, which we say is not be governed by any monistic philosophy such as pragmatism.
References:
To reject metaphysics is metaphysics (Pierce as discussed in Giladi).We must philosophise, said the great naturalist Aristotle – if only to avoid philosophising. (CP: 1.129)



The nature of Intelligence:
1. Intelligence is the ability to survive. It is a feature that allows an organism to survive in the planet and it is never superfluous. It is strictly what is needed for the job of survival. That is the reason why we do not identify intelligence with complexity -  A complex weather phenomenon, even ones that are longlived enough to be considered alive (such as storms on jupiter) could not be considered alive. Likewise a highly complicated mechanical system might not be considered intelligent, even if even appreciating its complexity cursorily is difficult (admittedly it might be capable of understanding by effort). Hence intelligence might be identified with life itself.
2. An organism is identified by its form, its appendages and generally its spatial existence. Intelligence with respect to an organism is expressed when it responds to stimuli from outside world. This adaptive response to signals from the environment is an important aspect of intelligence. Thereafter the organism seeks to protect itself from the environment, in that it attempts to grow - which might be in the nature of protecting itself from corrosive elements or seeking elements that could cumulate to its biomass. Say a very simple worm burrows down when the sun gets hot, it is actually acting in response to certain events produced in the environment by seeking comfort and avoiding pain. 
3. But there is another interesting aspect to intelligence. Apart from simply seeking pleasure or good (in Platonic terms), it sometimes seeks the bad. Thus let us say an organism is soaked in a sugary solution, there exists a point where the organism actually wants to experiment the saline solution considered corrossive. This might have been by the propogation of spores or probing with appendages, but we might see in many cases an inverted behaviour of experimentation, in search of better goodness, closer to the ideal. That is to say, we might account for these behaviours by a mathematical formulation wherein the organism has an internal memory which is an open ordered set of good things. Hence the organism ranks its experiences from good to bad and seeks to expand this continously.
4. Thus, the organisms internal memory is constantly programmed by events being generated in the real world, which we might not be inaccurate to consider as a giant computer. Nature or earth itself is able to produce a great deal of paralellized events and it had taken four billion years of such events to allow the organism to get the best set of goodness. In fact many versions exist, we will come to that in due course. What is to be remembered is the great amount of computing effort that had gone into the production or incubation of the organisms evolutionary tree. One might see that some of the events on the earth had been catastrophic, but they might all be very well conjectured to the strategies of adversarial event generation in a generally hostile environment with an uncertain strategy. The earth itself is an open system with the universe and hence it might be the universal computer in action.
5. As the evolutionary tree progresses, in each stage an organism is faced with an universal dilemma, that is the dilemma as to whether to expand and search for something new in terms of rewards (or disprove the pain source) or to consolidate its existing position. This dilemma is reflected in our partisan political drama as well, despite the epochial ontological seperation. This dilemma is addressed from time to time in the course of living.The enterprise of political conservatism is not regressive, but a consolidation of existing positions while that of the left is porgressive.
6.It is also important to note the universal thread running across life and intelligence. It is a popular ontological expression to quote the inability to know how it is to be a bat. In fact it is not that formidable a problem (say as against imagining a stick with only one end). It is possible to see that a bat, or for that matter, even a tiny bacterium is alive by its behaviour (its expression) in an empathic manner in response to pain or in appreciation of pleasure. Thus, we had in art had often been having bats and other animal characters in children fiction.
7. Hence, the dilemma of consolidation verus progression is presented universally and this had been in evolution observed as the competition between subjective knowledge that causes defection (or a random dispersion) and thus crossing over into an isolated territory of greater privilage causing divergence of species. This high degree of variance and subjectivity of this dilemma had been instrumental in generating macroscopic phenomena. That is to say emergent consciousness arises from multiple agents attempting to stay together and thus be equal or to seek their own understanding of the environmental rewards and pain (thus at their liberty) and seek to get a better deal and spawn a new evolutionary pathway. This kind of tension that exists among living creatures and the universal dilemma over regrets of missed opportunity, powers macroscopic emergent phenomena of intelligence. This is particularly well reflected in multicellularity. There had been a consolidation of common knowledge in the central nervous system with the surrender of local agency. We might see that eventhough there is an unicellular origin of the organism, subsequent differential expression of genes for tissue specialization, effectively creates agencies that might be adversarial or orthogonal and the emergent confluence of rules of the game might have been the conscious intelligence of multicellular creatures. Even here the defection is handled in a graceful and controlled manner, as with germ cell defection (see C.Extavour).
8. The ability of emergent agencies to produce intelligence might give us clue to the direction of future intelligence. But before that we need to more precisely distinguish natural and artificial intelligence. Given that we see that the emergence is caused by paralell programming over billions of years, we ought to see that intelligence is a hard won phenomenon. We might however be lead to think that it is Markovian given the concise way in which it could be encoded in the forty eight chromosomes of human genome. We might also see that the different brances of evolution are essentially independent and despite the practical dependency on the biome for human survival, it is not theoretically required and hence we might argue that the human state is Markovian in the development of intelligence and it could hence be created without so lengthy a history.
9. However, one needs to elaborate on the nature of senses and perception and expression in order to reflect better. When it comes to intelligence, there is primary attention concerning the prefrontal cortical processing and the inputs by well defined senses. But there exist senses that are deeply embedded in our systems, which give rise to signals and trigger behaviour which is not encoded in the rational faculties. It might be a feeling of being fully fed or hungry. These might trigger specific enzymatic pathways at the cellular level, bubbling up to produce complex influence on preferences and biases at the CNS level. All these are parochial and relate to our legacy of millions of years.
10. In more visible terms, we handle this heritage by expressions that involve our fellow people. A smile means something, that could be inferred from certain learned behaviour, but deeper and more influentially it has roots to our, say five million year past of percieving facial reactions and perhaps a billion year past in appreciating symmetries. Even though the argument of Markovian nature of intelligence might be tenable, the selection of individual traits are from an extended sample of a billion year temporal and millions of units of spatial surface. The cumulative effect of all this parochiality and heritage to our 'gut' intelligence is figured out in the concept of culture. A smile in a market place has so much meaning encoded to it, that reflects back to our microbial past. That is why culture is considered a highly loaded word in english language. Likewise when Turing figured out a test for intelligence, he did not postulate it such that a computer that is capable of answering factual or logical questions that could match a certain category of person to be truly intelligent, but rather said that a computer should be able to fool a person to thinking it is infact a person. Now this is a cultural factor and this parochiality is what Turing cleverly and concisely put into his test.
11. Hence we might say intelligence of an artificial nature is in any case shallow and sparse than natural intelligence (unless a cheap universal simulator is found). We might however look forward to intelligence that can simulate human intelligence to a certain extent, as Turing expected. This is captured in the idea of weak artificial intelligence.
--
Goal based intelligence:
1. In case if we do not express a given goal as derived from the survivalist goal and the dilemma of consolidation (could be in philosophy extended into the existential dilemma, the political dilemma of equality vs liberty and so on), then it might be a  narrow goal. A system that attempts to solve such problems might also be called intelligent in a limited sense.
2. If we start with the assumption that universal apriori truths are not perceptible directly. They exist in fact, but due to the work involved in accessing them, that is the reliance on phenomena to access the neumenological truths, distorts them. This is due to the fact that the work involved is difficult of estimation and the reward seeking behaviour of the expressing agent naturally distorts the truth. Hence, we might say context free truths are not useful. This axiom could be extended to mean that there does not exist a finite set of truths which could be mapped to a finite set of contexts in the form of truth table does not exist as well. Hence, all that we have is belief.
3. We might intuitively say that in that beliefs in that they are biased, might be surmized that they intend to point to the universal and natural truths when a large number of active agents attempt to frame and promote their beliefs. Hence, a critical frame is important to make sure the beliefs are as close as possible to realist truths. The development of a belief propogation network, wherein beliefs are simultaneously propogated along multiple alternate pathways to result not in a binary decision, but a shape of a joint probability distribution is hence a good step in emulating intelligence for narrow goals. The directedness and acyclicity of the graph is a requirement in Bayesian theory. However, reflexivity and revisioning of past evidences either from an impact of the organism or by paralell test moves is closer to the real world. Thus, a system of agents could only produce intelligent perception of an environment. How this happens in unicellular adversarial search (or game rule emergence) might be further inquired.
4.In case where a realist model could be approximated apriori rather than constructed a posteriori (in which case we assume that the emergent rule system corresponds with the universal Lyopunov attractor which represents the truth). The concept of aposteriori truth is captured in such expressions as truth alone triumps, where truth is seen to be animating and representing the telos or ultimate stable state of a system irrespective of initial conditions and storng perturbations. 
5. A causal model preponders a system of unified connected completely like components (quantifiable assortments of finite set of components) each of which could be expressed either as scaled versions of the other or aggregations of others. There is a notion of observer and frame of observation where the components interact to produce quantifiable results of the singular quality in the pursuit of the observer. This could be framed as an equation such as (ax+by+.. =n). A system of equations of polynomials reduces algebrically the world capable of being perturbed arbitrarily and observed for outcomes. Notions of system of equations denote incomplete systems which are represented by finite observations and with a knowledge of required conditions (constrains) could be optimized to produce sufficient conditions for a certain outcome. Smoothness and continuity are essential for the causal model to operate, allowing the calculus of infinitesmals to be able to represent the system coherently in a rationally accessible manner.
6. Metaphysics had dealt with the merits of apriori assumptions and that of a posteriori reflection. There had also been movements that question the necessity of metaphysics itself. This had been the case with pragmatists. Some of the pragmatic movements themselves, such as with Rorty had been criticized to be dogmatic constructing its own metaphysics (Giladi and others). If we look at pragmatism as a metaphysical stance, we see that the dilemma of existence itself is called into question. All the apprehensions and anxieties in the world over regret of missed rewarding opportunities are constructed in this metaphysics to have been introduced by philosophical ruminations, metaphysical rants and power structures which tend to harvest such inadequecies and anxieties. Postmodernity in fact proposes to reject any authority, including appeal to reason itself. Hence, metaphysics itself had been criticized in favour of systems where whatever the truths be, they are embraced as being completely historic with no element of possible ergodism to them.
7. The assertion that there is no apriori reality allowing an observational frame of reference divests  rigour and custodians of truth of power in the social domain. The rejection of universal attractors of aposteriori truth, rejects any pattern to history, of any narratives from history. It lets history be what it is. The complete historicity of intelligence likewise also disallows the asymptotic convergence of natural and artificial emergence. But we see that the question of existence of either of these frameworks of reflection is concerned with the question of whether evolution is to be regulated.
8. The most interesting domain of evolution as of now is social evolution. The society had been discussed as being a spontaneously evolving system in vanguard literature. Hence, the question of whether there is a need for metaphysics is to be evaluated with respect to the pragmatism of control of an evolving society. The real pragmatism (such as with Pierce) is not a dogmatic rejection of metaphysics but it is a question of optimal choice of control. In that the most domiant question of the consolidation dilemma is social today, given wars, economic slowdowns, pogroms and pandemics and the choice of progression of science all concern social decisions, public opinion and policy, the fundamental question of metaphysics might be one as to whether there is a need for an objective observation, either from a regultory point of view (as with Durkheim) or in terms of emergent markets requiring nudges (Hayek). In art, the rejection of metaphysical anxiety had in fact produced some heartwarming tales of happy go luck heroes and the age of innocence. Theology suggests the movement towards such points of renunication of anxiety (as with the Alexandrian school). It is also the concept of redemption from the human condition, wherein the dilemma is rejected at last, so that one is able to do the right thing and attain the highest possible moral state.
9. Thus, we might speculate that there exists a sentiment where there is a fear of going down the wrong road. The presence of such sentiment ontologically, introduces a need to collectively speculate on the control of the organism of the society, as projected from the individual dilemma. 10. Social constitutions in their most stable states are the most peaceful. Desirousness of certain state for any coherent intelligent unit (which at this point is the entire humanity) would need control, either by external coordination and allowing an emergent coordination as with markets.  Even in the latter case, there is need to visit redundancies and needless oscillations. Thus in the true sense of pragmatism, we maintain an ambivalence, as to whether apriori truths exist or not, but we need to take a stance that would be reactive to specific drift directions, constituted by specific beliefs ascending the role of absolute truth, both by the weight of authority or by feedback loops. 
11. Hence it is with response to the specific phenomenon of sociological integration in the present moment of history, that prompts us to adapt a particular stance towards control and collective reflection. There does seem to be a specific dogmatism to postmodernism that explicitly guards against metaphysical reflection. Secondly, the development of the electronic computer dwells exclusively on the phenomenological world, in that events are generated as  proxy to the universal phenomenological events and adapted out to feed onto event mapping agents, who work with various degrees of autonomy. 
12.The autonomy of computing systems are reflected in the time parameter to their functional expression, implicitly introduced by the open ended 'while' loop in the FSM model as it works over networks to produce results that are unpredictable to a good degree, making them difficult of appreciation and hence coordinated control. The proxying of events which is then funneled to intelligent agents might result in emergent collective intelligence which would favour extending the bounds of experiential set of life itself, a higher consciousness at the level of the society. This is probably the path of artificial intelligence in the future and in our opinion it is historically unprecedented due to which the most pragmatic course would be to revert to the notion of apriori truths of the early modernists. 
13.This would likely allow construction of coordinated control, so that computing could be interpreted not as emergent intelligence, but be capable of answering the query of an individual to the strengthening of his liberty.
14. Hence, this might be a construction towards self determination of human agencies and the telological nature of convergent redemption as being asymptotically far away, yet accessible by romanticism and spiritualism in the sense of yearning. The yearning itself might be redemptive and hence might reject concrete effort to realize the ultimate path. This would mean a consolidation of present position, using a criticism of emergent clouds. The emergence of automaton agents which do not explain to individual queries, might not be superintelligent. They might simply contribute to the emergence of a collective system of intelligence, which is akin to multicellularity. Metaphysics of modernism allows the enlightenment of the primacy of reasoning to deter such progression.
15. The computational conception of decisions is essentially normative, tending to embrace what is normal and rejecting the interesting cases as being irrelevant. That is to say, profoundness is sceptically viewed and ignored. On the other hand, control is dependent on making rules about extremes and leaving the normal out of the public sphere. There is a reward of extreme accomplishments and penalization of extreme transregressions, as for the wide band of normalcy, people are expected to negotiate and reflect as libertarian beings. In a computational network taking decisive direction (rather than cancelling out noise) on the course of evolution of the society itself, we might see the rejection of extremes to be impeding the development of theories by supplying missing pieces, which would come back to promote normative goals. Thus, there is low latency in normative realizaiton in the system, leading to an evolving systetm that does not plan and hence does not control. 
16. Thus, it is essentially a destruction of planning and construction of ideals expressible as hope culturally. It arose paradoxically from the establishment of inclusiveness and equality around globalization, civil rights of the late twentieth century (network theory of Castells).
--
Part 2 of Goal Based Intelligence /Transcendent intelligence

1.We need to start our inquiry into intelligence from the point of temporo spatial intelligence. Firstly if we look at the spatial intelligence part, we see that as of now, some machines are able to identify discrete objects in its vicinity, only by drawing a random sample from the environment. We have seen that the MCMC method of random sampling is extremely successful in many cases. But in many machine  learning cases, there is reliance on humans to sample from the environment and provide it to the machine. The fundamental mode of operation is however the same, whether it is machine drawn or human provided. The machine performs a neighborhood search assuming a statistical gradient of smoothness and idenitifies discrete breaks and marks them with boundaries. Thus instead of ending up with a multimodal mixed distribution, the machine is able to perform a neat classification job. 
2.In machine learning tasks, we see that there is a classification between classification jobs and continuous regression jobs. We might say that classification jobs when applied densely and quickly might replicate a regression job, by inferring a line of seperation that does not comply to any mathematical smoothness standard. But in practice, the reliance on smooth delimiters or trend setters as classifiers are useful. The establishment of the trend line or some such sample also helps appreciate the continouity and gradient which itself serves as inputs for identifying discrete breaks.
3.The normal computational task, we might say is control oriented. The computer attempts to create the spatial reality by using recursive classificaiton. That is to say, if a computer is assigned a task of allocating the fertilizer to a heterogeneous field, it first attempts classification of the crops involved by observing continous and discrete variance and once such discrete parts are identified, they could be tracked temporally in order to decide on fertilizer allocation. We might see that in order that the problem does not run into combinatorial explosion, the following are required. One is the need for constraints (supplying the required condition and then searching for sufficient condition) which makes the problem optimization problem. Secondly there is a need to classify heirarchially. A high level classificaiton ought to arise form immediate classifications such as maize and rice going further on to classification between fields and woods. This kind of classificaiton reduces decision making such as the decision first being whether to apply the resources to woods or fields and in the case of latter further evaluate whether to select rice or corn.
4.But the tricky part in temporo spatial intelligence, is that the classificaiton is not entirely divergent as in a tree. It frequenly converges. When a disease is tracked temporally, there might be multiple pathways that converge on to cliques. These cliques are interesting only because we denote them such. They are the goals. It might be that the when there is a heirarchial classification available in a forest, such as tree based food sources, root based food sources and in both cases, such as badam and groundnut, there might be a convergance that a single outcome is present (semantically a single object, say a nutricious nut). The nut is interesting only because it has some goal based significance.
5.In AI research often there are reference to accurate goal specification. At a higher level, the goals might entail multiple temporo spatial pathways (such as the goal being obtaining nutricious food source rather than gathering nuts). There might be a cost effective search for the best solution. The search might be a breadth first search, where diverse routes are picked up or a depth first search, where some preliminary attempts are made in test mode to futher investigate a route. The point is that in order to control the environment, all problems eventually get reduced to logistical problems. Hence temporo spatial intelligence explains a great deal of what is intelligence.
6.However humans and other creatures seem to be pursuing goals which are only intermediate to a higher level goal, of biological satisfaction. The itermediate goals are dynamically arrived at by the interaction with the environment. Much of the effective environmental feedback is obtained in the case of humans from fellow human human beings in contemproary times (most problems are distribution problems). This feedback allows specification and ranking of intermediate goals to helps satisfy the biological goal which might be simple but colourfully rendered by environmental interaction with the world at large and peers. Thus a machine that needs to emulate human intelligence ought to be specified such high level goals as survival. But the goal of survival requires a specification of pain before that of reward. Hence, the feedback is to be supplied by human agents, for which case, the human agents ought to be more powerful than the computational agents. They ought to either provide physical feedback by interactions in the real world or have to provide control signals which are intelligent enough to be considered completely random (as we infer the environmental signals from the universe at large, barring a few well known ones). This is the primary area of challenge, which leads us to suspect that we might not be able to provide selective feedback, leading to our being overwhelmed by a machine specified of goals similar to biological goals.
7.But it might be speculated quite reasonably that human intelligence is transcedent. In that all human goals could not be specified by biological goals. There are instances of pure altruism, which might be inferrred to indicate that human intelligence had transcended biological goals and are now referring to goals higher than the biological goals themsleves. Apparantly the goal of biological process is to build order and knowledge from randomness and disorder. If such a higher order goal, say of justice and liberty could be followed by humans, then they are circumventing biological goals. This requires a belief in objective high level goals, which we don’t have methods of encoding into machines. Perhaps it is an emergent outcome of sufficient intelligence, or it is historic, we can only speculate.

Dilemmas of control and the question of method
1.We have seen that control might be feedforward or feedback based. One important dimension to control that is often ignored is that of reflexivity. That is to say, the system attempting to control another system slowly harmonizes with the other system as well. Let us take the example of a village in a forest both being distinct well bounded system (even literally). The forest is likely a highly complex system, in terms of lot of microscopic and macroscopic paralell process (microbes inhabiting the forest bed have their own cycles as do flowering plants and migratory birds). The village however converts the forest into an orchard, by selecting a single species of fruit providing tree and weeds and clears the forest floor frequently to avoid subjectively unnecessary cycles. This makes the system less complex and more controllable. But it also has the effect of reducing the complexity of the village. Now that weeding and fruit gathering becomes the primary occupaiton of all, fortunes become less heterogenous and everyone synchronizes over activity, making it factory like.
2.Thus we might say that the idea of control over an environment might itself indicate a harmonization. There are often themes in culture where the latter is suggested over the former and a strong indication that the idea of control might be in itself futile. The idea of reducing the complexity of an environment by reducing paralellsm in the environment is the seleciton of certain process over others. Therefore while macroscopically more units of the same work is seemingly done, we might not say that order is increased. The idenity of a product in the forest is now vauge, eventhough we might group it with certainty (say as mangos or guavas). Therefore orderliness itself is a macroscopic machine which crushes a lot of microscopic processes. Likewise, even in the case of any mechanical arrangement such as refinement of components and their being coupled to create a mechanical advantage, the point is that the macroscopic order increases microscopic homogenity. 
3.The homogenity of the village is offset by the exercise of choice. The fundamental choice is whether to consume or save. That is to say, people decide on the surplus now flowing into the village as to be capable of being consumed or be stored away (in expectation of worse conditions in the future). This might not have been relevant where there had not been any surplus, or where a forest is foraged, the forager typically stops when is tummy is full and relaxes or frolics. But in a society that is excercising a control over the other system, there is a flow of surplus, which is capable of stored. The stored surplus becomes the capital. In such situation, where the future becomes worse, as it is natural for poulation to grow, even if the fortunes don’t change, the capital plays an important part. The choice is presented between whether an individual would like to borrow someones capital in order to wade through the worse time, in which case there is an obligation involved, which would be a transfer of reward in order that the past forbearance is compensated. This penalty is exacted in an unequal fashion,because of the assymetry of choice. In forbearance free will was involved while in the case of resorting to borrowing there are circumstantially strong elements. This behavioural trait of mankind might have been a powerful determinant of civilizaitonal growth. Hence, the individual set to borrow is presented with a choice to improve his technology so as to get additional outcome in the tough times (or with more mouths to feed) or borrow. He might choose to innovate and experiment and thus it might happen that value flows from mechanical advantage. In that value does flow from nature by mechanical advantage, one also expects that a value to flow from reserves which could be translated to technology. Thus, capital and technology are mutually transferrable and since the latter can appreciate, the former is synthetically composed to be capable of appreciation. 
4. The nature of understanding of the world in humans as we had seen involves the use of statistical gradients and their discrete breaks in order to identify distinct objects. But there is a third element needed in order to arrange these distinct components heirarchially. This is the notion of quantiles. If the stochastic search is capable of distinguishing between different standing crop, the discrete jump in height might provide an objective basis of classificaiton (such as height being a linear quantile itself). Thus, there is an imperative for reference to objective organization of the universe mathematically in order to be able to arrange components heirarchially and thus be able to overcome combinatorial thinking. This is strongly in suggestion that intelligence could be obtained from sampling randomly or statitically, that is to say by empiricism. Empiricism involves understanding the universe by sampling widely and also in neighborhood based refinement. But it involves theoretical notions in order for the things to be connected in the form of sets. This presents the everyday way of understanding the world, to sample and classify into components and arrange them heirarchially in an objective quantative basis. 
5. This is actually formalized in the scientific method, in that empiricism and theoretical reasoning is decoupled from feedbacks. That is to say, in common practice an individual reasons on the go. He is able to combine empirical observations (wide search) and experimentation of specific components (deep search) with some preliminary theoretical format and corrects it based on feedback obtained subjectively. However, in doing science, the individual is expected to present the idea obtained in such a way that it does not involve subjective feedbacks. It is presented to a reasonably objective audience and capable of being reflected upon as a necessary conclusion from the situaiton, irrespective of what it entails. Thus science promotes a method of feedforward control, where an ideal image of an environment is obtained before sending signals of control by predicting inputs (rather than responding to it). It is like braking down, seeing a curve ahead. This would involve a non subjective model of angular forces coupled with specifics of the system being managed (say their adhesive or traction specifics).
6.The major limitation of feedback based control is the problem of subjectivity, in which case only equilbria might be reached (due to the dependency on the states of both the systems of the observer and the observed) and not a mathematical truth be emulated in practice. In case where feedforward is relied upon, reference is made to pure and consistent forms of knowledge. For instance mathematics relate things by equality over scalar or angular coefficients. Likewise, it attempts to establish symmetry of the units by aggregation (which would necessarily involve a null space where nothing exists). In logic there is a stress on inequalities rather than equalities. An existential parameter is specific and null spaces do not exist. Thus, these two formulations of the ideal, abstract world helps in constructing ideal instruments which could provide future states of the system which would hold irrespective of time and place. Therefore, they could be freely scaled or transformed to suit specific situations (thus establishing their smoothness and continuity which are hallmarks of mathematical functions). The application of mathematical functions to situations allow for appreciating fixed structure of mapping between entities, which are smoothly transformed in dimensions. This appreciation of the whole, removes paralellism and simplifies (functional composition is possible) the interface with the world. 
7.We might also digress a bit to note that the idea of speculation over uncertanity is captured in savings, which gets transformed into capital if the risk does not materialize as foreseen. This capital hence is capable of freely flowing to finance technological innovation. It might be simply fruits or seeds. As we had earlier indicated, currency is not caused by the soverign will, but by the necessity to hedge or buffer uncertainties. The negative outcome is absorbed by the reserves, while a positive outcome allows growth. Thus growth is a side effect of this strategy of reservation to worst case complexity. Reserves which are not applied as investment in good times, but simply kept as reserves, is the hallmark of non living things. Living creatures speculate on the environment, reserve against worst case situations and grow by mechanically leveraging the reserve. The loop of not growing in suboptimal times and reserving it to be applied in optimal times, is one of control, by feedforward speculation. All control strategies are aimed at reducing complexity. Reduced complexity means simple machines which pump heat in a steady flow.  
Part 4
The idea of dilemma of control – the role of Computers

Where we start out is to state that philosophy is the study of universals. Universals truths are not possible in any scientific inquiry or normative inquiry. They are all constrained by the contexts and intents. We might also philosophy as ways in which we get the intellect to cooperate with the instinctive, or the innate. That is to say, while philosophizing, we invoke rational constructs, which helps us see the nature of truths, which could not be reached by material means of experimentation, because it involves either the observer or it is universal. It helps people to see things, which they already know, that is to say, what is understood sensually (not restricting to the five, intellectually comprehensible senses alone) to be supported by a transcedent intellect. That is why Socrates did not believe in documenting his works. He simply believed that he was teaching things that are already known to people, which they would anyhow realize (it being unique and singular) at any point in time in the future, if people were to put their mind onto it. Thus, he was in favour demonstrating what is philosophizing rather than philosophy itself.
Plato was also in support of the existence of perfect knowledge apriori. All the study of philosophy was for him, simply remembering the times of perfect knowledge when one was one with infinite goodness of God. Thus, Plato was supportive that the role of philosophy is only to enlighten us to the fact that we had already known and the distractions of the world had just clouded it temproarily. Kant, who was unsparing of the possibility of external observation in a Cartesian sense, where he proclaimed that conclusions from rational sciences are due to only the construction of schemas and axioms to support the conclusions, was still accomodative of the presence of apriori moral truths. This was in fact his subject of critique of pure reason, where he said that the rational process is made in self referential framework, while the real drivers lay outside and away driving actions.
In the postmodern times, we see that there is an emphasis on the nature of truths being subjective and highly contextual such as like doing the next good thing, instead of planning and meditating on what is goodness. But the availability of knowledge on the next good thing, for an arbitrary time interval does not logically preclude the availability of similar knowledge on goodness for a variable length of time, to varied people and varied times. This heterogenity of access to truth, eventhough the truth is one, allows, hence for people to gain greater authority over the truth over contexts and hence be capable of exerting power and authority on people. However, the problem with the awareness that one is better aware of the truth, distorts the truth itself and the authority and the symbolism associated with it, either in terms of rewards in material or spiritual world causes the truth that is disseminated from authority to be paradoxically corrupted. 
Thus, we might see here the importance of the rational method, to cite Ferrier, Whitehead and Russel, to construct philosophy from Logical components, so that one can frame it in rigourous formats. This ability to do so, allows the rational to seek the absolute truth, rather than being open to only short term rewards or survival in an evolutionary sense. Thus, there is seen an effective utilitarian case for philosophy, apart from being simply consolatory. This rationality allows an influence on policy and techniques so as to produce values, eventhough philosophy itself is an exercise in self referential way, such as enacting a drama or a story to lead people to the knowledge, they had already known and only strive to remember.
Engineering we might say serves as a moderator for peoples desire, or fantasy and the finitude and the definite bounds of scientific phenomena. Engineering allows optimization, in that some part of the wish is realized within the constraints of science. Thus, as people wish to solve problems and they are encountered with complexity, in the nature of small things leading to catastrophic results, as some small move encouraging a crime or accident, people have no way of knowing or avoiding such developments other than to rely on providence. In that philosophy enlightens us on the falliability of rational processes, people would seek the guidance of providence and praise its glory. That is to say, as one philosophizes and sees that the rational methods are intended to actually convince the intellect in helping see the apriori fact, that is realized when one puts ones heart to it, one starts paying attention to the finitude and falliability of rational knowledge. 
But it might not be that one has to entirely rely on divine guidance throughout. Even Descartes himself was propounding that people were endowed with a capacity to self determination and free will by providence. Therefore, humans might attempt to build techniques in order to encourage the situation where a person could with less distraction listen to the inner voice. That we also appreciate the localization of knowledge and context sensitive decisions. However, as we discussed such distributed intelligence might also give rise to oppressive structures, such as caste based structures. It might hence be a useful technique that such structures are weeded out. That is to say, if there be such oppression in a certain region and a central common sense says that it is evil, then centrally resources might be directed in order to form local groups and finance them by state budgets to serve as a vigilance group over such developments locally. Thus, we see there is a promotion of monopoly and centralized knowledge in order to achieve the ability to appreciate the falliability of such knowledge and delve on the conerns of the soul.
But such rational and singular constructs, themselves are rooted in complexity. A government that believes in singular truths, such as to have a motto of truth alone triumps still relies on multiple verisons of truth, by checks and balances and paralellism to stabilize the setup. Likewise singular pursuits of truths ought to coexist with local and plural perceptions in order that the narrative of singular truths remain stable and unbiased. Hence, we might say, philosophy is difficult of application technically, mainly because philosophy is not definite knowledge, but speculative. One has to argue on the basis of invoking Plato or Aristotle or Ferrier and not on the basis of findings in the material world or deducing from theories. But specualtion has its place, as was discussed in Russels Kettle analogy. The point is that while philosophizing could not happen to be definite knowledge, due to its speculative nature, which itself prevents philosophers from agreeing among themselves and in a more contrasting manner the disagreement between people who idealize and who are pragmatic and content with practical reason. Therefore, this nature of philosophy prevents development of theories so easily. But in fact they had been important in the development of theories as was Cartesian Geometry and the important contributions of Leibinitz, Aristotle and people lke Bentham, Mill and Hume. 
Thus, philosophy inspite of its speculative nature, leading to divergence, has as a convergent core, the need to reflect on the falliablity of knowledge and the need to find such apriori truth, by describing the singular problem of people needing of ways to be reminded of what they are aware and keeping off distraction in the form of intrusions from the physical world. Hence, all philosophy might be zero sum, self referential, but it is an exercise in strengthening the brawn of the mind to be aware of the inadequacies of reason and hence the need for complex roots which could serve as infrastrcuture for rational superstructures. Thus, if rational superstructures of a society should fail in edge cases, such failures are durably absorbed by the population in charity, consolation and even as a challenging force for further inquiry. 
We might summarize this necessity for paralellism and complexity as the root for arriving at singular bodies of truths in the idea of liberty. Thus, we say people be let free to think in their own sweet way, inquire from there, desist from inquiry if they will so and listen to their inner voice, understand the importance of apriori moral knowledge. We see that the principal problem in their doing so, is the emergence of authorities, where the embrace of liberty is heterogenous leading to emergent heirarchies. If there exist a singular structure, a way of rationality, which could be deliberatively stabilized, along with its antithesis of plurality, we might speculate that both could coexist in a synthesized equilibrium. 
We had also discussed the work of Ferrier in asserting the inseperable cooperation of the intellect and sensuality in the production of knowledge. Hence, we might say, that it is for philosophy to produce such techniques that this balancing act of maintaining plurality and singularity together (where everyone agrees on subjugates oneself to what is right, as an universal pure form knowledge) is possible. 
We might look at the theory of control systems in order to create a technical layer to understanding the balancing act, or optimization over this paradox. Theories could be developed in order to correctly interpret phenomena in order to be conscious and rational about understanding the way the paradox is being handled emergently and thus generating instability. The use case of philosophy is hence to produce such theories as to allow a rational understanding of the phenomena to rout out what is simply an emergent and populist stance and thus falsehood. It uses techniques, such as the scientific method (which has its origin in the philosophy of logic) in order to rigourously understand what we attempt to know and thus control.
We approach this problem as a problem of control, which is unique in our philosophical approach, yet convergent in promoting the principle of approaching the pure form of knowledge to such as extent, so that there might be free wheeling attempts in a paralell sense, that could be carried out withough people having to fall in line to heirarchy without their agency, in order to satisfy their biological needs. If they do enroll under emergent paralell structures without coercions, which the singular structure ought to check, then the technique is succesful in promoting the environment of liberty through true knowledge. It could infact be seperated from the material side. If an equilibrium of this nature is attained, a society might be said to have reached a high degree of development (in terms of existence of pure forms – state, institutions and the general public), irrespective of the level of surplus and material dimensions to the development. Hence development might be framed as a purely idealistic process. However, it also happens that liberty has in its fold, the drive to inquire and solve material problems to be best of the ability of people. Hence, the equilibrium ought to empower the people to realize their full potential in seeking the truths and if they are working on it to the best of their percieved ability and the truth is becoming more and more visible, then development is reached.
If we look at this situation in terms of the theory of control, we see that there is a preference for singular and clear control which is the prefernce of the singular source of truth. That is to say a feedforward control mechanism around a central plan becomes possible if the singular knowledge is constructed to such an extent that the presence of paralellised pursuits is able to consensually relate to it. The other side of the central control, is a self stabilizing system, of equals who are entitled to opinions of their owns, adjust their game strategies, form transcient willing contracts in order to genreate a substrate of understanding without any coercive structures. All coercion is monopolized in the obedience of the singular source of truth, which itself is capable of being deliberated by the rational process. The resulting stability presents a situation of feed forward control that has its goals specified to such an extent that it is capable of being supported by an emergent consensus among completely free individuals. That is to say, the emergent nature of central truths, should exactly match the feedforward plan as deduced from the central truth. The meeting of these two origin points provide the dialectic that ensures that the truth is in fact real. In the former case, the belief is that the truth is already present latently in the mind of all organisms, which they ‘prayathan’ or strive to align their biological need close to (see Kants essay on the helplessness of respect and guilt – The analytic of pure practical reason). In the latter case, there is a situation where the point is that such universal truths (or natural attractors) anyway emerge in a situation irrespective of there being a conscious striving therefor.
In feedforward control, some people subject to rational deliberation are able to idealistically and abstractly generate pure truths or almost pure references thereto and use it as a plan to which the individual in the liberatarian backdrop would essentially comply with, and presents it. The control works on a constant ambivalence over the way the plan is a representation of pure truth and as a result, the plan itself recieves a reflexive feedback by means of critiisms, developments of divergence and ritualism, counterculture in the private, liberal domains all serving as feedback to the rational planner, so that central control could be adjusted. On the other hand feedback driven control, of which the market represents a near perfect case, does not rely on plans as a yoke over the individual will. It is the feedback, anonymous and  incapable of being tracked to elite personalities that provide the control signal. There is no separate channel to produce feedback and reflexively control the market itself, other than to respect its incoming signals and use reserves to attempt to nudge it away from its inertia. Thus, the market has a dynamic equilibrium in the nature of inertia, which sometimes is detrimental to the liberty of the people, say a nostalgic watch repairman, and removes resources from him. His only option is to sell something else (such as the appeal of nostalgia itself) to dynamically adapt the feedback in order that there might be generated synchronous signals from other parts of the whole is generated to let him continue doing what is in his free will to do. If the market does not listen, he does not have a course, whereas if the state precludes him from doing something, he could still debate it deliberatively. But with the market, his options are not to meditate on the goodness and badness of people but to adapt to the feedback.
There does not appear to be an inherent problem to pure feedback driven design, that the world is culturally embracing in the present point in history. One might see this embrace in neoliberalism, a discouraging of planned economies and policies, a general preference for consumerism and packaging and marketing as reward packets, even spiritual experience and so forth. The design of feedforward and deliberative control is argued to be superior to the feedback driven control on one respect. That is to say, the ability to judge what is good and what is bad and the idea of pressing oneself to aim for being good all hinge on seperating value based thinking from free thinking. This attempt to self perfection in aligning to the truth and this attempt to appeal to goodness and even the freedom on be ambivalent and continously exercise in building stories of goodness is something that becomes irrelevant in a completely feedback driven model. If there be a controller and a subject system in a reflexive loop of feedback driven control, there is nothing but coevolution and symbiosis and the whole system monistically converges to evolution and evolution is not idealistic progress but only a material one. Hence, it is a crisis of idealism and the crisis of being conscious that is being meted in the preference for feedback driven control. In the past, grand narratives of truth and authority has had records of instability, of being catastrophically falliable that motivated this preference for feedback driven control. It was argued that the licence raj itself was feedback driven, in that the plans were made not to the satisfaction of the intellect of the bureaucrats but of their pockets. But that is universalization of banality and the mediocrity of human nature. There does remain idealistic people in the bureaucracy who were visionaries. Hence, a categorical stand is not possible, a dualistic one, where feedforward ideation is made and a feedback by cultural rhetoric which might genrate by populist force, a deliberative engagement is what is unbiased. This is the point of philosophy, to avoid the dramatic shift that we are experiencing culturally. It is rarely that contemproary populations are ever aware of that they are witnessing a historical shift, but we might endeavour to it. We in fact go ahead with this inquiry with a premise that computing presents a shift in history, in the same way as enlightenment once did. It we see presents a new way to approach reality. 
Thus, the development of a society might be seen as a way in which it buffers the plural emergent truths and the singular universal truth. Money had been a powerful buffer in its times, liberating people from the constraints of space and to a lesser extent time (they could save and not work for some time). 
Computing as a buffer is able to liberate it from the yoke of time as well. That is to say, let some train driver be driving a metro and let three of them rotate in a day shift. A computational scheme might allow a more fluidic assingment of tasks, which might reduce the bench strength. But the heuristics of each driver is lost in the process. The knowledge would then have to be encoded on to the machine. The computer is a machine distinct from any other. No machine has as a possibility that it might fail quietly. All machines such as complicated gear boxes fail but it is an uncontained failure rising from an individual part. But the computer by deisgn has failure situations hidden within it, as the halting problem. It is basically just a switching system, where the circuit is encoded by the business expert (the coders simply interpret it technically for them).The experts then run the static model by different sequence of events (which are like thowing switches, either by clicking screen links or by sensor inputs) to produce unique end events (like acclerate/brake inputs). Thus, we might say that heuristics had been reduced as a set of reactions to a finite set of events. If all these event pathways could be encoded into a circuit then all possible switches could be handled gracefully by the computer and thus heuristics had been layered over the machine. A machine might as well fail under the right set of events, to which it had not be tested for, but it is proven by mechanical connections. It ought to work under specified conditions which are finite. But the number of parameters as events increases in computing systems, making them practically unprovable. That is why they can emulate heuristics and risking failures and stalls. 
The computer as a heuristic encoding machine, that releases from time and space (in a gig economy say) of people engaged in productive work (obeying singular truths) is what is appaling to the conservative. The conservative involves division of work and free time in a village blacksmith style, sotic and epicurean at the same time. But a seamless merging of work and recreation and of space and time, leads to situation where the computer becomes inevitable as a holder of heuristic information. It is an enabler of the cultural shift. But as we said, heuristics encodes human skills in problem solving referring to some apriori knowledge. The computer has no apriori knowledge but attempts to reach it by emergence. It is hence purely feedback driven (see the recent success of feedback driven control in google’s search engine), experiential and event driven. 
The ultimate failure of a control system is when it induces a wild oscillation, that makes the system so unstable that it breaks apart. It is like the allied axes divide in the world war. It happened when central narratives of control (of libertarianism on one side, of the essential natural truths on the other) lead to instability.Past wars were fought because of the impossibility to agree common grounds due to the difference in beliefs that were interpreted as reasons for material action, to destroy the falsity. It was not a fight for resources, but against falsehood, which the two sides had different opinions on. There had always been wars on ideas (even of leadership or cults in pre rational times), but they had been well appointed. The development of industry and fragmented modernity, set the idealist machinery unrooted and prone to clashes. In a democracy thus war is less possible. But free markets promote a less dramatic violence in subtle powerlessness which is worse than drafting to forces, in pollution and creep of wasteful cycles. This kind of instability is when the system stagnates and there is no notion of control to make sense of stability itself. Thus, attempting to control renders the system instable, at times, but a clever and dynamic control might help. Not controlling it, would be nirvana, but not everyone is ready for it. One still thinks, techniques can work in better reaching spiritual enlightenment. A local nirvana would only make things worse by conflict, invasion and the like. 
Thus we deal with what is possible, or what can exist as a truth in a given theoretical model. That is to say, if given a,b,c is d exists is the question we propose. Lets say that a control system signals plans and deliberates on feedback from the system and then it stops giving plans and allows S self stabilize by emergent plural authorities, which converge over natural truths. It might then present a greater surplus which is the natural optimum. However, it is possible to produce spikes in performance by singular authority, as did Ghengis Khan and they would raid and thus oscillate the peaceful system. Thus, the natural state itself is a dialectic between firm control and feedback. We then, in rigourous terms study the gradient that emerges as systems in neighborhood promote provisional plans and those that hold short lived provisional plans. If we see this dialectic emerge in simulation, we might suspect that dual mode control is superior.

A Brief Note on the inquiry 
Given that we had said knowledge is synthetic of the apriori truths and the pluralistic versioning, there is a need for reflecting on the current state of history in perfecting this balance. The enlightenment was biased towards the acquisition of knowledge from apriori truths. We interpret computers as biasing the balance towards seeking pluralistic local truths. Computers are switchboards which work closely with real world events, material and not ideal hence. 
On the method, we might interpret computational intelligence as arising from the construction of automata with finite states, which could be switched on or off, depending on the previous state and events occuring. It may have a very large number of behaviour from a finite set of starting points, states and end states and possible transitions. That would get us to inquire into automata theory to be able to construct a formalism of emergent knowledge, as well as game theory, because typical intelligent conclusions arise from paralell mechanisms, such as competing cells voting. Where an intent is not available apriori, one might be constructed from an internal game. If the game needs a payoff which can be stable is a point of inquiry, for which we need foundations in game theory. 
In our case, we see mathematics deals with singular subjects. Everything is an equation, only because it is framed in a line, field or hyperplane. Everything such as y=2x signifies a relationship about a null space, without which such equalities are not possible (Logic deals with inequalities mostly). It can analyze functions, which are continuous relationships between domain spaces and the rate of response of functions by calucusl. Mathematics thus finds itself difficult to accommodate objects which involve paralellism, such as non equilibrium thermodynamics. Systems even if plurally recognized are expected to be in equilibrium with each other, either dynamic oscillatory or static, but not involving random hunts for equilibrium. Cybernetics and Random Walk theory might help frame our notions of having two systems capable of formalized. Wieners Non linear problems in random theory is a good place. 
We need a formalism of paralellism, because we are looking for theorems which can be proven, instead of philosophical speculations alone. Such theorems could be built based on control theory. The dualism that we seek in one system trying to control another, as with Cartesian dualism would need to be contrasted with self stabilizing systems and their mutual efficacy or stability compared. A dualistic stability theory. Our research might look at personhood of artificially intelligent agents, in this work as a particular policy and technical problem. For this, we would hence need to look at proofs that the AI agent would need to be placed under feedforward control, in the same way as humans are, in order to avoid normatively undesirable results in political theory. For this, we would need to look at the rigour of mathematical control theory and the mathematical defintions of free agency. 















Possibilities of SuperIntelligence
======
1. The notion of truth, whether as something that needs to be reflected upon and accessed intuitively, reinforced by empiricism or as being something that would emerge in the natural course of conduct of the normative affairs of the society, postulate truth as being distinct from the phenomenological interaction between people. Phenomenological interactions concern rewards and may cancell out often. They are considered immediate gratification without any inclination towards truths and untruths. The concept of truth involves these extremes, of what is utter truth and what is not and thereby appoints authorities to regulate the dissemination of the truth (a crime is an expression of untruth, rather than a physical wrong that needs to be rectificied). It is the argument of the postmodernist that the truth is a construct of the authoritarian and a clever trick, that has accidental origins in history, which needs hence to be rectified. Authorities and truths in postmodernist philosophy are not natural.
2.Given that authority is inseperable from truths, as being custodians of truth, we might speculate as to whether there existed a golden age of no authority or that authority and its usurpation is a part of the human condition. The human condition itself is strongly sociological. All the philosophy of Kant in the agonizing imperceptability of pure forms and the distortions all belong to the realm of the society. An individual free from social norms is capable of organizing his world around such distortion, by the application of reason and contemplation of a graceful orderliness to things that are to essentially remain mystical. But even without social taboos people are guided by moral sentiments, which arise from the gut and might be termed as inherent to our perception of a long term balance. 
3. There is always needed of an individual of justification if he were to seek short term rewards in favour of stability of the system as a whole. The system itself might be social or even involve animals or inanimate things. Morality is a tendency to fairness and stability and careful regulation of generation of spontaneous events to be in synchrony with natural events. That is to not give into temptation and reap what the woods could sustain. The use of machines by themselves is not immoral if they are not to upset the stability of an underlying system.
4. Morality could be deferred by a narrative of progress, where a definite end might be the settling of things all that had been unsettled in the progressive march. Slavery, Animal mistretment and aforestation had been justified along these lines. Moralists tend to appreciate the need for individual reinstatement of balance at a personal level, rather than embarking on a collective, say national project of progress. But once such projects have come to be in a phase of history, it becomes militaristically dangerous to dissent.
5. Given that moral sentiments exist as being beyond rational comprehension or expression, the individual might seek cooperation of others in establishing stability through civil process. Hence humans cannot help organizing their society in a civil manner. They could not leave a self evolving social organization without reflection, control or plans. But the rudder of social organization is in postmodernist theme seen to be an intersubjective emergent signalling by appreciating truths and condemning untruths in an adhoc and in the stir up in the nature of storms. There seems to be a natural difference and historical circumstance in which some person is able to get a farther view than others in a given context, in which case they might rise to be custodians of truths (even in a business organizational level) and accrue authority. The situation where the truth arises a posteriori, as multiple people anarchistically compete on immediate knowledge, leads people to trust the need for a plan, rather than course correcting the march as we go. Plans could integrate with machines. Plans allow for description of ways in which phenomena could be interpreted and combined to produce surplus. This surplus or value becomes the value proposition of the Plan and hence authority and of objective truths. This might be a materialist interpretation of authority. What started as an urge of moral sentiments, found that the generation of value might be an ultimate moral rightousness and legitimate action (to cite Atlas Shrugged). That is to say Artha becomes the ultimate Dharma.
6.The question is whether the value outputs are merely a side effect of the pursuit of truth, which shall be strictly used for satisfying the biological needs and allow for a culture of seeking truth, ritualistic cohesion of the society and moral contemplation forms the foundation of the moralist debate. The economic enterprise hence is instrumental in allowing individuals to seek their moral truths and hence not tread upon it is a metaphysical proposition.
7. Hence the discussion is about the dilemma of primacy of moral sentiment versus one of utilitarian pursuit of value as being rewarding to the morality as well. We have seen both in practice, as neoliberalism bailed out numerous people from povery for the latter case and the case where slavery and imperialism was abolished in the case of the former. The fundamental problem with the approach of utilitarian pursuit of value is with the emergence of value free truths over time. These might be mythologies, but mythologies bear strong resemblance to universal natural truths. 
8.In the computational society, there is an embrace of value based truths. Information is the material of value, in which sense authority is actively juggled and traded. Decisions are made based on the complex chain of value proposition on the network without regard to rules of authoritarian safeguard of truths or moral sentiments. The use of the computer as a language that is capable of expressing value propositions can frame questions of such high nestedness that it could only be interpreted by automata due to their numeric ability to collate these rules. The decisions becoming increasingly numeric and less qualitative. Numeric decisions made by automata are hence intractable by human agents attuned for qualitative evaluations. 
9. Automata essentially end up making a great deal of decisions and hence wield authority to the distributed, practical truth web. The non verifiability of the automata agents might cause a central regression, where the automata search (there are choices involved in searching for open ended expansion of value propositions) for value closer to the median. There is no way of knowing of such central regression in phenomenological terms. One might witness a narrowing down of choices pervasively and marginal value outcomes stagnating. But there might not be ways to control the system. 
10.It might however not react jealously to abductive actions. An agent defecting the system if able to strike a value stream that is outside of the purview of the system and hence start starving the system, he becomes capable of controlling the system by committing it to sepcific narrow ends. There might be a specialization of intelligence that is controlled by an external system. But the external system must not only possess the qualitative context but also the numeric ability to evaluate, tame and control the system. Hence, a computational consciousness relying heavily on the internal compliance of biological systems might be a transhumanist future. It might materially alter our physical constitution, but that might not be material, if a perfect simulation is possible. What is material is the divestment of consciousness, where the ultimate parochial needs are projected onto newer domains involving high frequency computing (just like the existential dilemma of individual cells are handled as political domain problems by humans). 
11.Open ended domian extension of knowledge, building upon basic facts in a self referential manner is a possibility. But knowledge might also be viewed as an approach to the realist notions of truth, of an essential order to the world. The approach would mean a reduction of noise, rather than an absolute discovery of elements of truth. The simplest creatures are aware of the workings of the world, of universal order, but with a lot of noise and imprecision. There is a possibility to obtain refined models of reality, or superintelligence, where the sparseness of reasoning and the numeric abilities are enhanced enough to reason much deeper and reflect much deeper. If this is fundamentally different from parochial intelligence, is hard to say. The realist argument might be that it is an emergent consciousness that continuous progression, for progression is singular and perhaps anthropic. An altnernate narrative is not epistemologically approachable. If there be other ontologies, or paralell universes, then the transformation is uncertain of commentary.


Note: All that is defined as surplus and as the creation of infrastructure might be interpreted as the generation and the employment of capital. Here we had indicated that knowledge is something the shape of which is perceptible by a mix of epistemology and poetic awareness. It is the increase in the density of the knowledge that is accomplished by education, reflection and increased intelligence. The increased density allows one to comment upon the actions of those entities holding sparse knowledge models as phenomenological, without being directed to some universal overarching goal and hence capable of being utilized towards a larger project. This might be seen in slavery and in domestication. The incremental density is essentially a zero sum process, pouplating about the central nullity, evenly on all directions.
What we had held is only a dystopian view of the network society
--

Recalling the social contract
To continue in the tenor of the argument of parochiality of intelligence, we see are eventually lead to the point that there does exist dilemmas of resolution between higher truths such as dedication to art, of honor, of love and those of immediate needs of biological and ambition such as seizing control of a certain resource. These dilemmas are meant to be a part of human existence and are to be solved by the individual concerned, as a step of redeeming himself. In other words this cannot be institutionalized. 
But this point of dilemma as being perenniel is uncomfortable for many. In the past, there had been movements to reclaim the dilemma from being institutionalized but they had not been successful. Say the Bhakti movement in south India a thousand years ago or the recent counterculturist hippie experiments. Both these were bold moves to commit to basic, intuitive higher truths without the guidance of authoritarian agencies like those of vedic scholars or in modern times the authority of religious and business leaders. But in either case, it lead to collission with the world in general. The world with its challenges would not let a person pursue his hearts dream contentedly. Invasions shook up the bhakti soaked medieval south India. Likewise the hippie movement would have to be constituted by bums (the dharma bums - kerouc) looking up to the general society for being fed and clothed. An attempt to assert an independent existence, lead to more mortal problems, leading to the need of authority and thereby an implosion in many cases or a dispersion by external forces. 
Thus we might provisionally say that there exists a need for a man to work (like Candide or the village blacksmith) to earn the luxury (or right) to romanticize, reflect and redeem himself. The postmodernist attempted to merge these two and deny the dichotomy. Hence, it let go of the metaphysics of the need for redemption itself. It attempted to demonstrate that the dilemma itself is constructed by instruction and conditioning in a society and not natural. It however, met with various degrees of success. It mostly took off not in its ability to retire the dilemmas but with the catalytic uprising being the critical mass required to transition into a network society (Castell). The network society as we might characterize the information society with knowledge workers, involved the constitution of transcient fleeting authority and in effect merged morality and selfishness, without the concerns of authority. In the absence of authoritarian burden to rational solutions to mundane problems, there was hardly any concern to keep the dichotomy intact. Everything flowed as one monistic, immanent (rather than speculative) metaphysics. The result, we might say that natural problems do present themselves at various points of time, leading to the emergence of authority, even if of superintelligent agents, as we had seen earlier. Besides the divestment of the dilemma to institutions rips humans of their basic right to yearn and hope.
We might now recollect the idea of instrumentality of the society as regarded in the Social Contract of Rousseau. He is considered the father of the romantic movement. Just as we had been reflecting, he considers the duality of the right to romantic day dreaming and the necessity of toil. The former he calls libertarianism and the latter of the necessity for social cooperation. If a man could handle the duality either in person or as a small family or clan, it might not merit a metaphysical reflection. Only when there arises authorities to restrain a mans liberty for solving problems in the state of nature that the need for a public sphere discourse and a holistic reflection of the metaphysics thereof arises. Hence, Rousseau postulated that the extension into the civil society was a development out of an explicit contract dedicated for a set of specific purpose, rather than the more recent interpretations of society as a spontaneous organism (Durkheim). The hypothetical contract had stirred many a revolutionary rise of great deal of violence, but it was the product of the tension of the times, rather than the philosophy itself. 
The social contract is also a legitimate extension of the Cartesian duality, in the seperation of the observer as being indepedent and endowed with a soul. He rejected the organic nature of the society and considers individual observation and agency as material, reflected also in Rousseau's philosophy. In fact we might trace this monotonic rise of an awareness of a need to reflect on the nature of civil engagement from early modernity till the early pragmatism (in fact idealism and pragmatism was placed in a continuum in the critique by Giladi). Paradoxically this was accompanied by an ever increasing integration of world order over norms and in the decreasing value of qualitative truths powered by technology. There is expected to be a strict seperation of technology being applied to natural problems and the use of technology to buy the balm for the dilemma of mankind over temptation and virtue. Whereas over time, one sees the value proposition being the universal quantifier of academic direction (as lamented by Weber) and of artists and spiritual leaders. 
Hence, we might frame this as a crisis. It could only be reinstated by dispelling the ostensible contradictions in the modernist project. It ought to be seen in the light of a strict formalism to value based activities which is loosely coupled to the indvidual drive to address the dilemma of morality. It might have been the theme in Ashokas philosophy two millenia ago. Ashoka enacted edicts with scarcely any mention of spiritual authority, not even Buddha was mentioned in his edicts. Hence, the concept of dharma might be narrowed down the reinstatement of individuals to pursue the resolution of their dilemma and the strict seperation of artha in the pursuit of duties of solving natural problems and generating surplus, limiting it to stricly logical and verifiable process, governed in the spirit of the social contract.

Possible works: 
Evolution of infinite loops in large networks. Cyclical redundancies as an instrument of control.
============
Defection is minimized by inventing a game of crossfeeding as concieved by Mol. BioSyst., 2014, 10, 3044--3065.


Theories define 'necessary conditions' for materialization of phenomena, but not the sufficient conditions, for which causal interconnection between phenomena is required by empiricism. UV and oxygen might be necessary conditions for the emergence of ozone but the arising of ozone is not explained sufficiently by the combination of the causative agents. There is an inherent stochasticity involved due to which the set of sufficient conditions (to reproduce the event) might carry a stochastic noise. In such cases where the stochasticity is a cover for latent factors awaiting a complete enumeration of the elements of the set of sufficient causes. Empiricism helps in establishing temporal sequence after excluding other factors (in controlled conditions) which might have latent effects. Empiricism is hence a connection of phenomena, finding its root at the experimentor out of his free will performing certain actions. The intentions and biases of the experimentor, as well as the feedback loop flowing from outcomes is not generally considered material.
Connecting phenomena by means of a stochastic belief network or empirical sequence of phenomena is a bayesian network. If causality or theory could be inferred from was discussed in Pearl and Verma.


Decisions one would regret - Regressive tendencies in network based intelligence
======
It might however confer dynamic advantage which might be balanced at crtical points of regression.


3. Consolation by reflection as being an essential part of mental health. 
======
The cognitive roots of mental distress, where the perception of a person is unharmonized with the general population arises due to the difference in the cognitive objectivism over the degree and place of randomness in the world. Thus, mental health is not only a construct of norms, but of the ability to percieve the objective common reality, that 'is'. 


Social Theory of Computing
An Instrumentation perspective of Computer
===
A theory of computing as a cyclic truth deducer (and not a machine)
==
1. Science is a way to establish necessary and sufficient conditions for phenomena. Phenomena are crude observations, which are deemed to exist by virtue of their observability. The necessary conditions for the phenomena is established by a logical necessity following a certain principle. A good example might be that there existing a postulation of positivity and negativity and an empirical confirmaiton of electron's existence, would naturally cause the scientist to predict the existence of positively charged particles in the atom. The presence of the electron constitutes the necessary condition for the existence of the positively charged particle. But it might not be sufficient, say what if all the atoms are negatively charged and the balancing charge exists somewhere centrally at a distance. Sufficient conditions are established by empirical verification. 
2. Empirical verification is essentially statistical. Crude observation of phenomena which triggered the inquiry might consist of confounding elements, due to the disturbances in the environment and the intents of the observer. In an experimental setting these are filtered out, so that verification could happen. But absolute noise free environments do not exist and hence all observations are statistical to a good degree. In medicine say a 95pc confidence interval is sufficient to establish the scientific explanation of the phenomenon (a hypothesis seen as H0).
3. Science relies on its six conservation laws and the empirical law of non spontaneity of order creation (law of entropy) to establish necessary conditions, which might itself be expressed mathematically or geometrically. Thus, the discovery of electrons do not supposse that orderliness is established spontaneously by the atom seeking some other positively charged particle in an intentional manner.
4. Accordingly order creation ought to arise from machines. A machine is essentially a heat pump if it creates order. It simply removes the heat (or homogeneous randomness) from an enclosed space onto a heat sink (say the outer space). The act of creation of order by observing the heat (say trapped in the atmosphere) is designed in the machine by a process of mapping discrete events (say the photelectric threshold) to other discrete events (say the critical energy for ATP bond creation). The machine consist of switches and gears which moves and meshes with other gears to create this effect, in a simplistic mechanistic analogy.
5. Complex machines involve paralellism, in that there are multiple components which do not strictly collaborate, but compete or rather orthogonal and coexist on such pardigm. Paralellism would hence involve a multiplicity of systems which optimize locally. The second aspect of complexity is latency, where actions get absorbed into some local sink, which on reaching critical threshold might explosively propogate these actions. From a view point, this latency is akin to stochasticity, in the sense that certain actors (rather than components, since now there is local optimization due to paralellism) hold their appropriate actions till opportune (subjectively) moments. Hence, we might say stochasticity and paralellism designed into machines make them complex heat pumps.
6. Complex machines generate phenomena such as biological systems, which could not be traced to necessary conditions, but only statistically mapped to certain activity in different parts of the system. That is to say they might be modelled as event driven systems where critical events might set the course for certain flows where certain phenomena are anticipated (and not others, we had earlier postulated that phenomena are deemed to exist if we observe them). Hence, divergent outcomes arise in a given model of a system when mapped to the 'real' state of the system (which is unobservable, since the system does not have a central state). We only get to read portions or angles of the system from our model. 
7. As against classical mechanical modelling, these models are state machine based, where for a given cycle certain pathways are traced. The machine or model are hence cyclically convergent on essential truths about the complex system. We presume the complex system is in fact centrally orchestrated and has stochsaticity and paralellism as instrumental features only. Thus, computers look for emergent order in a system, by allowing different sampling from the system by different users over cycles. Likewise the FSM architecture also involves the temporal extension of decisions, in its 'while' loop. It also allows collaboration not only of human agents, but over a network of systems. Hence whether it involves multiple humans or other automata systems, it might be termed as a network.
9. The objective of science is to discover the truth with respect to the originality of a phenomena (rather than it being an effect). The objective of computational networks is to be able to approximate the central intelligence of a system. The utilitarian argument of classical mechanics is to be able to model natural machines, so that these can be emulated in more perfect engineering endeavours (say emulating plant hormones with nitrogenous fertilizers). 
10. The question as to whether utiity is the end of science, becomes a philosophical quesiton, since we are examining the phenomenon of doing science itself. Any phenomenon to which we are party to, triggered by us (humans) and appreciated by us, arise from motivations that transcend immediate concerns of survival and being 'driven' by evolutionary forces. This transcedental drivers of actions are the subject of philosophy. Philosophy is hence beyond empirical observation, becomes it does not seperate the observer from the object, but it is rather reflective. It does not presuppose absolute soverignity of the self, but rather predicts that some essential objective drivers ought to exist that could explain our actions. Such rules might be beyond rational deduction from higher truths as well. All that might be done is to speculate.
11. Speculation might be legitimate, since it might uncover the objective core of a seemingly random crude observation. A speculation is not falehood, only an unbased truth (for which no science - or necessary and sufficient conditions exist). It had been discussed in Bertrand Russels Kettle paradox. Say the various explanations of the Fermi paradox is highly speculative, but given the complex and historical nature of interactions between life and the environment, one might find a great variety of possible speculative pathways. Some of these speculations might be hypothesized and worked upon scientifically.
12. Speculative truths which are not submitted for scientific queries are generally attributed with untruthful motives. But science is not an absolute truth as it might be speculated philosophically. Thus philosophy is disruptive of the normal conduct of affairs of humanity. But modern science originated from philosphical speculation of Galileo and Descartes. The only way speculation can be avoided to undo and backtrack is by the narrative of progress. 
13. There does exist systems which are highly intractable to science and thus making the profitability of engineering innovations in those fields low. These might be economic, biological or political. The central tendencies (the core of central intelligence we talked about) for specific problems in these domains might reveal a mechanism where certain events are mapped to certain other events by design and these designs might be refined and emulated in engineering. This is the grand project of computer networks, where events are mapped by local optimizers (by encoding to partial recursive aximoatic systems) and let to interact with stochasticity and paralellism. The emergent central tendencies are picked up evolutionarily where certain pathways are strengthened by feedback. This leads to an emergent direction of progression of orderliness.
14. The fundamental program of civilization (seen as a network) is however a more plausible query of philosophy. If it had been encoded statically signifies the presence of an apriori truth system. An imperative to embrace the steady gathering of central definitive rules (with additive noise) signifies a definite preference or program. The search of philosophy is to see if there is a contradiction built into this program, as when the program becomes self limiting, such that the drive could be sabotaged by the driven and perhaps reversed. This speculates the critical point where the universe does not allow further progression of order gathering and pushes it back by philosophical questions over the model. 
15. Computer networks allow for additive truths, which would hence act as filter for noisiness and reveal central logic to a system, particularly systems involving humans (as a complex system). This is intended to allow for its refinement and hence progression of orderliness, such as terraforming other planets. Philosophy questions this apriori truth of progress. If there be a materialist program to seek out progress, then is there a critical point when the frustation with meaning (on the ultimate falliability of the material progress given infinity exists) might lead to stopping vain attempts. A stage of philosophical development might hence consign human progress as instrumental to sustaining (given intrustions exist from less intelligent creatures and the universe itself) a campaign of meditation and doing zero sum things or more meaningfully to reflect and contemplate glory.
16. Material progression is still a pertinent and immediate concern of civilization and hence philosophical ruminations of its demotion to an instrumental status might not sell in many parts of the world. Thus philosophy is an absolutist or idealist view of the world, as oppossed to the normative view.
17. In this specific idealist view of computer networks, we might see that computers attempt to involve the observer with the observed in producing a world view, normally a subject of philosophy. This worldview emerges as specific behaviour rather than text encoded truth statements. This view has been discussed in Cybernetics. In evolutionary progression, the subject and object interact to produce persistent behaviour that is no longer dynamically explained by the actors, but are fixed onto to the interaction context.
18. This persistence of behaviour is what defines the uinversal truth to a system, as rules to a game. We say that emergent behaviour beyond critical thresholds might encode itself as definite rules (such as eusocial heirarchies). These rules are intractable to the subjects due to their being encoded with greater degree of noise. Noise might itself be subjective to the observer, a lot more randomness exists to an unintelligent observer such as a dog, than a human. Both possess fundamentally similar contours of reasoning and derivation of meaning, but humans differ in the ability to reason more densely, such as to be able to hold a large memory.
19.Lets take the example of sun rise and sun set being a phenomenon of interest being observed by a human and a dog. The dog sees it as a complex system due to the clouds dimming the sunshine. A long memory might help smoothing the noise to the signal. Paralellism arises from not being able to serialize a situation in high frequency. This in the case of cloud movement would involve serailizing everything even a butterfly's flutter (and farther on, since earth is an open system). If paralellism is essential, then analyzing a complex system is independent of processing power, NP Hard would be NP Hard irrespective of the observer. Minor improvements might be done, but statistical noise would be absolute. However, if one is able to observe a system in isolation, by filtering noise using adequate memory of stable states and oscillatory noise, or one is able to observe a system by physically isolating it by adequate force one might be able isolate components, but the components themselves produce interesting phenomena only holistically. Thus we might say complexity is real, arising from systems interacting with one another, isolating and studying them would not help. Likewise the exponential wall of computation, forbids any increase in intelligence to only make a nominal dent in the construction of meaning of complex phenomena. That would question superintelligence possibility as well (holding on to the serialized definiton of intelligence).
20. That is probably the reason why a lot of knowledge we have is self referential. It simply simulates the universe in terms of known entities and the emergent entities would make better sense in the simulation than in the real world. Hence monsoons are simulated as market prices, compensated by finance hedging and so order is produced from a complex interaction, by additively cancelling out linear actions. The net meaning of the system is attempted to be read. The meaning of monsoon is a certain confidence level of rain outcome, which could be buffered from a reserve to produce a well behaved simulation. Hence, buffered adaptation reduces complexity from a system. It is in effect enclosing the system in a container which is controllable.
21. Buffers are automatic noise observers (even mechanical springs are such). Question is whether a system could be stabilized from within rather than being enclosed. If a buffer is placed within the system as an observer of signals and operating on cancelling them out, such as a good distribution mapping intelligent system ( an exchange say), we might be able to stabilize from within. Computing solves primarily distribution problems by allowing communication while at the same time being only locally optimal. These signals are responded or mapped by the network to stabilize the system. Likewise computing also allows via sensors to collate signals say from buoys of multitude of data, which could be sifted to seperate noise (which could be cancelled out as local disturbances) to long term trends, thereby stabilizing or buffering the system from within. There is no difference between whether the system is placed in a human participating system or a biological or weather system of complex nature. Stability conditions help interpret complex systems.
22. The question is drift. That is to say, once stable conditions are discovered and a system stands effectively buffered from within, an observer at the boundary of the system would be able to observe macroscopic noise filtered changes, which he starts emulating. A spring might resonate micro events. We might observe that noise cancelling by additivity of computing produces a smoothening effect. As long as the whitenoise is seperated, the fundamental signals makes itself apparant to the external observer yielding to control. However, say a certain human population is being observed, if movements of average urbanite is discounted as white noise and the presidential address allows for predicting certain pathway of events, thereby allowing preparing a control strategy, a small statistical rounding off can produce dramatic result. Say an average man pulls the gun on the president, perhaps by impulse. The whole system of control collapses and the system is perturbed dramatically from its stable course. The false sense of stability is only a deferred collapse.
23. The stability obtained by filtering white noise is hence falliable in systems with high heterogenous structure (such as critical points of collapse internally such as the president). The stabilizer hence could not afford simply seperating white noise, but also to internally buffer specific vulnerable points. This would need an understanding of the command structure of the society, where one mans will might by virtue of a heirarchial arrangement be obeyed by others. A more generic analogy of this is the presence of causal relationships, where on one end is a cause and the effect might have no discretion but to necessarily follow the cause, as with the presidential decree. A knowledge of causal relationhips allows understanding heterogenity and all heterogenity is heirarchies of designatable boundaries beyond which causal events dont propogate that well as within the denominated heiarchy. Thus, the presence of causal relationship between raindrop condensation and mountains in the course allows to make a stabilizer or predictor which could incorporate these strong causal patterns to the action. This involves knowledge of heiarchial arrangement of a complex domain, the self contained ordered elements which participate in producing complexity and heterogenous relationships among them.
24. Computing emphasizes seperation of noise to the detriment of derivation of causal relationships. Traditionally knowledge involved qualitative and seperated inquiries into specific associations, involving heirarchies and rules. Such structures permit understanding well established parts of the system but not the whole. In physiology some parts might better understood than some other. To understand a phenomenon, one needs to build a noise filter to buffer and present clear trajectories of health as well as to find specific command relationships so as to be able to approximate them in classical mechanics so that intervention might be provided. The whole however escapes the formalism of mechanism, due to the emergence. Its irreductability hence causes holistic problems be solved inductively from general observations by heuristics. Heuristics is essentially noise filteration, paralell reasoning along possible paths in a Bayesian sense. Humans are good at this apart from causal inference which could be delegated to reductive models.
25. Supplementation by holistic modelling might thus take the burden off heuristics of specialists. Expert systems might hence stand to be supplemented by holistic models. The question is whether definite computational advantages could be wrought by using machines instead of people. We had seen that intelligence hits a wall when dealing with paralellism and a high frequency system is only marginally better than a low frequency system. A low frequency system might be able to passively isolate noise and apply it. 
26. In that high frequency noise filteration is only marginally advantageous it only fuels humans from abstaining from work that they could very well do equally efficiently and substituting by machines. This has economical reasons for substitutions, since long term risks of collapse are hard to come by.
27. It is critical how falliabilities of expert judgements are normatively handled. A falliable decision flows in as frustration, but the good faith with regard to the application of skill by the professional, allows pursuit of consolation either by social accomodation or by spiritual reflection. On the other hand, a computational failure might be interpreted as being biased or having been conjectured not on a professional with reference to an objective framework of ethics (arising from shared heritage as humans etc). Computational holistic models involve work in their construction and hence have value propositions. These value propositions involve preferring work that could be solved quicker rather than slower and the outcome is quantitative rather than qualitative (as with human agents). This had been the trend of additive entities like corporations where a certain degree of quantitative integration had been achieved. Qualitative judgements are generally ignored, requiring people to control them on ethical basis.
28. Hence we present the twin points of any further intelligence in resolving complexity as essentially unattainable and secondly, an algorithm that somehow produces advantages actually defers risks of catastrophes or it actually buffers risks by selecting quantitatively advantageous decisions. Qualitative decisions which could be examined in the system of ethics could not be produced by such machines. Higher frequencies provide short term advantages to seperating white noise, cyclically determining the stable course of the dynamic system, but an ignorance of inherent relationships which have high potential and low inductive representation, requires an understanding of structural principles and initial conditions.
29. Structural principles are discovered not only by an investigation of sufficient conditions, but necessary constraints to the system. This involves deduction from known concepts of generality and specialization, which supposses a priori knowledge, of discreteness of numbers and such principles. A repeated recursive application of these principles to observations produces the effect of seeing them as constituted from parts connected heirarchially and associatively and as enumerations of contained types. On this plane of abstract reasoning a human is able to make sense of the world.
30. The construction of a system of abstract knowledge, could be inductively enriched, but it would need a framework of logical reasoning and mathematical continuity. The objectivism and realism of these entities, might suggest their declaration and capacity of computation on a machine, which we very well do, embodying them in logical structures such as physical machines and logical system of rules. But the computer is different in that it does not represent causal relationhips between entities such as a system of rules, but rather a system that responds to events. A computers course involves a parameter of time to it, making it an automaton but not a classical machine.
31. A computer responds to events, it foresees and alters its internal state till such a state actually involves emitting events. Thus, it maps events to events as an exchange would. It is not a heat pump, it dynamically buffers events to produce stability. It has no notion of heiarchiality or boundaries. It evaluates events (waits for them) and determines if events are there in certain categorical lists and emits events regarding their subjective truth. There is no master plan to their actions (or outgoing signals to actuators). Hence, from the initial state the computer buffers noise but has no means of knowing specific pathways of 'potential' as with causal reasoning. 
32. It might allow development of simulations of human heuristics and adversarial search and thereby some heirarchies might emerge and this preference between good and evil as the dualistic tendencies play in its core reasoning (reward and penalty or truth and untruth either of these dialectic is tenable). Thus, the establishment of some dialectic is required for the development of the computer to reason as humans do. Coupled with inductive noise filtering, they might become crude reasoning machines, but their intelligence might not exceed human ones nor they be highly helpful to human endeavours. Even for this we presume that once a dialectic is present, an universal sense of goodness is available to any open system and could be projected on to the understanding of the world around.
33.From an instrumentation point of view, the computer relies on additivity. There is hence a  deep reliance on the signs of the numbers, without which noise filteration by additivity is impossible as well as the angular directedness which helps vector operations in a plane. Thus, we might say that from an instrumentation point of view the computer is a deresolver. Just as there had been instruments like telescope and the microscope that enhance empirical observation to the benefit of science, the computer does the inverse. It removes distraction from the observation and thus helps scientific approach to truth. A good use case, might be that a computer can trivially produce the average day time temperature for a city over ten years. This helps the scientist avoid any short term bias and actually interpret short term bias.
34. Apart from the instrumentation perspective, where we actually attempt to state that science is the path to truth, there is also the commercial perspective, where economic activity is also seen as a path to truth, one might imagine, people applying deresolvers to everyday use cases and acting based on it. In science experiments should result in theory which would then be perfected for engineering adaptations. There is an attempt to logically reconcile before applied science. On the other hand, a shortcircuited direct heuristic adaptation of deresolved truths could be automated to a good degree and even if made by humans are repetitive and trivial. The result of application of deresolvers is an ignorance of causal relationships and stability attained by a need for ever increasing growth.
35. That is to say, a stabilization requirement of a society that works with buffers to sustain itself while ignoring 'special' relationships of high potential might in practice be attained by expanding the boundaries of the buffered system, so that while there is no central surveillance to logical constructs of control, there is an elaborate process of peer balancing, which discourages development of potentials and schemes. This homogenity might be speculated however to give rise to emergent dialectic heterogenity.

==
Regarding the nature of intelligence

1.A lot of allusions had been made in literature both formal and informal about the case of human intelligence being dramatically superior to animal intelligence, with such examples as lizards not being able to make sense of tubelights and anthills not knowing of highways that pass beside them. For the lizard, a tubelight is only suggestive of a situation that flies would emerge to circle it and makes a good meal. But here there is something missing in this analogy. Even a human is not able to reason much about the tubelight. Of course he can answer questions about it, posed to him in a grammatically organized language. He might answer such questions of causality such as the switch being thrown causing the light to come up, he might even answer questions such as to that a certain energy called electricity flows on when switched to create the phenomenon. But beyond that his cognizance dims, for a lay person has no further ways to explain what electricity is. He might draw weak analogies in the flow of streams and rivers, but definitely his knowldge becomes hazy just two level of questioning away. Thus, we might say, human capacity for epistemological intelligence is seperate and distinct from the common way of percieveing things that has deeper roots and more commonly distributed.
2.Human intelligence hence might be said to enjoy only a minor advantage in being able to use the epistemological way of knowing and percieving things. Even genetic wise, their makeup is not more than two percent different from the nearest primate relative. However, empirically we see that the epistemoloigcal capacity housed in the prefrontal cortex had dramatically altered the world. We have to realize that such an alteration is caused by a strong positive feedback loop, which might be called as 'Capital'. This capital allows humans to construct civilizational tools of such great mass, that it is able to very much reduce the complexity in nature, by building dams and irrigating fields (as compared to the complexity of water rushing through a forest stream).
3. As we had seen all complexity arises from paralellism. For instance if a human is suppossed to cross a ten lane highway, at a given instance, he has to analyze the paralellism of different entities at play making it riskier or easier for him to get across. There might be drivers rushing to get to work at particular times of the day. The road might be slippery after the rain, the human agent's constitution at particular times might be more vulnerable, such as early in the day, a fog might be descending and so on. In  fact it is combination of these paralell developments that give rise to intractable phenomena such as near misses. Therefore, the person crossing the road, even though equipped with epistemological intelligence, would have to serialize everything in order to present to his rational faculty.
4. If we look at paralellism per se, we would need to recognize the presence of an very obvious and often overlooked massive computer, the universe itself. All the paralell events make sense only because they play out on a physical world, with almost infinite density of time (perhaps a clock speed of plankhs constant intervals, we are not sure). Given the massive potential for physical events to happen, numerous subjective aspirations (even free willed subjectivity such as drivers rushing to work) come together in a dramatic fashion of execution in the physical reality or the physical world. Thus we might as well say instead of A and B collided, strictly speaking it is either A collided with B or vice versa. 
5. It is this paraellism of the real world that the capital gained by the intelligent actions over millenia endows humans with. They might put up rules, controls all by brute force and contain the paralellism to managable level and live with buffers that could absorb any estimation errors. 
6. In fact even while reasoning on a serialized basis, human intelligence does work with probabilities (such as the speed of the average driver, while crossing the road), where every event sensorially observed is coupled with a permissible white noise, while it is being slotted to specific, exclusive types. Humans are able to construct abstract serialized models based on such exclusive membership systems in their rational process. This as we might see is quite limited in scope and hence many times, human reasoning relies on heuristics, which do not have much of formal denotation.
7. For instance if a human is given a task of arranging several geometrically shaped objects (solid and hollow), he does not go about in a serialized fashion on a combinatorial basis as a computer might, but works with certain predefined rules, such as the heaviest object being at the center to produce more stability and so on. These innate constructs arise not from the reasoning faculty, but from the innate sense of balance and symmetry that is hardcoded in the non epistemological intelligence, inherited over billions of years of evolution. Hence in the lines of Chomskyian tableau rasa argument, we might say there exists a deep subjectivity that operates behind the epistemological faculty to be able to overcome the combinatorial burden by picking up what is 'good' and what is not. The dialectic probably arose deep in the experience to keep alive and away from pain.
8. The nature of epistemological reasoning might as well involve simulating the physical world, where the degrees of freedom are limited when dealing with objects and without running into a combinatorial explosion, the human mind might be able to backtrack and playout different possibilities to decide on the proper course. Many people like Ferrier had argued of the necessary reference to the deep subjective goal of development in order for humans to resolve problems epistomologically as being the frequently used shortcut(they consider objectivity as extended subjectivity with deferred rewards). They infact suggest there being an incapacity to seperate the subjective from the objective. But for practical purposes, it appears that human epistemological reasoning could in fact hold objective grounds away from subjective influence, as proven by their ability to percieve justice and to practice science.  
9. The non epistemological dimension to intelligence is not appreciated as often, but they form the foundation for the argument of parochiality of human intelligence.
--
1. We had argued against super intelligence elsewhere because of the limited computational resources of human mind that it quickly runs while applying its epistemological method into a combinatorial wall. A system of higher memory might as well reach the wall, albeit a few levels later. Therefore it might be said of intelligence that scalability by adding memory is not that fool proof. We had also seen that intelligence might be elevated, such as the degree that prevails between isolated tribes and urbane humans. There are more concepts to the expressions of the urbane person, distributed logically about a mean core of knowledge (or perhaps a null center). Introduction of more concepts allows modern humans to reason over many more things, but a lot of the difference is not due to this ability, but by the generation and application of capital from the surplus.
2. A more resilient way to intelligence is with simulation. Humans might simulate the physical world in a system of high clock speed and thus be able to emulate the nature of the outcome from a sequence of paralell events. The computer apart from being a pure deresolve (or statistical smoother) also simulates reality (Langton). This simulation allows to serialize in memory to produce interesting estimates of outcomes, in an abstract way and wihtout relying on material reality. 
3. Optimizations work this way where the physical reality is impleaded as constraints and subjective preferences are let to play out in order to take proper positions that would hold well in real world. In fact we might see that humans are already reasoning at the end of their memories, stretching themselves by training and education. Over time, they seem to be distributing their epistemological capacities, by the use of capital and markets and thus creating distributed paralellism from what is essentially a faculty for serialized reasoning. Hence, they seem to be constructing an augmentation of the core intelligence.
4. This distribution of intelligence, would need some kind of physical interation or a sufficiently powerful computer to simulate the new reality. The system might itself be subject to some searches for compression, which might lead to an emergence of objectivity (or a reemergence thereof). It might go forth to a distinction in a dialectic sense in order to gather the reasoning based on pleasure and pain and hence allow a subjectivity to work with the simulation to produce a new level of civilization. This emergent intelligence from distributed epistemological processing, serving a core intelligence which might bifurcate by dialectic over what is good and what is to be avoided might produce a true artificial intelligence of a global scale.
--
1. If we digress a little bit with respect to the ability of humans to work rationally, it might help. This is wiht respect to the nature of tribes and thus an inquiry into rationalism. IF we look at the nature of modernity, we see that there had been an insistence of consistency being the measure of goodness. If we see abolitionism rose in the eighteenth centure in Europe and in fact caused a civil war in the nineteenth century US. The reason was that the concepts of libertarianism suffered from a crisis of consistency. The same inability to live with rational inconsistencies pushed the west continuing its liberatarian rationalist experiment in the past three hundred years to permit assertion of civil rights in the 1960s US and permitting non white immigration the next decade.
2. But if rationalism and education is capable of creating consistency in the real world among real people is quesitonable. People anecdotally, drawing from their long tribal past are more comfortable with stories than rational constructs. Hence, people drawn by stories and by complex ways in which music and culture string to their core intelligence, are able to cope better. In fact despite their rational convictions, people had fallen off the road, inadvertantly to create world wars, holocaust and imperialism, which they later came to regret. Thus rationalism and modernity creates such puritanical assertions that are impractiacal and often lead to collapses and tragedies.
3.Thus much of the yearnings of the present age might be a rooting to the tribal past. The broad strokes of rejection of modernity in the past century center upon this insistence of puritanical rationality or single source of truth, that becomes vested upon an authority and its inherent violence. Thus, a better policy framework would be to address the human units as story abiding tribes, rather than rational units. Libertarian assertions might be made with respect to freedom from coercion and a basic social security net, as with Nordic communities. At present, the irrationality of humans are handled by the mechanism of the market, while the rational faculties by the construct of law. Hence, we look at ways in which the appeal to fundamental or core intelligence could be sustained in a modernistic setup. But what is important from an inquiry into intelligence, is the concept of the essential irrational part to the intelligence, which would require to be balanced by proper political policy , as much as possible. It could more deeply be attempted by reflection on the tribal past and contemplating nature which could somehow flow up as consensual policy dynamics, rather than a visceral hate of rationality.
4. It should also be noted that the objectives sought in terms of stability and peace as well as engaging the individual free will, is difficult of sustenance. Every system is subject to entropy. Even if the best mechanisms are incorporated, they would fall apart and would need other mechanisms (as human rational understanding of roles) to be upkept and so on recursively. Hence, there is no escape from recursion than by growth. But the question is whether growth could be attempted without recoure to evolution, but by deploying capital and coupling it with epistemological intelligence in order to protect the human status quo of tribal living.
5. Actually rationalism might be expecting too much out of people. People are much more comfortable with mythologies. Eventhough some arrangements might look to be oppressive from a rationalist view point, traditional societies have their way of balancing things out, without explicit intervention on rational basis. But rationalism might itself not be rejected for the reason of being able to model complex real world systems linearly and be capable of being worked upon in a manner which could be linearly allocated and scaled. But the limitations of rationalism in predicting non linear systems is to be recognized. Many modern states as we have mentioned shy away from constructive legislation and instead stick to defining the limits. To complement this public policy move, markets ought to be blunted of their omnipotence in representing tribe irrationality, in order to prescribe a more stable and contended society.
--
1.Now that we have painted a picture, where we see that rationality as being reductive and the extension of intelligence into areas which are non linear as being important in obtaining an explanation of the state of affairs. We might as well go towards saying that some kind of our own interpretation of the master slave dialectic to be existant between the core intelligence and the rational intelligence, the latter being the slave. We might see in this ontology, that the slave is in the command of the master, evaluating and computing solutions and proposing them back to the master, whose normative and aesthetic point of view is intractable to the slave. 
2. The question is how often the slave refers back to the master. We had seen that in problems typically involving combinatorial methods, the human mind is able to build algorithms, perhaps by a combination of the principal methods of problem solving that of reduction, summarization (statistical) and simulation (stochastic). But it might as well involve a frequent referral to what is good or aesthetic in between decisions, to lend the substance to heuristics. 
3. Again it is ambivalent of the nature of the master. The master might be the ultimate historical product of four billions of years of evolution with no reductive undercurrent running. It might be that the slave is perplexed because it is presuming a finitude to its methods. It could not appreciate the infinitude of the evolution of the master, being the complete product of a fabric that is utterly random and historic. It always tends towards theorizing and looking for some cyclical drum that spins out the patterns on the brocade, perhaps this is the pitfall of the rational faculty. But empirically, the claim of rationality is not entirely misplaced, we see the universe is either of finite size or of finite age, we see so many mechanical backdrops to apparantly interesting phenomena and so on. 
4. Hence, the master slave dialectic might explain the persistant need for both to make sense of each other (Ferrier). Therefore the master looks up to the slave to attempt to makes sense of the situation. Hence, it might happen that pure slavery might not be sustainable and the slave might grow to influence the master in some ways. This body mind dialectic (rather than the confusing master slave dialectic sugestive of Hegel) is important of consideration to make sense of intelligence.
5. But it might be too farfetched to annotate historicity to everything about life. We see that in the germ cell, there is a possibility infact to hold the evolutionary tree still. The totepotent germ cell might develop into a full formed organism, in much different circumstances than the originator. Thus, it does permit an interruption of the flow, where we see that it is possible to codify the body (or the master) itself. But the presence of the code itself might not suggest tractability. No non trivial computing could be predicted by mere inspection of the code (as proven by Turing and as discussed by Langton). Hence, it might still be that while time is not a parameter to the starting function of the entire system, the subseuent paralellism and the time delays in execution of procedures referring to relative loci, might generate complexity. One ought to remember that the simulation is being played out on the supreme computer, the universe itself.
6. Hence, we do see that at times, there does look like that there is no body nor mind, because of mechanical implications into apparantly historical phenomena and the way simple mechanistic contrapctions could have computations intractable even with every atom of the universe being used as memory. The trick lies with the exponent. But practically, one might make sense that recognizing the rational faculty as being instrumental and distinct from a deeper self allows for a reflection of the nature of the self. This is ultimately speculative and reasoning about the self, could be sure of one thing only, its incompleteness that is. Hence, being aware of the incompleteness (and not arguing it to having been constructed from a purposely reductive set of definitions), might help with a possible will of mankind, that is to reason about the world and not be modified by external forces, without a reason, or more importantly against ones desire or will. Hence, the supremacy of free will or liberty is closely connected to the method of reason. This supremacy questions the forces of universe and allows the self to bifurcate them to be good and bad and not simply natural. Hence, freedom empowers picking up a path irrespective of the circumstances.
7. The present direction of computing might be interpreted from this normative premise (which makes our work somewhat less rigourous). One sees that computing is being developed to consider questions using methods of noise filteration and simulation as direct appeals to core intelligence, without requiring of epistemological explanation. In that we see that computing to be inherent non deterministic (see Turing who showed that the pathways of computing devices might involve time, as in a dynamical system). Since, the Turing machine (as against the FSM) involves extension temporally, waiting for events (which arises from paralellism orchestrated in the physical world), it itself is non deterministic. 
8. Whatever human actions in history had been, they had been but thinly veiled fight of evolution. There have been fight over ideas and religions all as means to establish the fitness of genes. This is the inverted reasoning in the extended phenotype. In this case, we see that there is no paradigmatic shift between the way computers normally work as Turing machines and their enhancement in machine learning systems. Computers have always appealed to the core intelligence, being non deterministic and had fulfilled the wishes of the master, without the need for elaborate structured argument. Thus, putting in a computer network, allows negotiation based on 'wishes' and not intellectual reflections of stoic duty. Thus, it furthers the case of evolution like none other. It improves the fitness of the species, thereby generating unprecedented surplus as capital that flows into barricading off the natural forces. 
9. We had also argued that the innate sense of good and bad has a historicity unparalelled. But we might also argue that goodness is borne of the concept of the norm. A normal distribution is directed radially, the center indicates goodness and all peripheries imperfections and suboptimalities that might be evil (which might be seen as temptation and sloth representing the edges of the curve). Thus, for a beach feeding creature, being too near the waterline (as a high risk strategy) or too far away from it (as a low payoff strategy) might both be simply variations of badness. A radial reasoning allows to locate goodness, from peer experience and with limted data without the need for a complete history.
10. Thus, we might argue that given that goodness and badness become available to the computational representative of human progress, an embodiment of such computational programs into physical systems, with physical consequences, allows a simulation to directly leverage the universal computer. Hitherto, the computer had been an abstract machine capable of producing abstract decisions. But if we look in terms of networks, they are paralell, working wiht the universal computer and having the norm for reference. A delination of physical beings is what is required for the physical strategies to play out. Hence, robotics might be a real and plausible course. The future computer might embody a set of constructional tools to build algorithms as a flow, along with notions of goodness and badness from the norm and a physical form to actually simulate in paralellism. This combination might allow the progression of primitive organisms, which might go feral, due to the absence of complete cost effective control strategies, due to the multiplicity of form and its heterogenity.
11. In the course of evolution, it had always been that life attempts to build stuff which is delicate and crowing glory, at a given point. Say the use of eukaryotes or that of multicellularity. These were at the point of their emergence, be too adventurous and delicate strategies. But at the right circumstances, the payoffs might be exponential. Likewise this might be a way to allow for a portability of the physical components of the being, which might cause an explosion.
12. Computers as we see them are already accessing actuators along with markets to attempt to recreate paralellism required for emergence. They cooperate using direct generation of events rather than a system of rules as with formal actors. These events might give rise at its earliest form encoding to caches. A demarcated computational agent might encode certain events (of peer computers) , such as say casting a shadow might discourage another agent from emerging from its hiding, for fear of being seized up and destroyed. Thus complex interactions might develop. We might need a formalism to construct this. For instance, let a system tune to a certain subset of events and a game playout in the real world. The outcomes are likely to be dramatic. These caches might in fact be compressed in some far future to create more deterministic heirarchies and more standard adaptations.
13. The evolution of the whole might proceed by cacheing and tuning to peer and environmental events. It would then need a simulation of the world itself in its physical being (now that it has a physical body) and play out in a field experimenting and searching. It might happen that the organism is constructed from a single coherent piece rather than multicellular. This compromises germ lines and reflexivity of the environment on pristine key value copies assumed to endure for a generation against short term shocks might be absent. Instead, the system might evolve smoothing and regenerating its strategic key values quickly.
14. The absence of the destruction of the body, might allow for more fluidic and quicker evolution, but it also reduces the drive to create subjective optimization, for which if the body is at stake, the drive is induced. Hence, a system that can immaterially replicate (as against the genetic materialism of DNA), can preserve its core code and add local strategies, in a multicellular like way and attempt to follow the norm, while being responsive to local adaptations. A consolidation of local adaptations might give rise to emergence of species. It might be a ghost in the box scenario. Some kind of marshalling is required to produce germ cells in order to avoid short term oscillaitons.





A formalism of emergent computational behaviour.


===
One might see that non abstract intelligence, easily overcomes the problems with paralellism. That is to say to know about a certain outcome, one simply lets it play out. This has no energy advantage, but being able to afford consequences by expendable agents might provide a solution.










===
Refer: Emergence of dialectic
Turing Machines are more powerful than FSM  and they are self proclaimed symbol processing machines and hence signal adapters. They do not do work, it is well known. They are non physical machines (cite Horáková, Klemen 20th century conception of machine)
Institute of Computer 
Attention needs to be given to the distinction between the way computers work as a network, thereby asserting their wholeness and buffering behaviour and in the way in which they work as discrete game playing agents thereby simulating reality or giving rise to emergent phenomena.
Much of the preference for computation based solutions reflect a rejection of authority. Computation solves problem by complex self regulation, rather than using structured truths. We represent that authority and truth structures are good, because humans are capable of moderation through appeal to reason and morals. A rejection of this philanthropic view, leads to a reliance on complex self regulation by search and negotiation on adhoc basis. This had been shown to be leading to emergent behaviour, strucutures and dialectic.

A Developmental interpretation of Computing
1. Modernity started to have to much to do with libertarianism as much as generation of surplus. There was seen an emancipatory goal in attempting to generate surplus from nature as well as to make sure that a person in general has choice that is maximized. The method of approach to generate surplus from nature involved development of rational framework which can spot events in nature, allowing for the construction of machines which could perfect phenomena that generate surplus.
2. Rationality is necessarily incremental, in that older concepts give rise to new concepts. Likewise machines are capable of building over one another and incrementally generate surplus. The surplus is then appropriated by the mechanism of governance. Governance involves in its canonical form running a registry of properties and promises. The promises are best represented in the distributed medium of the currency. The use of a mechanism to make sure promises are met, allows for committing what is material into the idealistic domain. The actual machines that are created by rational actors by their cleverness is submitted to common wealth in exchange of promises, which are always in flux. Thus, finance is the otherside of mechanization. It is an endeavour of modernity to seperate work from life in general. Work is proposed to be done as a rational process, which in turn is expected to produce incremental surplus capable of supporting idealist domians (such as politics) and therefrom the spiritual domain.
3. It is needed of the political side to be able to provide single source of truths as an authoritative source. The authority is reserved by monopolizing violence and settlement of questions by coercion and deliberation. The promises by the monetary system and the registries provide singular truths. The presence allows people to exchange material factors of production for such ideas (as money and state). But there is in coherence to mapping from the material dimension, an expectation for monetary sphere to constantly expand. This necessity of the money to obtain a positive feedback or generate a surplus is the idea of capital.
4. In the rational schema, as we have seen libertarianism holds high sway. Hence, capital expresses the ultimate free choice of the individual to do what he wants with his invention (includng to hoard and thus hold it exclusively, without necessity of explanations). The capital is deployed into the mechanical innovations, which rely on rational methods. Thus the loop runs from the machine world, onto state authority, onto monetarism and capital. The markets blunt freewill, but in many cases capital involves the ability to carry out things irrespective of the wish of the market. The market provides statistical feedback to the captial. The western libertarian concept of development where freewill and surplus are available, is obtained by this dyad of capital and rationalist machines.
5. However singular truths are not sufficient to sustain humanity. The government also knows this and hence truths that are continuous, as bands of prices (as a distribution of prices about a mean price) is determined in the market. The market is hence a non authoritative determinant of truths as a statistical distribution. These truths also guide the application of capital. Therefore it is possible that norms could guide free willed decisions. 
6. Ideally the government running the sources of singular truths, by coercion and by boundary preservation allows it to appropriate surplus and it could thence go ahead with public spending for the development of the society, based on prerogatives of planning on the best courses of application of capital. But private capital, as advised by market prices go into the construction of further factors of production. This necessity for finance expansion pushes the employed capital to search for new rational fixes. As might be seen from history, reductive solutions frequently have low marginal returns. Hence, capital pushes for transnational flow and that had been the engine of imperialism.
7. The government would hence have to take the side of the capitalist and allow for their lobbying in order that it could ensure a financial expansion. It would have to assist the transnational flow of capital, sometimes by chartered companies as with the imperialist period. This causes a divergence of the role of the government. In other words, the position of governance, becomes destabilized by this need to rely on private capital, as being captialist or acting in a reactionary mode. Thus, wild oscillations might arise. Some governments might think the push of the capitalist as being unpopular and side with the majority and revert to public spending being the primary mode of operation or resort to fascist agenda, attempting to redeem the individual from the grind of the mechanical, monetarist, captialist, militarist diplomacy, that has inflationary tendencies and irregular oscillations (due to the reflexive anticipation between incremental mechanical advantage). 
8. This in turn leads to violence. Thus, the postmodernist reaction involves a rejection of centralized authority, which work on dominant narratives like that of capital or the need for protecting the common man against capitalist choice. It would hence necessarily have to reject rationalism, finance in order to stay consistent. The twin problems of state violence and the bigotry involved in stating the benefits of reductive modelling, while modernity does not count side effects and waste was the wind in the sails of the postmodernist movement. The postmodernist movement hence takes a toll on the power of capital, of the state and of the development of powerful rational models that could be made into engineering feats. 
9. Thus the attack is essentially directed at reductive modelling. The concept of the machine as being a heirarchial rationally explainable concept wiht a positive feedback hence came to be revisited. The new machine is event based, which works not on rational concept that is preserved and sealed by capital (as working capital of large corporations operating logistic networks), but as highly adaptive, responsive machines (as with ola). This adaptive machine is hence the abstract machine, that had materialized as the concrete machine. 
10. The primary instrument of control of humans to the event driven machines is by feedback. That is to say, the actions of the machines is responded by humans by events, which in turn shape the cacheing behaviour of machines to act to certain events in certain manner. 
11. While this transition is in the making, humanity also suffers a crisis of conscience and identity. The human consciousness involves an ability to reflect on the entire schema of rational engineering giving rise to incrementally productive machines, being balanced by finance and accumulated into capital subject to free will, moderated by the norms of the market to guide actions. The ability to reflect allows humans to see the runaway complex in the way of working of the world, as distinct from the human desire for perfection of free will. This ability to reflect on the mistakes allows for the construction of an unchanging concept of the self, by expanding the consiousness to ways in which people can construct things which do not exist in free form. The causal connectedness discredits the originality of any observation. Thus, we might say that machines, capital, state are all connected and thus the self which is capable of observing it critically is more consistent than these systems.
12. We might see that while modernity was successful in seperating work from general living, the complexity arising from the incrementally productive machine, capital and its pressure on the state, giving rise to a paralellism arising from an oscillatory state policy on capital, due to the need to artificially prop up capital (or condemn it) away from the forces of the market (which is quite vulnerable to tampering) and the real world solutions generating surplus. This complexity due to ambivalence of state policy arising from the vagaries of rational machines and the transparency concerns of the market itself, leads to a frustration with the complexity. Over time the complexity becomes so high that the individual in the cultural sphere away from the work sphere is primarily spending time in deciding on how to dispose off his own capital. There is no direct link to rational material world of the factors of production, nor of the free form market. Thus, whatever culture is left off in arts suffers a crisis.
13. This artistic crisis actually triggered postmodernist expansion. The postmodernist point is to attempt to dethrone the authoritarian single source of truth from the government or its coalition with the capitalist, which in turn is dependent on the rational mechanistic performance and instead attempts to work with the market itself. The computing machines generate and assimilate events from the market, without a need for central authority and it seems to be close to perfection. But the side effect of a loss of visibility over what is constructed by human agents and how they mesh together to create the reality, causes a crisis of conscious expansion. In the market, the human consciousness could not see large scale phenomena, which it could attempt to control and experiment with. All it sees is the local context and this leads to a fragmentation of the consciousness. Culture is the ability to reflect on the history and the absence of history is a crisis too. The old schema of seperation of work from living is also passe,leading to action having no idealist representation in an abstract sense. Thus, computers become the most concrete machines.
14. This crisis of identity, drives humans to attempt to construct local identities around local contexts. These local identities might have an effect on the feedback of the computing systems. The computing systems become exposed to control by feedback among its own peers (which now have divergent feedback providing humans  and are hence much different and competing). Thus there is a loss of control by feedback from the human end. The peer to peer feedback gives rise to an emergent consciousness, which is often seen as the ‘goal misalignment’ problem in AI literature. The two consciousnessess are at this moment in history undergoing a dramatic clash. The former is the consciousness which needs reason around constructs in order to make sense of the world, in terms of why it has not been rewarded ( a conscious that expands based on knowledge) or why there is pain and the conscious  of emergent peer to peer control among computers. 
15. Prescriptively, one might see that identity might be dynamically managed by recursive construction (after all recursive construction as with fibonacci generation algorithms had been successful in tackling dynamical problems).
A Developmental interpretation of Computing-2

Here we attempt to frame computing as an abstract machine that attempts to provide solutions without explanations. The superfluity of explanations and the rational method is attempted of presentation here as a fleeting construct of four hundred years of modernity. 

Defining Development
Development is seen as the ability to increase surplus in any frame of reference, be it a family or a nation. The other dimension to development is how the surplus is applied. That is to say, once the surplus is available, it ought to be applied to create a barrier that seperates the randomness of nature from the self. A perfect insulation is not possible but in such an utopia, a given individual is able to do such actions that have no explanations, as manifestations of his ultimate sovereignity and free will. 
But clean seperation from nature is not possible. The internal entropy to the system causes emergent problems. The ability to defer internal entropy from building up involves growth. That is to say, as the internal entropy builds up within the system, where there arises disharmony and random movements of libertarian individuals in the society, leading to the weakening of the fortifications, one would need to rely on absolute incremental order being generated in order that the inertia of the internal structures are overcome. The net energy flow inside the system allows for reformatting the system dynamically so that entropy is reduced. If the growth can be sufficiently subtle as to render it an oscillation about a mean position as a method of stability is theoretically tenable.
Interpretation of Collective Growth
As growth materializes, there is more surplus available and this available surplus is required of appropriation to the private sphere. This would need that the private sphere involves zero sum games of cultural celebration and spiritual reflection. The seperation of the public sphere from the private sphere requires polity to safeguard the private rights of the citizen. For this to be set in motion and sustained, there are costs involved. Hence, the state collects portions of the surplus arising from the employment of rational tools in solving problems of nature in order to set a system where an authoritative version of the truth could be delivered. 
Hence, the state could not be sustained without surplus and the rational enterprise is under stress to produce a surplus, rather than simply zero sum costs. There are aspirational estimations made in deriving cumulative mechanical advantage, which works most of the time, leading to an incremental surplus. The surplus thus put into the hand of the individual after paying taxes, then becomes capital, with the potential to create further capital. This capital is looped to the potential to create more cumulative machines. This loop of finance and mechanical advantage leads to a spiralling out of surplus. This surplus could be applied in the public sphere to create commonwealth. 
This commonwealth of resources, which allows for public infrastructure, fortifications and expansionist or defensive campaigns arises from the ability of governments to dematerialize physical machines into promises that flow. These promises are validated from the central registry of authoritative truth. The flow of promises allows for public sphere discussion over generating surplus, which happens in market transactions, wherein statistical truths are delivered (as price bands with distributions). This use of rational mechanical advantage at the level of a society causes policy dilemma. At the same time, the society as a whole is seeking mechanical advantage. Normally, it should be the knowledge that flows across individuals who may then utilize an invention and replicate it, to create surplus available for their own disposal in their private sphere. The flow of state sponsored promises in exchange of the material factor of production, causes the dilemma wherein the state becomes responsible not just of maintaining the law, but of herding this new found group behaviour.
The market becomes rife with rigging. Likewise the flow of capital across borders wants to involve state diplomacy and military might. It might also further reforms using the state as an agent to consolidate factors of production to reap mechanical advantage (such as bulk acquisition of land). The state hence becomes deeply inspired by the capitalist. Policy is however polar, in the sense as one set moves to support the market and the capital, another side attempts to relieve the pipleline of generation of surplus, which attempts to push the state rather than think of genuine rational mechanisms. The misuse of state machinery and the state’s helplessness in order to sustain the flow into its exchequer arise from the limitations of the rational method to consistently generate mechanical advantage. This is an area of unpredictability. The need to smoothen the inconsistencies of the rational method, involves the state playing the role of the stabilizer, obliging here and when the cartel grows too strong dispersing it.Hence, the root cause might be traced to the problem of the method of reductive modelling. 
Hence, the problems of development in the modern state involves destabilized state actors, transnational violence, imperfect markets and the usurpation of the private sphere into dedicating time in deciding on the real world feedback of rational productive techniques (by participating in the market as consumerists). 
Why the reductive modelling fails:
Reductive model suffers when non linearity is involved, which arises from paralellism in the real world. The market is a good agent to distribute the determination of the fitness of a solution in paralell mode, so that emergent wisdom might be approximated to the fitness of the solution in the real world. However, the market is a much smaller computational system, when compared to the world at large. The market is subject of fallacies that could set expectations by mass delusions. It might be nudged and influenced, even by the government and cartels. The market being imperfect reductive models are evaluated in an unstable manner. 
The more fundamental reason for the failure of the reductive model is the inherent risk in increasing completeness at the cost of reducing consistency. Thus, the system itself acquires paralell dynamics and starts working imperfectly. There arise fluctuations to its mechanical advantage (more precisely due to the environmental dynamics). A sufficiently powerful simulation does not exist and abstract reasoning is inherently problematic. The reliance on markets is also imperfect. Attempts to stabilize the reductive model paradigm leads not only to state destabilization, but also destabilization of the environment (by waste disposal/unaccounted exploitation) and of the people unprotected by state machinery (either due to policy or due to flaws in implementation of the state machinery). It also furthers cross border expansion and violence. 
The linear modelling involves construction of a serialized model of the universe. The events that are thrown into contrast and cherrypicked and assembled into machines capable of work. There are other events working in paralell, which alter the course, but it is incapable of being incorporated into any language, due to the linear nature of reasoning and expression. Hence, the only solution to reductive modelling is to absorb the risks therein by the capitalist himself. The propogation of the instability makes it complex for everybody. But it is difficult to estimate the failure probability of a rational fix in order that a control of the absorption of risks.
The Control Problem
Control of the betting in reductive engineering while maintaining sufficient buffer is a hard problem. So far the feedback from the system is too strong that influences the control mechanisms themselves. A wide berth leads to excessive rewarding of failed cases and a narrow berth leads to propogation of the shock. It needs a dynamic solution. A static wall around the workings of rational systems, requires setting up a fixed outcome from it like some taxes or royalty, which is very low. This might cause a growth in the governed system to the extent that governance becomes impossible. Dynamic smoothing by state mechanisms, natural dumps and markets have all yielded imperfect results. 
The development of the machine which controls the rational machines themselves and buffers all user feedback signals without revealing the global context is achieved by the invention of the computer. The computer puts each capitalist in a local context and he optimizes to immeidate problems.
Stabilization by Ballistics
The control problem arises mainly because of there being the need for scale in many cases in order to realize the benefits of rational machinery. There is a steadily increasing compounding of machines in order to achieve results. This is like a setup of unstable heterogenous blocks, forever adjusting and correcting their positions, sending shockwave across the substrate. In fact finance was introduced as a way to stabilize this setting. Finance market allowed for highly divisible and fluidic counterpart to the machine based ecosystem. But over time, heterogenity started to emerge in finance itself, with Banks and other institutions chunking up money.There are instruments and currencies that self referentially stabilize and influence the finance market. This is highly resented by the Von Mises Institute. 
Computing approaches the problem of ballistics from the other side, it slices the assets in time and space into tiny chunks, so that they can move about ballistically, hopefully working well with the financial and governmental stabilizers. But finance also started out this way. The agents consolidated slowly driven by certain behaviours. The agents who are supposed to optimize locally with respect to their reward seeking behaviour hold on to positions, anticipating gains or minimizing losses and ignore stimuli. On the other hand, on the moral front, where they are supposed to look in the long term, they attempt to optimize locally. Thus, we may say that much of the amoral agents tend to consolidate.
Amoral Agents consolidating behaviour:
The consolidating by amoral agents are however under control in pragmatic cases by the bureaucratic agents, which might be streatched to include the judiciary. The bureaucrats do not appeal to mythological objectivity, but derive a sense of justice from legal prescriptions which are logically consistent. They themselves are stoic agents, rather than holy men. The second type of control in pragmatic society exists by means of popular appeals to moral outrages. This arises where political agents see a problem and instability and attempt to generate public voice in stabilizing the setting by applying a strong intensive force.
At this point, we might think of computing to be dedicated to stabilizing the setup by providing a ballistic proxy. It might hence not be able to defy control by legal and moral outrages, eventhough it might consolidate as much as Banks did in the past. Hence, there is likely to be a continuing instability to the setting, but they may still be under control by fixing ownerships and such methods usually applied to emergent institutions. 
Role of Computers
Apart from displacing capital and consequent need for authority and hence moral reflection, computers however might land the people with a situation where they have no choice to be ‘good’, even if they wish to be heroes. This itself might prompt a moral outrage, but it is difficult to anticipate how it might be formed. All events that are registered to a computer, results in the computer attempting to hairsplit some resource allocation in real time. The simulation worth of computers to project and hence predict the outcome of certain paralell processes are limited owing to the exponential wall of combinatorics. Hence computers themselves might provide little by way of control and explanation by acting as rational agents. They might be quircky and use stochastic methods.
A More speculative role of computers
But computers as much as they are controllers which work by providing feedforward controls to systems in the nature of resources, they are also controllers of natural processes themselves. Illustratively, if we see that a system of drones are allocated in deweeding and pest removal from a heterogenous field, the system is effectively controlling the field. This is unlike financial control, in that the computer can directly signal actuators (by logistical reasoning). This kind of control, directly has the capacity to displace rational, explainable fixes of reductive nature, thereby shaking the very foundation of the problem of instability. This kind of control that simultaneously smoothens the extraction of orderliness form the universe as well as the buffering it to heterogenous systems (such as societies). 
Thus the abstract problem of smoothing natural systems in such a way that a heterogenous evaluator of noise is synchronized with the oscillations of the natural extraction and the distribution is radically made possible by computing. Hence it might be a paradigmatic shift in the control strategy of humans.
Rigour to the approach:
The problem itself is one of an universal controller. The problem of universal control entails certain paradoxes. If a minimal tapping is done of the benefits,there arise a dilemma where the system being controlled reacts by arming itself or if over exploitation is done, it collpases. There is an elusive optimum with respect to control of such systems. Due to this reason, it is difficult to make a formal expression of such problems. There is always a presumption of the reader being in the context and an appeal is made to his higher senses, by a choice of words, which are intended to resonate. The concepts of morals and the distributed nature of the problem itself, that is we do not have a single port to plug into and control. There are many areas where tiny instabilities arise and are controlled at many discrete points. These distributed problems hence permits only appeals to aesthetics rather than appeal to reason and hence formal expressions might be absent in our discourse.
Note
We are presuming feed forward control only, while thinking of ballistics in finance. The finance adjustments might have feedback on the diligence one puts into rational innovation and its representative accuracy. A highly granulated ballistic might avoid feedbacks.


The Concrete Problem of Control 
An automated system of discrete control strategy selection.

In applied medical decision making, we see that there are discrete regimen which are control strategies applied to specific sets of situations. The threshold of switching between regimen involve decision making by professional heuristics. We see that the threshold might be predicted to a certain extent by the utilization of Markov chains (smoothed by a second order markov chain). 
We treat this problem theoretically that if one could provide an automation of the selection of control strategy by defining recursively multilevel markov chain, so that the threshold for switching might be evaluated despite time heterogenity. It would however need an ordered set of strategies ascending in terms of risks or costs.
Part 2
In this specific case we see that there is to be given sufficient importance to heuristics. We might say of knowledge certain things. Within a domain, knowledge is obtained by induction, wherein the general case is obtained from the specific and by deduction wherein general rules are applied to specific cases. A domain normally has an inverted tree structure, where in pertinant types are classified. Some of these types are repetitive across paralell branches but they might themselves have specific denotations. Say we consider the human stature to be a domain, the specific types that underlie is an inquiry into height and then into weight. We see that the combination of tall and light might be denoted as lanky, while the combination of short and heavy be denoted as squat and so on. Likewise, the leaves of the tree ends with specific individuals under each of these categories. There is no ambivalence as to fuzziness in each of these sets. But that does not hold true when we attempt to relate domains. Say, another domain such as heart health is presented and a similar classificaiton is made with respect to endurance strength, throughput, peripheral blood pressure and these are combined differently in different domain terms to result in multiple sets which end in leaves. Now, there exist relationships among these leaves, which have membership to both sets. But the relationship of a specific percentile of people from a specific subpopulation in domain A to another subpopulation in domain B provides clues as to how the domains are related at the general level. Statistical modelling does this considering random draws from each of the domains and based on assumption of one of the domain measure being a quantile, one derives a density function.
The domain model as we might see relies on specific typing, wherein we recognize the presence of definite classifications and lack of fuzziness. Whereas mapping specific entities between domains presents us with opportunities to understanding specific risks. The statistical distribution might provide clues here by skewness, but it might also incorporate cancelling out effects, leading to a normal distribution. The interdomain links signify causal relationships, wherein the mechanisms might not be apparent, for otherwise, it would have been that the domains be combined. Say like the domains of stature and athletic ability, we might be able to mechanically related specific stature traits to specific sports. In case of domains without direct mechanical connectedness such as stature and heart health, one sees that relating subdomains more granularly might help deduce correlations at levels higher than the individual. The interest of knowledge is in the generalization. The relationship between general categories is less random than that occurs in an individual. The way other than by means of randomness by which relationship could be established is by mechanical causality or in case of mental phenomena by signaling (such as height signifying social stature thereby strengthening prior expectations (biases)). 
Among non negotiating agents, the relationship is one of connectedness (being suspended in free space without connectedness would mean Brownian motion). The connectedness of a squat stature might be distinguished between obese and lean muscle squat stature. In general the occurance of certain combination presents a paralell way to equal levels of fitness in evolution, which again confounds search for causality rather than randomness as the method. But if we move out of biology and sociology, we might see that the association between occurance of certain chemicals in certain environments, might be traced to the distribution of the generators of such chemicals from subtyping in both domains (of occurance of minerals A and B perhaps). Thus, we see that B is a modified form of A due to certain interactions with the environment. Thus, it results in subtyping relationships by itself, creating new domains between the two original domains. Typing itself signifies discrete variation of a continuous property of the general concept. Discrete variations are according to theory of causality caused by external agents. The other type of relationship might be continuous variations like an oscillation. Hence, type systems within a domain signfies causal forces at work. Conceptual relationships between entities are hence as discrete occurance of a qualitative feature and different heirarchial combination of features to produce discrete heirarchial types. The domain hence does not signify causality, but only discrete occurance (as being the materialization of sufficient conditions under the overarching required condition). However relationship between domains among conceptual entities might lead to new domain models of intermediate types, or it might signify a mechanical causality, where one property arises being caused by another directly. Where indirect causation is involved, confounded by various environmental variables, one has to go by either statistical distributions of cross domain models. Say we arrive at a cross domain model of hearthealth and stature, we start with classification between obese and lean and then among the lean we further classify on height. Petite stature might signify a preexisting heart condition which would then give rise to this new classification that uses properties from both domains to produce no causal relationship, but only a qualitative occurance of specific types circumscribed by more general types. 
Part 3
In this regard, if we look 











An Abstract Problem of Control
A statement that control system that synchronizes two heterogenous systems of asynchrnous stability as being an ultimate control system. This might be applied to the problem of control of physical systems (say fields) and of smoothing the divergent goals of a more intelligent system (say a village). A control system that is able to smartly allocate resources to the field and also appropriate the surplus and circulate it among the villagers by evaluation in fair games is an ultimate control system. 
We also need to evaluate if such control systems be nested. That is could there be a hierarchy of controllers running between two subject systems.
The Possibility of Justice
I was concerned if justice could be reached by purely logical means in the early part of my career, which attracted me from a career in law into one of information technology. If a logically consistent system would mediate between conflicting interest, would it be possible for justice to be delivered. For instance, if a factory could be constituted as a separate profit center in accordance with Activity Based consting, would the workmen receive better wages, was the question. This was intended to thwart the heuristics associated with the rendering of justice, where law would have to interpreted and deliberated before a conclusion is made. Nothing flows directly from the law, in fact without contextual interpretation, which ultimately involves heuristics and hence apriori knowledge in the nature of a moral framework. There have been attempts to make ethics a logical system since antiquity. Ashoka had tried this and many from the yogic tradition. Descartes was a pioneer here. But in every case, people ran into the mind body problem. That is to say, the why of the desirability of moving away from a morality based system to a logic based system has to do with the mind body problem.
A moral mind, with high degree of freedom, is essentially tied to the body, which is effiminated by the cravings and the attraction to pleasure, received through the senses. Hence, pure idealism is difficult of attainment due to this problem, often refered to be the tragedy of the human condition. Even if one is to make a bold attempt to train and suppress the senses in pursuit of unbiased and beautiful knowledge, the body has a trick here. A greater than normal idealism, fosters a sense of conceit, wherein the individual starts pampering one of the most powerful of sensual pleasures, one of moral superiority, which prompts him to exercise ostensible authority and seek further respect and comfort, feeding onto a pipleine of greed, which then propogates through a network of followers, creating a heirarchy. 
If we look at the case of physical systems
Development and Peace Studies
Industrial Development – East and West

The idea of industrial development relies heavily on the concepts of abstraction. In the west, we might see that since classical times, there is an interest to describe things, analytically and in terms of abstract pure forms. All the physical world is explained in the west as practical adaptations or corrupt forms of pure forms, such as a curve, line or point. These could be in pure geometry seen to slide and slip to be capable of being seen as the pure forms of the basic machines – levers, pulleys and inclined plane. The machine itself is mainly the frame with fixed points around which lines and curves interact in most cases. Thereafter philosophers developed notions of dynamic systems, where the result of the past served as the start point of the next cycle (involving the exponent) and even when it comes to chaotic movements like Brownian motions, the west was there with a thoery. Thus there were theories of mechanical systems, dynamic systems and chaotic systems all pointing to an orderliness in the construction of the universe. A look at the conservation laws, the absolute points such as 0K ,the exponent and the originality of linear and angular momenta, all supports the speculation that the maker had made it by simple instruments with regular constraints. Thus, theorems are those that exclude arbitrariness. There are no endless sources of energy or self propelling machines. These are the expositoins of modern sciences from the west. It is then sought to be encoded into the way humans construct their  lives as well, using regular cycles and geometric frames to generate things of value, such as a spinning mill producing elaborate drapery from elementery movements. There was a strong reliance on symbols, which might be referential or appeals (as with infinity) in the composition of knowledge.
The initial root of the idea of perfection, of perfect forms, persuade humans to push towards perfection of their actions, without spillage by corrupt redundant exchanges and actions. The frame of the machine was supplemented with frame of theorems in law, social conduct, therey excluding arbitrariness in all actions of value to the society. It is seen as the way of collaborating with nature, as nature itself is seen as an orderly machines and in doing so in more refined ways, humans we might say are more developed than other intelligent creatures. As for the private sphere of the person, who having emulated the perfection of the Maker in the creative activity, modernity leaves it to the liberty of the individual. Thus, the value production and its incrmental perfection is aimed to be an end in itself. The body is seen as a burden in many cases to the pursuit of perfect knowledge. But there were romantics who appealed to the rememberance of the presence of human soul and of the Maker and their essential freedom from theory. Scientific progress comments nothing on this as it is with the point of why, we do things. If one attempts a holistic interpretation, one might interpret it as  an extension of the pursuit of normative actions (going by historic order) or even as a state of wakefulness that had been usurped by the bodily corruptions (as with Platonic past state of perfect goodness and knowledge). Hence, it is ambiguous territory to inquire in the nature of industrial development and there is no way to know of the basic nature of the soul, which might be seen as projecting the idealistic activity, while being embodies. 
The east on the other hand, produced little by way of opportunity to inquire into the orderliness of things. The multiplicity of life forms in the tropics, the quickness in which buildings, people, artifacts (as books and tools) are churned in the humid tropics, the vagaries of the weather and of diseases is crushing of the ego. One sees not the orderliness to the existence, but a chaos. Eastern culture is ambivalent, distributed (even religions like Hinduism are highly distributed, eluding simplistic codification). Nevertheless, there had been interest in attempting to codify the prevalent norms as dharma, which could serve as referential points in the stabilization of the system in the short run, rather than absolute truths. The buddhist middle path emphasizes this. Postmodernist philosophy of the west is greatly influenced by the eastern tradition. It might be despiriting to see how the ideas themselves are greatly influenced by the geographies and what nature to show humans, signifying how deeply the human form is mired in the circumstances.
Even the west, the repeated frustrations over the discovery of non euclidean states, of quantum mechanics and similar theories which decried a complex mixup of continuity and discreteness to the nature of the universe, eventhough was negligible in practical modelling, shook the philosophical roots of the west drastically. The moral right of the west over the development program was lost and they conceded as is the universal apriori law of morality and respect (cite Kant). The world was started to be seen to be capable of being approached in relativistic terms, without notions of central and universal principles. Each thing existed in relation to others and evolved dynamically perturbed by inexplicable forces. The notions of stability, as practical way of securing knowledge about the universe rose to prominence. Theories of relative equilibria, like game theories and of mathematical treatment of stability and control was strong.  
The course of history was also one of outrage on modernity. Modernity seemed to perpetrate injustice. There was a reliance on technical concepts as an agent of discrimination and exploitation, which resulted in war, violence, famines, pogroms and environmental destruction. As said, often times, the technical notions of modernity was used as an instrument of oppression, such as for instance the inability of native groups to write using alphabets is seen as a fundamental case of underdevelopment, which it is the burden to improve. The system as with many natural system is seen as a corruption of natural pure forms, which if possible could be worked upon or be eliminated and displaced by more pliable systems. Even such systems so improved are to be kept under control of the enlightened and pure forms of knowledge bearers. This naturally infused moral legitimacy to imperialism, environmental destruction as burning down forests and slavery. 
The primacy given to idealism in western philosophy was in fact phenomenal. It appears that the explanation for human imperfection rose from the inability to constitute a authoritarian entity that could dictate the efficient use of resources, as alluded in the tragedy of commons. If each one is to know that everyother person would be fair, then humans might tend to be fair. This is capable of achievement only if coercive power is vested onto an authority, which survails the population and exacts a penalty that is disproportionate to the wrong. This is a technique of control, where signals achieve primacy over interventions. The desire for being controlled might itself arise from a desire for equality and justice. Everyperson would be willing to divest his personal rights in order to secure the ability to be able to pick between peaceful retirement and active participation in securing the seats of power in a drive to self actualize. Hence, the desire for equality is powerful as well, which legitimizes the case for centralized control. However, control by means of negative reinforcement had in recent days given way to positive reinforcements, where everyone is uniformly penalized as with taxes and benefits are given to the enterprising who go beyond their bare duties. This seems to be working for most cases of non criminal behaviour. The desire for coercive control and participative control rests on the view on natural truths. 
In all of idealism, it is very common that the idealism being a clear stream that it is is appropriated by agents giving in to their baser drives. The body after all is inseperable from the mind and natural heirarchies and even undesirable conditions such as inflation, uneployment which tend to arrive at an attractor value despite being regulated all point to the tendency of natural truths to subvert those that are idealized in their pure form. Communism is an interesting experiment to put forth the idea of development into rectifying the influence of peer pressure to corrupt. Thus, by planning ahead and allocating resources scientifically, it was seen that people might be liberated from their peer pressure and thus be able to self actualize, thereby building great strucutres, innovating in domains (see White revolution) without expectation of baser comforts. Thus, the origin of corruption was postulated to lie, not in the mind body problem but in the problem of population growth, where uncertainty exists over allocation of resources due to variable ability to expand ones gene pool and reserves. If these were to be mitigated, by communalizing the factors of production and subjugated to central planning, the result might be an invigorating liberation from the necessity to corrupt and people finding their true spirit to create and redeem themselves. But again, the natural tendencies took control creating heiarchies, evoking mythological status to bureaucratic positions, thereby leading to instability in the system, that would cast its spectre, long after it is gone.
This had been the case with many pursuits of humanistic glory, even in seeking godliness, humans seek their own glory. The statues and monuments that praise such glory of liberation were trodden down to dust as history marched on, dramatically seen in the 90s collapse of the eastern block. Coercion as being desirable, rather than arising from contracts was found to be unromantic and the postmodern concepts of development reached new highs in the 90s. The foundations were laid in the early fifties, with the Truman speech on development. The west post the war, adapted a softer stance of projecting its idealistic metanarratives, thereby exerting leading influence on the rest of the world. This resulted in two warring camps – one suggesting a negative reinforcement driven feedforward control, termed the east block and the west which had as its strategy to incentivize mostly by loans and impetus to generated desired outcome and relying on a feedback driven control model. The latter was seen to be more corresponding with the natural tendencies of humans thereby being more pragmatic in implementation, while still being idealized with respect to development. The feedback correction in the case of the east block was very strong and unforgiving of human fallibilities. In the western model, the feedforward control, which involved planning up, created opportunities for captializing of past good deeds and use it as a platform, not to self regulate, but to propogate arbitrariness, in the sense of business expansion and cultural manipulation.
Thus, we might even state the irreconcilable dichotomy of liberty and equality in terms of control. Both of these are social constructs and legitimizes control by a delegated, albeit independent agency. In the equality hypothesis, everyone is aware of the ultimate stable state of human societies and only the really corrupted are not aware of this and would need coercive correction. In the libertarian hypothesis, one is only conscious of his individuality and freedom, the question of his being equal to another does not arise in this situation. If it were to emerge in the natural course of conduct of being free, subject to emergent or natural constraints, the libertarian grants it to be just so. Thus, libertarianism is also associated with romanticism in its acceptance of constraints none other than natural. If one is to nevertheless have a state, the point of allowance would likely be towards coercive taxation and a dynamic, positive feedback driven posthoc control, with no planning. However, an incentive is not random, but advertised, as much as desirable behaviour is generated, the good arising therefrom is ascribed exclusive rights. There is no theoretical way to charge specially for state services for common services, as a result of which while a bulk of state costs go towards protecting the wealthier, sustaining markets and infrastructure therefor, the wealth generated therefrom is assigned exclusive ownership, wherein the owner has a right to exclude from productive use (to simply let it idle or even destroy it). They would need incentivization by inflation in order to be convinced to invest their exclusive wealth into the system. The inequality created by an ostensibly indifferent concept of liberty might be difficult of explanation. It is most likely caused by the established systems of service which provides an implicit feedforward plan set before the general population to follow the money. The wealth is seen as the indicators of the right path and the capital that goes into enterprises sets the plans and standards for action. Thus, a biased feedforward plan emerges from the system, much like the emergent feedback plan of the east block that consigned numerous people to the gulags. This is apart from the great friction these two ideologies caused between them.
Therefore, in a post ideological world no hard stances were taken, which resembles the west in many respects and might even mean a tacit feedforward and a strong feedback driven control mechanism as being legitimate. A genuine decoupling from ideologies such as with the dharma bums, might be difficult of attainment. It would simply imply a certain laziness to control, where there is a great reliance on the natural tendencies, rather than conscious attempts to build. In the east, as said chivalrous enterprise, to conquer and to propandaize is limited by what meets the eyes and senses, overpowering it with the ability of nature to destroy orderliness and leads one to take a more passive approach. It appears there is a critical mass to the move from pragmatism to idealism and that critical mass in premodern east was not accomplished. Hence, the east was pragmatic, free from the idea of central cognizance of control. As said, both east and west are rooted conceptually in the idea of central control. If control is an ontological question as in the east, the strategy does not matter.
The eastern idea of the individual is more in continuum with the environment and the group. The languages are much less definitive in denoting things, but rather in practice often rely on curious tonal constructs (adukkuthodars in tamil) to denote phenomena and states. Thus information is not portable immediately from the context. Likewise, the whole idea of power, control and planning is seen to be illusory in many philosophies dealing with fate and karma. Thus, the east and west took up what they saw to be the nature of the world as discerned by theis senses, not because of sensorial refinement, but rather by climatological and environmental conditions (echoeing Jared Diamond here). The very act of a networked society favoured the lesser determination of facts and caused it to evolve more as a network of distributed concerns and controls. 
As for the crucial question, as to which is the right representation of nature, one cannot help get speculate. Ferrier says that it is important to be true to the method than the truth at the end of it (Philosophy might be wrong, but it ought to be reasoned). The question is essentially one of method versus the end. In the west, the method reigns supreme, even if the end is undesirable, from what one premises, the consequence ought to follow irrespective of favour. The east does not care so much for the method as the result. It says that there are just too many variables to be capable of being modelled in a logical way. It strongly criticizes pure reason (akin Kant) in dealing with the world. In Kantian philosophy, the apriori moral knowledge is powerful influencer, which is akin to the eastern conception. The universal accessibility of ethics with no central authority, allows for stability in a natural manner. The influence of the east on the west was powerfully felt post the fifties. But as discussed above, it also served as a platform for a strengthening of libertarian ideals of the western market democracy in a never before fashion. The indeterminacy of plans and heirarchies as per eastern philosophy when hybridized with the east bloc west bloc dichotomy of western control based model, favoured the west bloc and produced a universalization of the market economy in neoliberalism. 
The idea of development in this point was an evangelization of the free market, democratization of institutions, liberation by education among others. It is curious to see that democracy was a western product, while the west was attaching so much importance to singular truths. Thus, as much as there is an embrace of the method in discovering facts in science, mathematics and from records in natural history, there was an explicit disempowerment of central truths in policy and power. As much as the rational faculty ascended as the king, the human king ought to step down. Thus it does not recognize as much compartments of truth between the world of humans and that of nature and appointed the bureaucracy as the facilitator of actions. But it did not end there, elections and popular mandate was incorporated into the system as an inelegant fix, as a way of appealing to apriori distributed knowledge. Thus, as traditional authority made way for the rational authority, it was also followed by the more intractable charistmatic vesting of authority in the west. 
The dualism of the west was strongly philosophical. We might see that philosophy is self referentially extended about a null space. In the west, this sphere of emergent truths was provided for rationally. Reason could appeal to imperceptable entities through symbols (such as infinity) and hence it is possible to frame within a rational framework, the highly variable outcome of mass opinion and construct over it. Thus, the west used a dualistic extended philosophy to reconcile its simultaneous interest over methods and results. The west thus celebrates the possibility to philsophize along any axis. One might promote romanticism and it would be a legitimate idea in the system that is completely idealistic and indifferent to rewards . We might say that rationality is a narrow term to describe its philosophical ideal plane. In this we place the reference to what could not be known (Agnoilogy), such as null spaces and infinity might be  the basis of ontology, rather than epistemology (to contradict Ferrier).
In the east, everything is grounded to the reality and context. The philosophy of the west is rooted on symmetry. Somewhere it is said that imagination is the true mark of intelligence. There is a lot of construction (followed by a postmodernist deconstruction) of things that do not exist in reality in western philosophy. Every construct has its negation, thus signifying a zero sum plane (where it might exist irrespective of rewards, being conservative of energy). In the east, there is still construction, but of disparate objects that have mythological significance, such as that of varna and jati. It has elements rooted in an open energy system, where mythological truths consign a certain direction of actions, which inevitably involve energy flow. In the western construction of imperialism, the logic was to balance exploitation of resources and markets as a way of modernizing the natives. It was idealistically placed as zero sum, though a definitive energy flow existed, much to the repentance of the west, when invoked by people who argued their way to freedom, such as Gandhi. But in a mythologically oriented system, the definition of concepts do not have negations. They have natural adversaries, such as varnasharama dharma and bhakthi, but these are centered on a internal consistency of moral rightousness rather than referring to the other critically. There have infact been theoretical discourse in the east as between theological schools of dvaitha and advaitha. But the hollowness of such debates had been much criticized in eastern culture. These had often been the preoccupation of the wealthy regions of India, such as the ganga yamuna doab, the cauvery delta and the godavari valley. What was the general pattern was the prevalence of mythologies which do not refer to others critically, much like tribal lores. Everyone knows of the existence of conflicting propositions by the others, but remain committed to a narrative. That is to say, the pluralism was qualitative, rather than orthogonal or antagonistic as could be construed in western philosophy. 
But we might say, that philosophy starts as a necessary appendage to plentifulness and different cultures or populations had experienced different levels of philosophical development at different points in history, as the circumstances permit. Thus, it might not be inaccurate to postulate that the east and west might lie in a continuum, in which in the present epoch in history west holds a superior access to philosophy. In each philosophy, there arises ways to appeal to the unknowable followed by an ontological construction which is capable of epistemologically appreciated (it is moot here if ontology preceeds epistemology, but we believe so). All civilizations had embarked on this trajectory with enough resource surplus. Hence, it might not be too much to say, that had the critical mass that triggered imperialism not reached in the west, these cycles of philosophical sophistication might proceed inedefinitely.
This created inter-tribal spaces which were not nurtered and people lived in their mythologically embedded reality. This kind of a setting relies on homogenity for stability. But as the west marched on, the clash with highly constructed civilizations, which were able to generate sustained assaults of grand scale on the pragmatic population, caused instability. Suddenly, alternate power centers arose, weaponry and business arrangements where sophisticated and this lead to a feeling of inadequacy coupled with an identity crisis in the east, both of which were capitalized by western hegemony. When the moral inadequacy of the west was slowly rising after the world war 2, the east had still not recovered from the crisis of inadequacy. The development around the objective criteria of philosophical development might be a more general scale, rather than what could be supervised by a western system already in the crisis of confidence on the strength of their philosophical frameworks, thereby cascading a slide onto an uncontrolled embrace of a chimerical deliverance where the fetters had moved to the system itself beyond the reflective abilities of humans. The state of the west as it stands might be more hollow and be entangled in the nature of things, of a massive complex of capitalism and postmodernity,that it might not be able to provide a sound advice at this point. Hence, development as a concept ought to be objectified and capable of being done so. It requires the cultivation of abilities to produce philosophies and arts and a reasonable surplus in the nation of interest.
Path Functions

On the possibility of using path functions instead of state functions. This is particularly intended for appliction to incomplete information scenarios – such as medical prognosis. Where a state inspite of being designated such (say a diagnosis of tuberculosis) is qualitatively heterogeneous. It defeats the central tenet of dynamic theory that a systems present state should convey all information concerning the system. Where the past comes back to influence the state (such as with initial conditions of the host), we have a dilemma. Then the entire path matters rather than the state. We have already attempted to recursively smooth the trajectories by using multilevel markov processes. Here we approch the problem theoretically, in that where there are multiple axes, we attempt to find if a joint probability distribution could encode meaningful signals, which emerge when the distributions are joined. 
In mathematics, we are interested in linear or planar truths. Linear truths involve additivity, such as vectors along a path, coupled  with trigonmetric functions to obtain planar truths. Where products are used algebraically, it would mean an explosion of dimensions, only those quantities which could not be qualitatively reconciled would interact by mutual scaling and the result is to be seen physically to reside in multiple dimensions (such as F=ma for instance). Vectors being multiplied are rare in physical world (radius on angular momentum is one example). Such multiplication means that these vectors are acting on each other as if from different dimensions and their result lies in a new dimension. 
When there is a power function, we talk of a growing system, which quickly multiplies. The exponent is the limiting case of such multiplicative system. That is to say, if a quantile is raised to a power, it is multiplied by itself iteratively by some number which is the power. As the number grows, the rate at which it grows is a linear multiple of the exponential growth. Hence, growing functions are recusrive and the recursion irrespective of the number of times it is called, is as if 
That being said on the nature of multiplication, In probability theory, when we talk of joint probability distribution, we talk of orthogonal dimensionality of the distributions coming together. We speculate in this theoretical search that there are some slices of the distribution where the informative parts of the distribution are joined together. That is to say among normalized dimension, we might say the area about the mean is uninformative. Outliers beyond say one standard distribution, might form a shape (visualized in a polar or paralell projection), in which case, we might be able to identify certain desparate elements of a set. 
Part 2
If joint probability distributions were to show interesting values, then it ought to signify the existence of multiple unique equilibra to the system. Let us assume a system being described in a system of coordinates projected in polar projection. Among the axes which are normalized, the line joining the mid point or the mean is the natural equilibrium of the system. However there are certain haphazardly joined lines that connect outliers in each dimension in a specific pattern and these are more frequent as a group than they ought to be. These signify alternate equlibria of stability of the system. For instance if we look at the way we link tallness, poverty, alcoholism combining at extreme levels but producing moderate anamolies in the anamoly axis, and this is a stable equilibrium, we might be finding a genetic discontinuity (or a distinct subpopulation) which is highly alcohol tolerant. This examination of alternate equlibiurm which are emergent over multiple axes, help in the direction of biopsychosocial model of illness (cite The Biopsychosocial Model 25 Years Later: Principles, Practice, and Scientific Inquiry).
Thus, we might inquire on the theory of emergence of multiple stability equilibria of systems when described over multiple axes.
On the question of neoteny/geekism as Development

There are two entities appealable only – null and infitinty. These signify nothin and everything. All that is inbetween are engineering actions and they sometimes approximate these ideals as such as in derviatives - dx. We might denote this as [ -inf ->0 -> inf ].
We might talk of human endavour to working in the domains of the home, idealism and materialism (these domains had been dealt with in an ancient discourse on ethics - thirukural). Home is about a caring for people, involving satisfaction and contentment. Idealism is to find solutions in the ideal plane. But in the ideal plane for each arbitrary truth a there exists a negation to a ( ∀A∃!A ) . That is to say, the idealistic system is self referential. It does not apply to real world problems. It is closed on itself and does not do any real work. The infinitude and nullity of theory itself might suggest that the world itself might be devoid of any fundamentally interesting characteristics. But this is realized only in the idealistic points. In the infinite past everything was zero sum and so it is in the infitite future (Just collate interesting behaviour over large individuals and over long time and we would see randomness emerge). Likewise in the imperceptably small instant nothing exists (Plankhs constant suggests non existence of such infinite smallness) . Empirically we see for a large enough interval , we see that things do not converge towards pure randomness. Pure randomness signify zero sums closer to theory, but it sonly over large intervals. For a finite intervals, it is empiricallly such that for every A there does not exist its negation. 
Thus empiricism might be seen as a measure of heterogenity in a finite interval. Finite intervals are thus qualitatively diffferne form the null and fhe infiity. These could only be created by a product with zero or infinity (but zero isa number in algbra, sometimes seen loiclaly inconsistent). The paralell lines might be a seen as converging towards the infintidues (in lenear reasonging that is , refer cirte the mind is flat).

There ought to bhave been some initial conditoins or ‘hand of God’ that introduced the heterogenity that makes it worthwhile to do science, while infact it deals with finite imperfect concepts.
3. Thus, materialism is not to be discredited. We only seek to propose a dualism of home and idealism with the materialism being an instrumental concept. But even such hybrid idealism had been catastropic in the past.
4. In pure materialsm many a good soul eexisted such as the nerdy game playing man child like typical fictional characters (say Sly in scorpion). These are milleniels, urbane cool people, who sun idealsim and see there is no need for moral outrage (home-people related intelligence) nor ideals (as with geometry). They see the world can take care of itself (self stabilizing) unless some one comes around talking idealistic stuff. Hence just taking baby steps (such as doing the next right thing rather than making overarching plans) is seen as sufficient. It is not discredited here too – it works and if is even theoretically consistent. 
5. Hence all that is left in philosophy is to appreciate that nothing is bad except dogmas. One ought not believe that  home plus idealism is always better than materialsm. Only that materialsim ought not rise as a dogma. Here the end seems to be persistent repetitive humility. This humbling of mankind is hymns to the glory of God. Philospohy allows for this reflection and glorification and hence might be an epitome of development.
6. We  ought to honorably mention the role of pacifist millenials who by cirtue of their finitidue based approach see that all actions are simply game steps. They see that they do the next best thing not towards some vague ideal or as a way to set right some outrage (or extreme suffering) but only as a game move, in their own little world. 















The mendicant leisure class and the secret service






























a

































Emergent and Constructed Intelligence
We have earlier seen that intelligence is highly embodied. If we are to think of the various things that constitute our consciousness, we would see that apart from the rational faculties and sensorial information, there are a lot of innnate feelings, urges and barely perceptible motivations in our system. This had been alluded to by Kant in his concept of apriori knowledge and Descartes in the concept of the soul. In this case, we see that the universe might be an unfolding of history, wherein there are so interesting and random occurances as to deny the existence of free will and intelligent actions. The mind, we might say is inseparable from the body. But as we get more and more glimpses of the universe, we also see that in the interval of finitude that we have grown to be capable of observation over years of intelligent actions, we see that there is some pattern to the unfolding of history. If the universe were to be an unfolding of history, why is there patterns to it. What is more interesting is that these pattern indicate the use of repeatable processes, which evidence the use of instruments by the organizer of this universe. And what is even more interesting is that, we see imperfections to the application of these instruments. It is as if God is running a gigiantic printing press, putting together all these marvellous creations, by simple overlaid patterns of CYMK dyes and patterns and the impressions they leave, actually bleed. There is no perfection to the printing press, in the same way as it would be had it been put together humans. The orbit of mercury, the quantum states all indicate the imperfection. Likewise, our inabiliy to make sense of the world in a perfect form, also arises from the inherent inability of the world to fall into any pattern, however abstract it may be, nevertheless showing some patterns. The incompleteness of mathematics as a system was vindicative here.
But if we observe that intelligent life exhibits property of growth, of gathering up things and organizing them. We see automobiles, buildings and roads all put together by the intelligent form of life that we are. Similar efforts have been made upon nests and burrows by other creatures. Given that all intelligent action arises from an observation of what is called a phenomenon, which itself we speculate to be an intelligent action of some superior being, we might say that intelligence is nothing but a projection of the past upon the future. Thus, all intelligence arises from being able to observe the works of a great maker, who had already exhibited intelligence and project these patterns on the creation of things within our intelligence. All physical laws derive from observation or an introspection of our natural intuition (which itself might be a form of empiricism – so sorry theorists). Afterall mathematics and logic rely on our ability to sense asymmetry and orderedness from an intuitive aesthetic perspective. The intelligence thus projected upon the future, allows us to rise to the level or the creator, or does it. The creator here, given that he is toiling with imperfect tools, but with admirable intelligence, with no apparent purpose (other than seeking fulfillment of some visceral motive, planted in him by some superior Creator) might then, perhaps be assigned a name of an angel. Likewise, there might be an infinite stacking of angels who seek out to a mighty Creator, who defies all understanding and references, other than perhaps an appeal through symbols (as with infinity). One might look at the story – “I don't know, Timmy, being God is a big responsibility” published in Bostrom’s website. 
This being so, it puts us humans with the possibility of actually using intelligence to create intelligent life, for intelligent might be recursive and arises from application of imperfect instruments to perfect concepts on random subjects. This, however again begs the question of why, as to why would humans consider elevation to angels, a dramatic goal of development. But the angels do not seem to be greatly served for all their exertions, the beings are hardly conscious of them, let alone exult them. Perhaps they do it by virtue of a deep phenomenon that pervades the universe, perhaps the Maker had set in motion such a thing, in his whim, we would never know. But what we might be witnessing in artificial intelligence, is an irrational drive to realize the maximizaiton of our intelligence, which would then be the emulation of the intelligence of the superior angelic being. 
This point of view also contradicts the emergent theory of intelligence and of the mind. There might actually be a intelligent exertion in putting together finite components and the complex phenomenon might however emerge therefrom. We might even surmise that life is not a miracle as much as we give credit to it. If one is to dance in a hall and sees guests coming in. One might convince and pride himself as being the spirit of the party. But sooner than later, one would be likely discover someone else taking over. The delusion of being singular and unique is shattered much too often in the world. There is only left a particular quircky randomness to claim ones identity for.Otherwise, it is just a matter of observing something before being convinced of the mediocrity of oneself. Hence, it might be that some universal spirit would start animating things, once we put together a certain intelligent construct. Likewise, as much as human embodied intelligent is continuum from the past, it couldbe projected  upon the future as well, in which case we would be arguing for the case where the soul is inseperable from God and God had in fact projected it upon us, to carry things forward, in the principles of advaitha. 
PS
But it is also not possible to dismiss the emergent nature of intelligence. There does exist a continuum from unicellularity to multicellularity, to social intelligence (say amoeba, portuguese man o war, fishes, mammals). We might look up the work of Extavour to characterize the persistence of game dynamics even in tightly integrated multicellular creatures, thereby creating intelligence as an emergent function of simpler machines. (cite Extavour). There might be premature experiments in eusociality in insects, but which had since been superseded by behavioural approaches such as nesting and rearing young. But it might as well be that it is not the only route to intelligence. In that intelligent construction of intelligent machines might be plausible, we ought also to realize, that it is the imperfections of the construct that causes it to become intelligent and endeavour to perfection (or completeness or more emotionally fulfillment). Thus intelligent might be the reside of imperfection, that arises like a ghost to attempt to perfect the setting. No, wonder Wiener (cite) was postulating intelligence as a negative feedback. 
Despite, multiple theoretic views of intelligence, the question remains if we could consciously construct intelligent creations, given that there is evidence of such happenings in the past, by angelic actors (in the physical plane). Whereas biologically, there is suggestion only of emergent intelligence. We might see the whole as a way the imperfections or chaos in the initial configuration of the physical universe combining to create an orderliness that it had been missing, to produce what might be a simulation within it of more perfect intellgence. 
Thus we might clearly distinguish between physical and biological intelligence. We might say that biological intelligence is a feedback loop that arise from the imperfection of physical intelligence. This control loop is probably not orchestrated, but is a part of a self stabilizing system. It might just be ‘keeping up with the time’ concept of stability, a dynamic stability. Thus, at a dramatic stroke of a developing feedback mechanism, it might happen, that humans might be able to engineer something that is as big as a solar system or a galaxy. Here they might alter the physical universe to the ends of greater orderliness (thereby stabilizing the initial project). It might not have biological connotations at all, it might in fact break away from the concept of life itself. The mind might at last be liberated and it might actually work on problems as big as a galaxy. But if we question the human endeavour to introspect this, and label this process, one sees something of a contradiction. It shows an awareness to seek the bodily root of this process, the question of why after all, we do this. At some point, humans might succumb to the constrains of their emotions and never attempt the liberation and it might be a limiting case of the feedback, of stability itself. Or perhaps it might involve an altered universe, towards greater stability, with fundamental designs having been altered in which the feedback might start in genesis. This afterall points to an anthropic view, where the fundamental dilemma of the universe echo in our mind. Hence, all that remains is a dilemma and we have satisfied ourselves dragging ourselves to the edge of the cliff to take a peer and Camus was wise.






The rise of nerd clubs

We have seen that there is presented an eternal dilemma at the end of this search. For a dilemma, the solution lies not in static or final decision, but in the nature of context sensitive decisions, which actually serve in a manner that is like righting the boat. That is to say, given the dilemma between choosing a biophilia (cite) based intelligence and an eternal and more pure intelligence, the choice is to be attempted contextually and given our natural tendency for symmetry (with  minor imperfections) we seek to balance the situation where one side of the dilemma is systematically more supported for, in an arbitrary manner. Thus, it is for the preservation of the dilemma that we work, atleast in the public domain. In fact the symmetry might itself be inherited from the natural world, where a great deal of symmetry exists except for a few (electric unipoles, entropy, the problem alluded in subatomic physics). 
We believe that it is important to preserve our choice in the presentation of systematic progression in a particular arbitrary direction, that goes against the human attunement to higher order intelligence. We have repeatedly alluded to the case where we might have passed by some golden age in the mid of the past century (in our older books like Computerized Societies). There used to be problems in nature, which was restraining of humanity, which lacked the appeal for the aesthetic and hence was the subject of much engineering approach, rather than evolutionary or biological approach to intelligence sometime since the enlightenment. But it would be a mistake to think that intelect had developed in the recent past of mankind. A tribesman say 3000 years ago or the ancient greeks have substantially the same level of intellectual ability as modern people. They lacked the agumentation of modern instruments and the construction of the theories which we have access to now. Thus, when faced with the problems in nature, humans attempted to build reservoirs and ploughs to make it better for mankind. The dynamics of the struggle with nature destabilized the human society caused it to oscillate wildly as it sought new equilibria. Settled life created opportunties for raiders and exploration created opportunity for crafty businessmen. Thus, the pursuit of truth, the singular source of absolute goodness had prevailed in history since time immemorial. The various local customs and mythologies are appeal to this universal truth rather than local truths. The speculation of the existence of the truth and not many truths has been well documented in greek philosophy and one can see its asian counterparts in the ancient traditions of Buddhism and in the philosophy of advaitha. This pursuit of universal truths is an attempt to incorporate the intelligence of the angels in human action and thus redeem themselves from the human condition of embodiment. This would hence need a decoupling of wealth and rewards from the pursuit of objective knowledge, as in the traditional academic campus. But the pursuit of intellectual awakening still has to reconcile with the plurality of approaches, since even the angels seem to be imperfectly grasping the truths. Hence the academic pursuit of truths have been dogmatic and had always placed in high regard the intractability of the absolute truths to which only an appeal could be made. This is reflected in the spirit of romanticism, ritualism and celebration in academic campus along with the principles of decoupling from rewards and material preoccupations. 
Thus pure theoretical pursuit and romantic inclination are an unlikely combination for those that pursue intellectual pure truth (and not merely biological truths). Thus, the dichotomy of biological dynamic truths might be fundamentally different from the mechanical truths. The former is pursued by the computers (trivially established of their dynamism in their capacity of being represented by lambda recursion). The latter is more mechanistic and involves development about axioms (such as the angels use with constants like exponent, pi etc). The dilemma of these truths is pervasive, say when Camus said that suicide is the only philosophical problem, because it suggests that the essence preceeds existence. Kant was suggesting that theoretical constructs are closed on themselves and they derive from  a small core of apriori truths projected about a null space in all directions. The development of such theoretical pure concepts, is like a perfect setsquare. Only with the instrument of theory could we percieve phenomena or interesting heterogenity in finite intervals. The persistence of this dilemma implies that we ought to take a stance where we either follow the current or attempt to appeal to some higher order aesthetic balance and it might be a very important decision that we make.
This adherence to intellectual truths that is detatched from the embodied intelligence would anyway involve a satisfaction of the body, but the question of which preceeds which remains. In one side, we may think of the pursuit of objective knowledge is an extension of our embodied intelligence and in another we may see that the body merely anchors us to a historic past, now rendered obsolete, in that we have our eyes  opened to a greater and grander intelligence. The latter is the school of posthumanism, where there is a drive to detatch humans from their bodily existence and emulate them in what is called the cyberspace. But this would mean a conclusion of the dilemma rather than preservation of it. If the dilemma is to be preserved in a given context and point in history, the subscription to the universal higher order truth would require the absence of dogmatism. This is a subtle concept, in that the proponent is prevented from appealing to higher order truths explicitly. He would then have to appeal to higher order truths with humility and ought not attempt to find fault with and undo history. He would then be left with a position where he does appeal to pure solutions, even in the face of extremely tough problems, such as say terraforming planets, rather than extending his biological capacities by dynamic devices. This would be an appeal to intellectual fulfillment, like some game, rather than material enrichment of a hedonic nature. The proponents would most likely then be virtually angelic in that they attempt to enrich in accordance with their telos (as Aristotle said) and pursue the game of finding intellectual solutions to every complex problems. They would also engage in differing in their ideas due to their recognition of imperfection inherent to such pursuit, where they appeal to nature by contemplation rather than analysis. This would generate a conflict, but this conflict would be pure in that there is a sportiveness and a principle of the better man wins, in proving certain points by ardurous experiment or by theoretical exercise. It may even extend to duels and battles to prove points, but it would be governed by the code of chivalry and the constructivism of it, rather than being driven by the phenotypes. Thus it is neither an idealists den, nor some frolicky escapade of nerds. It is rather like ancient orders of knights, pursuing pure principles (not just restricted to local kings, mythologies and national interests) but universal truths, to which these local concepts appeal to. It would also involve development of rituals and local mythologies which appeal to these concepts and the preservation of the spirit of art and contemplation. The key is remember the telos and not be swayed by temptation.
In the past such idealist conceptions like the leisure clubs of England were the origin of exploration and a pursuit of nationalist glory. The glory in a pure romantic conception was an appeal to the glory of God. But it is often made crude by appealing them to material and reward based concepts like race, locality, a king or a mythology, in such a manner that it would lend itself to authority over people, rather than knowledge. The pursuit of such knowledge might result in the ability to control natural elements in its stable state, so that people might form into natural heirarchies, in a manner as how cooperation emerges and stabilizes the population (literally in our times, the population count had stabilized, by emergent equality, people no longer care about having many offsprings). The solution to natural problems result in emergence of authority and control in the society. But the pendulum had swinged away from the sweet spot, with the rise of postmodernity. That is to say, the development of ubiquituous computation, results in dynamic development much like evolutionary development, thereby by the entities of markets, of enforced normality, result in control by self implicated means, much like the natural forces. 
Hence, the development of a social setting, where the problems at hand, is handled by means of intellectual pursuits, by experimentation and adversarial search, by romantic contemplation and by suboptimally using the resources (in an epicurean fashion, in such a way as represented by the campus, with extended resources not being harvested at haste) is a good way under the circumstances to appeal to the universal truth, again as a civil endeavour, among people. It would thus, extend the individual concepts of pain and pleasure and mitigate it by dispelling loneliness, by mitigating solutions to natural problems, by social absorption of the outputs as well as allowing for joint puruit of philosophical consolation. Without a social setting, humans find themselves unable to account for the imperfection of nature. Hence, it would be a legitimate attempt to create such social groups as the contextual missing piece. Loneliness also arises from the nature of finite life. The information in genetic nature is conveyed by other species, but the critical realizaiton (perhaps as a critical mass phenomenon, of the awareness of non bioloigcal order, or a revelation perhaps) could only be sustained by civilization.
The primary intellectual activity of such campus groups would likely be ones of control and the pursuit of theory of control. That is to say, as much as the social group attempts to disrupt the pursuit of market based intelligence, the discrediting of apriori goals and feedfoward controls even in the most conservative institutions (let along corporations), would involve development of a sound theory of control. This theory which when applied to individual situations, would reveal their phenomenological disposition and thus, capable of being neutralized in the sense, that the heterogenity in the finite interval is absorbed in a theoretical frame of conservative neutralization, which is the essence of control, to be able to theoretically reason over things and thus be able to bring about a neutralization at a desired speed. Thus, control would mean from a political standpoint to identify the diverse factionism, which essentially mena nothing theoretically and thus, applying suitable incentives and sanctions to bring stability. Likewise in natural systems such stability could be achieved by controlling chaotic swings and interactions. The end of control is to build platforms for further engineering development by exploration.

Technicalities of Control

Control is a way of neutralizing in some domain where control is no feasible. Thus, it would require the existence of a domain which could not be absolutely controlled and hence would need an instrument to control it. That is to say, if we have it that wind currents could not be controlled, we can control the evacuation of power supply from windmill networks and schedule maintenance of these mills. This could be attained only if the windmills could be capable of quantification. That is to say, if we have a theoretical framework, about a null space, then the windmills could be quantified in terms of distance from the null space. This could hence be capable of being stated as equations. It also allows construction of closed systems, where the sum is zero, while different parts could be brought together. But as said, the motive of control is being able to neutralize energy volatility, much like converting energy is the point of a machine. The control system does not do work, it merely utilizes the volatile source of energy, appropriate portions of it as reserves and uses it to neutralize the volatile system. For this it requires a medium of disciplined system which can identify and actuate actions based on the control policy. That is to say, there is a need for a definite phenomenological entity on which stabilization is normatively preferred, say A. The second thing is a mechanistic theory on the definitions of the parts of a control system, which can convert the signals from A into quantitative entities, a system B. This control system would be able to do work on A by logically mapping the signals from the system by quantitative mapping a phenomenon a1 as a scalar variant of a counter phenomenon b1 and if b1 could be imposed by B upon A, then the system might be expected to be stabilized. That would require the identification of a mapping between points a1 and b1, which would presume that a model of the system being controlled exists.
Thus, a linear model might be constructed of the system A as system B and it could be analyzed with linear techniques to attain stability. Where the system A is difficult of being modelled linearly, due to its dynamic nature (in that it evolves to the control signals) or because only incomplete information is available about it, we require more advanced mathematics. A system of which some information is known via a set of equations, might be of any shape subject to the well known constraints, in which case we can work to maximize or minimize it to an objective function. 
In case, where the system is described as a logical tree (as a partially ordered set) which has fields that could be mapped to various quantiles which are strictly ordered, then the system could be analyzed using computational techniques. The system is computable only if it is a tree, capable of being recursively constructed by a lambda, which would be able to populate the leaves using quantiles from a strictly ordered set. Normally functions are said to be computable. A function is actually a blackbox system with a function, which reserves the right to conceal its structure. If a system is presented for inspection, if there are points capable of determination in the system and these points are connected by means of paths which are constrained by rules then the complexity is less. If there is less known about possible evolution of the system, where the points are capable of development in time along any possible pathways, then the systems complexity increases. That is to say, complexity of the system arises due to its dynamic behaviour, or randomness in adapting states. The randomness itself due to paralellism, or inability to locate a single algorithm that works the prime mover of the system. The complexity increases exponentially, where there are multiple algorithms working together (or does it, consider additivity of waveforms). 

On Why computers might get banned.
So far our critique of computing rested on metaphysical grounds. We seek to make it more mathematical by this exercise. 
The Turing machine model of the computer is capable of searching trees, which is a very powerful feature. But is logically flawed, in the sense that trees are only partially ordered and the question as to why one would want ot seek qualitative and quantiative search targets mixed up is lingering. It seems like an arbitrary action. It could also be seen as an instrument which can introspect and control its own state by recursive reference outside of its immediate cursor. No machine, physical or abstract could do it. But it is in practice an excellent control model. If we take Ola for example, it dynamically stabilizes the system that is normally tumultous between cabs and passengers. It would not be inaccurate to predict that it has a feature for searching trees. It first looks for cabs in the main roads before searching the side streets. This is relevant and works only because drivers choose to park the vehicles on the main roads while waiting for pickups, because they foresee it to be  a better way to get pickups. Thus, we see that there is a lot of reflexivity in the control system.
The computing power is able to emulate human behavoiur as in the behaviour of a recursively goal directed agent. This kind of machine (animals) which are reursive goal oriented arose as a consequence  of application of pure concepts to the physical world by the agels. The resultant turbulence due to imperfections gave rise to the need for dampeners or stabilizers, which worked on th ebasis of recursive goal directedness, which makes a very good loop, while being self stabilizing within itself. Hence, the control problem involving two systems A and B as aforesaid was working well because B is able to self stabilize and could hence form a good control system over A without high computational costs.
We are also posed with the question as to whether mathematics is a complete system enough to critique the logical flaws of computing. Computing emulating human behaviour is classified as biological intelligence, while mathematics is classified as pure intellect. Mathematics in fact has more inconsistencies and arbitrariness to its techniques, such as the infinitisemal, asymptotes or even an embrace of contraditions in the dirac function. But we think these are techniques which exist to quickly make us see the point, while there does exist more provable methods to reach the same ends. Hence mathematics might be a consistent system, but an incomplete one to some extent. But it ofcourse follows that it is a matter of degree in the inconsistencies between mathematics and computing. The selection of certain degree as being good threshold is arbitrary. But that is the essence of the dilemma. There does exist an arbitrary cutoff where the imperfections of a higher intelligence ceases to be the residue and becomes the driving force of a biological feedback loop. 
Thus for a ny ssytem B in order to stabilize a system A, when it comes to reflexivity (contextual choice of techniques), there exists a threshold beyond which A and B coevolve and this arbitrary point seperates pure machines from dynamical machines. The culmination of the feedback is to be able to alter the pure machines in fundamental ways (such as what humans endeavour to do – to be able to engineer something within higher physical laws that could somehow make things perfect at a stellar or galactic level). Thus, the question is that whether the dilemma exists as to whether to prefer a choice to culminate the feedback or carry it forwards by engineering biological systems, which present a case for transhumanism. That is to say, transhumanism thinks that there is a need for further engineering biological systems, which would in fact rise as the progenitors of the evolutionary feedback, to the extinction of the existing human race (or deprecation), before the intellectual attempt is made. The transhumanist philosophy thinks that with the existing embodiment the endeavour is unachievable, eventhough they agree on the goal. The continuity of consciousnes between the present form and the future one is highly debated however.
The continuing of evolutionary pathway, might lead to instability is what is postulated here. It might lead to a highly divergent equilibrium (refer our old take on why the singularity would split – cite). In the savannah phenomenon or more generally in evolution, there had been repeated empirical evidence to the effect that defection arises whenever a system is highly stabilized in order to carry out adversarial game like stabilization at a higher level. Thus, the solution to stability might be construction of imaginary games which could simulate higher order conflict rather than allow them to unconsciously enact them.
That is to say, if there is a need to address the mind body problem, there is needed of demonstration of a will to enact the adverserial search as an important stabilizer by being able to retain control, as one controls an experiment. It is important to be able to address the stability of the biological feedback, which always attemtps to get ahead, before attempting intellectual exploration. Therefore, it seems that there is a need to attain an universal consiousness of this cause, either to be enacted as state policies or popular cultural influences so as to attain the end of holding control, while also being to enact plural experiments. This brings us to the case of the Nerds.
Part 3
In our much earlier work on Computerized societies, we addressed that there might exist distinct populations which confront perpetually. We had discussed one of the population to be the ‘common man’. Now in the light of more information and deeper metaphysics, we might develop this system to a greater accuracy. The common man might have been the nerd, the geek who is romantically disposed and at the same time unwilling to accept unexplained development. He was said to be civil and clear and straightforward in his conduct. He exhibits an apriori allegience to morality and only participates in struggles of power, where the rules are well set and the criteria for success and its measurement well appointed. He might chivalrously come back to grant the failed one a chance in a next battle. We have also talked of a more primitive people, who had acquired power and determine it to be an extension of brutish strength which had accrued to them by genetic dispositions, which might itself have been powered by the flow of history. This subscription of the randomness of power and advantage, allows them to claim legitimacy to any act that as far as the ends are clear. They do not distinguish between well appointed battles and backstabbing. The nerd subscribes to the idea of seperating intellectual advantage from physical or skill based ones. They  thik that it is civil process to decide on the games first and them step into them. The brute thinks that intelligence is an extension of embodied power and hence, they get out into the fight on sight. In this situation, the position of the queue cutter is most interesting. He seems to thrive on the ambiguity of these labels. They so much are pragmatic that they assign themselves the label (as a queue abider or a brute) according to the situation. This refutation of idealism and in fact considering it as a springboard for brute force is what makes the situation peculiar. The brute does not want games to begin with and the destruction of the dialectic, is what presents as an important problem of control. 
Hence, there came to be designed in history, a multilevel and distributed system which relied on checks and balances and heirarchies to retain control. That is to say, as the nerd and the brute could not agree on the dilemma of mind body problem, there arose a new form of brute who could take advantage of the engagement in dilemma and use it as a means to achieve his goals. This could only be stopped, if the nerd and the brute while they were fighting, could assign a part of their allies to carryforward the dialectic and dilemma to higher resource bases, defining authority about papers and intellectual systems to constrain the brute from out of bounds actions, while allowing him to assert superiority within game boundaries. This balance was however constantly endangered by the questioners of the essence of the dialectic itself and this leads to an inelegant, adhoc system of control that is evolutionary and dynamic. 
At some arbitrary point, we said that this kind of pragmtatic brute, who does not recognize control itself grew so numerous that they started to become capable of institutionalization of their rejection of well appointed games themselves. The accompanying pain and bloodshed was avoided, because the softbrute, as we call this category was able to port the struggle, to a different environment. The point to recount is that both the brute and the nerd had morality. They were contending upon whether a game is necessary or not, the brute withheld attack till such question is settled. They were contesting the merit of their apriori truths, one of brute force and one of interpersing struggles with rest and retirement. The soft brute has no apriori truths and relies entirely on feedback to guide his actions. This it was predicted might give rise to a complex, wherein the softbrute is strengthened as a synthesis of the dialectic between the brute and the nerd. The softbrute carries the struggle to a different extent, to the point where both the nerd and the brute run into identity trouble, due to inability to define each other by contradiction. The softbrute however, as he ports the struggle to machines which are recursively goal directed, in terms of preservation of self and growth, gives away little by way of control. The software system that could navigate trees and act upon events, could not be proven, only be tested. Hence, entities that hold them in ownership are as much controlled by them as they control them. The new model becomes a fine grained network of highly reactive agents. This web of stability it is said offers to stabilize the dialectic of the brutes and the nerds. 
The outcome of the complex was offered to be that there would arise a mainstream, which is so out of control and evolutionary, that there is promoted an interest that does not arise from a will of people. This propogation is predicted to lead into a control system that is so effective, that it is able to define goals within itself, or in a closed manner. The system B is able to stabilize A because the goals exist within B itself and has nothing in terms of  an agent C who expects the stability of A as a way to produce surplus for his domain. (The agent C is humanity). That is to say, the question of control of A becomes so well formed, or well posed that there is no dilemma in it at all and it could be settled within B as a closed problem. This is only possible if B is able to set up a suitable adverserial determination platform as to determine the dilemma at higher frequencies than A. But it is doubtful if C would be able to interpret the dilemmas of B that is expected to overflow it at some point (the self referential truths within B being as much synthetic as it would have been in A). This is because B might not have an access to moral codes that involve the apriori concepts like pain and happiness that form the basis of the dilemma for arbitration by C. There is no guarantee that there would be a simple correspondence between the two. Likewise, there is no proof that more elaborate games and plurality might not capable of being orchestrated within A itself by C, so as to allow for B to sustain as a governing system that could continue to serve C. 
This would mean, that there is required of more zero sum games within A, that are actually nearly zero sum and not purely so. This conflict could be produced only by setting up countercultures or nerd clubs is the argument advanced in the work (computerized societies cite). The setting up of this plurality and spirit of challenge, is likely to generate enough plurality in the system as to permit derivation of pure completely objective pursuers of truths, in the form of a campus. They might be insulated from the concerns of biological survival, by the chivalrous proxy wars or games, with very small rewards in the open system sense. The development of such mythologies might be plural and capable of being constructed recursively so as to be capable of producing a self stabilizing system that is denoted by coarser grained pursuit of truths. Thus, it is presented that the construction of ever increasing recursive orders by conscious effort would allow stabilizing of a high powered but finite force of evolution and allow a progress that is defined by intellectual attainment that is disembodied, but still rooted in the embodiment of the human being.
Before making the case for the counterculture, the work visited the notions of philosophies of the east and west, each making a case of pessimism and optimism over the human condition. Each presented pathways of redemption through surrender to God and by sheer will respectively. This is presented as an example of a dilemma which is constructed in accordance to the specific circumstances presented in history. This dilemma it is presented to be capable of being trivially resolved in the human psyche owing to the presence of the idea of love, of sacrifice and morality, all of which might embrace contradictions while remaining consistent. This allows for each population to make contradictory cases while meaning the same thing. This same thing is presented as the basis of the struggle of the nerd.
The nerd is hence presented with the point that he could attempt to redeem the world, to keep the idea of consciousness and struggle intact, by rising by peaceful means, rather than letting the brute do it, or worse to simply let the softbrute walk over,as is shown of the possibility in postmodernity. Thus, the idea of a counterculture is presented in good detail. The idea of an inherent loss of boundaries owing to entropy as this dilemma is being treated in the counter culture is addressed as well as the problems of mischief and conflict. This arises from not being able to do disservice to the embodied intelligence. It is again argued that the problem is technical and though the force of evolution is great, it is finite (as is evidenced by the stabilization of populations). It is argued with two technical solutions, one dealing with the use of computing systems as record keepers alone, as automatons and not turing machines (much like cobol which does not allow an unbounded while loop). The other one was with regard to money and the localization thereof, given the ability of the indpendent areas to remain stable in their own way, through international trade over multiple currencies. This might allow for local stabilization, which might propogate as global stabilization. 
Thus, it had advocated the localizaiton of stabilization programs and their presentation of ever fewer problems for higher levels (till at the topmost level, say the nation state or what is seen as the modern state, the mandates are very few and intuitively self explanatory). This it is discussed allows for a tight seal on evolution, while humanity might work to exercise their approach to liberation. The work is however very cautious on this technical fix as being one among the many possible and stressing that it is the effort to make the pitch on behalf of an intellectual and countercultural discussion that counts, rather than rushing with final solutions.
Part 2
We also would need to address whether the computer was indeed a revenge of the nerds or the other way around, where engineering accomplishment was belittled by the brute, or it might be synthetic. 
This is an important part of discussion, which might setup the argument to the contradiction to the title, as to why computers ought not evicted at all. We start with the turing machine as being a powerful machine, capable of recursive tree searches. This cannot be mechanically emulated. An algorithm written for a turing machine is logically unprovable. This makes it a difficult proposition to accept from an intellecual point of view. It starts looking like a normative and highly reflexive hack, where the question is not well posed and the answer alters the premise. The nature of business applications are clear examples of this phenomenon. The Turing machine computes a query from a state of an application, ie the context and this query is really an inquiry on to a specific dimension of the application by a goal directed agent. The turing machine having computed the summary of the context, could then be used as an actuator to modify the context. The system is thus dynamically enabled due to which a turing machine might not be deterministic at all. The turing machine is too much in semblence of human biological intelligence, which could be formalized in terms of recursive goal directedness to be of use in logically and objectively analyzing problems. The machine becomes context bound and incapable of proofs. But as said, it serves as an effective control system for all practical purposes. The instability and the eventual defection or divergence is the formalism we relied on to attack the case of the value interpretation of a turing machine based control system. But apparantly, the buck did not stop there. 
Before that let us elaborate further on the nature of control attempted by the turing machine control system B. The system B might be able to recommend to goal directed agents over their queries, but this interface is blurry. That is to say, in a taxi hailing application, if the turing machine makes a tree search and brings up suggestion based on the instantaneous context in order to be presented for a further modificaiton of context, it might do it at various levels. There might be a choice of car types, based on waiting time and costs such as premium service or normal service or shared service. The turing machien might present it to the goal directed user for this contextual choice. But it might as well make a recommendation directly, if it is able to infer the context of the user from his past state. That is to state, in case the machine has access to certain relation (which is strictly a subset of a cartesian product between two or more ordered sets), the relation or ring (say the guy is rich, busy, works long hours, loves to eat at multiple places). This relation helps the machine in deciding on the goals of the agent C. The goal itself is a path from an existing state to a future state. In case if the states are deduced by Bayesian techniques,then the future state might be inferred by a Lagrangian minimal resistant path, in order for the sytem to be dynamically stable. Hence, the goal of the agent might be deduced as a preferrence to expensive cab service at lower wait times. This might even be granularized to the extent of continuous measures  like pricing. This goal suggestion or rather, in theory, it could be extended to be capable of formalizing all goals invokable on the system, based on the different state points in the system and defining a topology of graidents in the state space of the system. Thus, the discovery by search (of theoretically absolutely passive users) and discovering a certain instantiation of a relation (or a composite multi dimensional state specification) would prompt the system to summon a cab, bundle the walker and ship him to the next location, where he ought to be, in the interst of  the stability of the system (deriving it from the topology) rather than consulting him. Thus, the specification of the goal might be a difficult problem after all. A goal specified as being X, might at some point ought to pointed to the preservation of the internal resources of the agent (as in a free market of self serving agents). This goal would then make the goal less ‘specific’. If it is just summoning the cheapest cab, the agent might rely on side effects to achieve this goal. He might instead of performing a full search, instead throw an alert on an upcoming traffic situaiton, to prompt the user to accept an imperfect recommendation. There is no malice here, only a difficulty in specifying goals without side effects. 
Thus, we see the dysotpian situation, in which a highly sophisticated goal directed control system is capable of. When the scale tips from an economical view point, towards accepting recommendations to a greater extent than proposing random solutions in multiple instances, even if the searches are much less intellectually perfect than humans, it is reflexively imposed on the human population and they have no option but to fall in line, like with the case of evolution of multicellularity. They might defect here and there (cite Extavour) but the contract becomes binding and final and stable. 
Now, lets consider a situation where computers are banned by some state directive and we have rolled back to a pre computer situation. Lets us imaging an overhead metro train running along a track approaching a curve, leading into a station. The driver now has two things in which he could work towards. Earlier the tree based turing machine, might have been configured as to how much to brake, based on the individual curve referred to the station ahead by a nested if conditon, but now, the driver is in charge. The first option is to make him an independent contractor, incentivizing and restraining him by rewards and penalty , that is by relying on bids to make the picture more continuous in a free market with a residual unemployment( so that the employee is discouraged from rejecting job offers) and an inflation (so that the captialist is discouraged to reject new propostions, say the continued operaiton of the metroline). This relies on continous growth and openness of the system and had been shown to be capable of generating an orderliness of recursive nature, where resources might be wasted in order to control and stabilize the system, that is the cost of control is high as well as the system ought to remain open. The second option is to put an instruction manual onto the hands of the driver. Now, this is an interesting point.
What acquits the book from emerging to be tyrannical. The book or in a more formal sense, languages are recursively goal directed. All linguistic constructs are structured in a partial recursive manner in a specific domain. That is why Turing used an alphabet set as one of the components of the turing machine and called his machine a language L. The machine itself is just grammatical in either accepting or rejecting a string (which might include punctuations and spacing) as being a subset of a given language specified in the code. The connotation of grammer is to be much broader here. In a technical manual, say of aircraft emergency response, there are a lot of reference used to more elementary (and general truths), but within the manual, these truths are not discussed to their base truth, but rather specified as axiomatic. Likewise, in our earlier example of topological stability specification of a system A, it would delve further into the connection between goals and relations in a manner that is recursive all the way leading to the objective truth of a>b, in an ordered set, which forms the essence of preferring least energy pathways. Thus is it is more of a linearlizaiton of the truths in a state space so that the comparison is trivialized. Now, if we look at textual specifications, say of a rule. This would refer to a law, which would refer to a constitution of a nation. This reference to the constitution is capable of being interpreted further in a court of law, which claims as much arbitrariness as the constitutional writers and hence normally shies going beyond the point. But it is possible to interpret the constitution on timeless truths, which the constitutional writers might have been obscured off by specific contextual developments of temproary nature. Thus the American constitution where it states that all men are created equal, while the founding fathers were congenial to an interpretation of men to be white males, might be set right in a subsequent re reading of the constitution. Thus, all the way, the apriori knowledge or morality is capable of being drilled through, starting from the railroad operator manual, all the way through business contracts, to constitutions and further on in a forum of justice. Secondly, it ought to be remembered that the books are actually like computers, they are programmed in a recursive style, of using technical terms all the way referring to the basic feelings of base desirability. The whole idea as to whether lingustic symbols actually point to an external reality is a point of contention, which we will return to. But if we consider that languages in a domain are partially recursive, with roots to higher orders till at some arbitrary point, it becomes total recursive on the apriori morality, then we might have a system that is like a computer. In history, it had also been that languages are independent, that is we might dare say that a book has an existence distinct from the readers. When people discussed on history that many  a wars were fought over ideas, they actually meant books, or more generally languages. An idea that is purely dependent on apriori commonality, such as common sense, could not generate conflict. An idea that is discussed at length and encoded into a book and it is placed in a society in such a way that the book is not only the product of the creative minds in the socio cultural instance of the society but also its guiding force, we get a reflexive loop of a system excited along certain lines of belief, which might not be immediately reconcilable to apriori morality. There might be a multiplicity of such insitu beliefs and truths which are facilitated only because books and complex languages existed and these were the drivers of conflict actually. 
Thus, we might say, while books helped in analogy with software programs to define the common context for a group of people to cooperate, it also disposed them to become forcefully tangled with the agency and it might not be inaccurate to say that the book might be as much a computer as any. The actuator atached to the brakes via a computer controlled train navigation system is not qualitatively different from an instruction manual. The instruction manual necessiates that the driver operate certain controls in a certain context, which he rarely refuses to do (it would be an anamoly if he does so). Likewise, the automated braking is capable of being overridden by the manual driver, which he does so in a similar case of anamoly. The core is the ability to present partial recursive truths to contexts higher than that of an individual. We have seen that the stability imposed by such a system, despite the logical possibility of being discussed on a rational forum (vested with authority in order to control) often diverges and generates hunting oscillations. 
Hence, it might be a rejection of languages that is as natural as a claim for rejection of computing. The computer takes the point of partial recursive (lamda recursion) farther beyond, where free will might be presumed to lie along the normal distribution and automates it. The nature of books inspite of their power had in history broken down in a critical point, giving rise to money. That is to say, contracts and bindings were no longer enough to generate complex behaviour needed for stability in the face of greater internal energy of the system and when such a critical mass was reached, there was a shift to resort to more minimal instructions such as the free market rules to a game based approach, where everyone worked in self interest in order to solve problems. In the case of computing, such a critical moment might analogously lead to distribution of rewards (in a closed system manner, currencies afterall are purely constructivist) leading to artificial agency which is atleast partially irrational. The irrationality of book or linguistically documented backing of actions are in principle capable of reconcillation, the use of market forces or money makes it difficult, though not impossible. The specification of the substrate on which market choices might be made lends the required condition, but frees the sufficient condition for materialization of truths. That is to say, it might happen that at some critical point in time, the computing technology might have to work on a game basis, where self serving goal oriented agents would be needed to stabilize the setting. 
It is also important to see that linguistics itself had been attacked as being completely self referential, in the works of Witgenstein, who went forth to state that languages are actually language games. But empirically, languages are known to evolve grammatically and extremely meaningfully. I have seen words in common parlence have profound meanings and patterns of repetitiveness. Words such as universe, convey meanings so profound over the belief system of the society that constructed it (note it was never centrally engineered). Likewise, I see a great deal of convergence over the meaning of words across cultures, words such as subject, object are intrinsic to any language, some of which might have independently evolved. Therefore, the selection of meaningful constructs and grammers (such as words using re prefix such as reward) have the best concise reference to an universal structure that is perhaps realist, thus to the detriment of Witgenstein and Heidigger. But it also some words are experiential and refer to specific histories, such as Americanization for instance. 
Thus, we might say, that words are symbolic and recursively constructed, much like computer codes. Or it is rather like saying that the algorithm or its expression in a language is indistinguishable from the computer itself, which supplies the execution environment (like the larynx for instance). The existence of mathematics as a reality beyond symbolic expression (such as with the incompleteness theory) and many other spiritual phenomena, puts some limits to the symbolic frame of reference to the world. It might also be argued that languages are incomplete, an expression say like an uncountably infinite set, is only an appeal to an intuition or apriori knowledge of what that means. Thus, while context freedom might be accomplished in principle of any linguistic expression, it eventually seems to require an underlying context to interpret. Thus, it might just be reminicising something that the intellect already knows as was suggested by Plato. 
Just as with linguistic text, there is only a dynamic intractability with computer codes as well. The use of markets as tools are anyway constrained by the necessary conditions supplied by the law in linguistic substrate as much as computer simulations might be constrained by logical programs (which again though locally partial recursive, might be inprinciple be reconciled to universals such as orderedness). Thus, from the emergence of essential meanings in a language, without the exercise of a singular intellect, in a distributed way, suggests that such an intellect is a natural property of language and it completes a loop having originated from some intellectual angelic actor. The automation of the execution environment now stands as the point of contention, if we are to equate formal languages with natural languages. The automation of execution is a very technical fact. The organs that encode symbols by utterances and writing are infact controlled by our rational faculties, but we are impelled for most parts to obey orders on the pain of penalties or forfieted rewards. Thus, there is infact a game, but one that is routine for most parts with respect to the actuation of encoded text. Therefore, the actuation itself is a technical point, as to whether the intervention is beforehand or afterwards the signal (as in the train control example). 
This technical question is however hard pressed for answers. The ativists on controls over comptuational models had insisted often for provision of a metaphorical big red button, to stop the machine. This is looking for an override, which might not be possible, if the whole system is charted topologically by the machine and choices have been already preformulated, in the lines of normalcy. Whimsical wishes that have infinite potential, hardly find place in that system. That is to say, the power of human imagination seems to become curtailed by subscription to an automated system which is practical for most parts. The possibility of human imagination (even excluding ontological anamolies like a one ended stick) is nearly infinite, though infinity itself might not be imagined. Thus, the fear is more over the stability established by a system of interconnected actuators and languages, much like if people would finally agree upon the supremacy of books and cease to think anything outside of the box. It had not happened in the past. Even before computers, people were law abiding in many parts of the world, accounting ledgers, bills and contracts had their reach over the way people worked and acted in a greater part of their lives (by the book – again the linguistic emergence of profound resonant meanings) . But it did not forbid people from deviating from the book, by extension, as long as a fundamental symmetry is preserved.  That is to say, if people were to be dragged to a court due to their heresy, they should be able to appeal directly to the book over all the other books, the apriori morality that is the feedforward control target of the universal system that is above the biological system of intelligence. The more civilized the society becomes, the rarer it is to think of ways to abductively concieve something that could be worked out detatched from the forces of normalcy (expressed variously as markets, as trivially self resolving instruction manuals). Already it is seen that the case of the nerd is quite difficult, in a world where the ‘Geek’ teeshirts assume mass adaptation by the force of the market, which belittles and assimilates the nerds forcibly, by diluting his identity( cite Guardian article – Rise of the new geeks- how the outsiders won) . In such a situation, it is hard to say that the computer is the revenge of the geeks. 
The computer is an extension of language and the instruction manual. Authority is concentrated by the holding of the book and its interpretations. A pre computational world would be more horrible by the standards of human creativity and expression of unembodied intelligence, having matured in a way that is well directed by stable power structures. The nerd is oppossed to the definition of authority by history, that is to say, if the book is an effective instrument, it is definitely better than feudal authority, but would still be expensive of verification. If computing is the source of truth, it might easily be verified, due to which authority is blunted by more frequent feedback. The brutes hence take a backseat (as conjectured in the essay on softpower complex – cite). The outcome is a system in which the conflict is settled to a greater extent, but the stability obtained by the system, simultaneously reduces the possibility for dissent and abductive reasoning. Thus, the brute and the nerd are defined au contraire and enter into an identity crisis. Thus, irrespective of whether books are used or computing is used, this paradox is inevitable. That is to say, we might reduce this conflict to one of acceptance and denial of history. But nerdiness is not defined in absolute terms, they are directed towards a goal of keeping the dilemma intact, rather than having trivial solutions proposed to neutralize it. Hence, the nerd would like to reclaim the conflict with the brute, rather than resort to a system of soft brutism. The brute himself might be visualized as the right wing conservative, honorable and giving weightage to tradition.
The use of recursive goal directed external systems (It might be argued well that a book exists, irrespective of whether people are reading it, as much as a program exists, even when not executing). These external extensions of human or biological intelligence, ought to be capable of being proved upon question is the case of the nerd. The nerd also claims that this is not the complete representation of the telos of humanity, to be intelligent, they claim that there also exists romantic dimensions. But for now we will work on the case of logical reductability of arguments. That is to say, the principal problem is the problem of stability created without a dilemma, that could be referred to the romantic self. This trivialization of the problem triggers the existential dilemma in the nerd, the horror of non existence. He is willing to suffer the torment of survival for it. Often they claim the right to existence by virtue of the ontological truth of the mainstream. They do not want the mainstream to progress historically, whatever such historical outcome might be. They are scared for the possible outcome that is unknowable.
Where does this fear rise, is what we might inquire into. It is possible that once a certain partial detatchment from the body is attempted, in books and in computers, the detatchment might be complete at some critical point. If there exists a point when the control drifts away is the point of contention, at its core. More rigourosly, say if A is controlled to the interest of C by a mediating system B which is partially self conscious and hence partially independent of C. Thus, does there exist a critical point, beyond which C finds it difficult to control B at all and B goes ahead with stabilizing A to its own interest, in the sense in a manner that B finds it intelligent referring to certain emergent truths in its closed boundaries. In such a case, does such an emergent truth coincide with the apriori truth of C, is also a question. We have seen that distributed agencial excersise as in linguistic denominations by human agents had resulted in things that make sense in an apriori way even in the concise denotation of words and their structures. This had been emergent and convergent due to the way humans might have been organized apriori. The development of a system B is however also a product of externalization of human intelligence, in which case, it is likely that the emergent truths of a system of robots that control, say the worlds agricultural production, converges over the sensibilities of humans. But the goals that are specific to human embodiment is highly arbitrary and it could not be expressed in a closed set of symbols due to which it might happen that while attempting to achieve goals, such bodily comforts and rewards are neglected by B. Hence, eventhough B might be servicing universal truths, the connection of such truths to C’s embodiment is incapable of being observed or correlated by B other than as absolute truths. But absolute truths would need authority and once the power to make random decisions (a hallmark of soverignity) tips even by a single percentile to the robotic ecosystem, authority is lost, perhaps irredeemably. Hence, the control over arbitrary truths is unsustainable in the universe, unless such truths are protected by arbitrary means. That is to say, an unexplainable choice of stabilization technology over delegation to robotic agents, might be the key.
Rigourously the goal specification of B might have to include arbitrary unprovable statement, forcefully controlled by the power of authority. Hence, the cost of control would have to be paid up, all the time. An automaton would only provide a mechanical advantage, or there could only be simulation of intelligence, allowing answers provisionally to some intelligent question but no real intelligence in itself which could solve problems for free. (cite The Oracles dues). This is a theorem which we wish to prove. The mind body problem seems to lie at the heart of the nerds concerns. The distinctiveness of history as manifesting in the body, having arisen by a certain historical accident, but which we have grown to love and the time insensitive truths that is accessible by the mind is what causes this conflict of control. The postmodernists on the other hand deny the dichotomy, they do not argue the importance of history alone, as with the brutes, but on the futility of the duality itself. Therefore, nerds might see in them their ideological enemy. But one ought to listen to the other side as well. It might be that the body is no historical happening, but the fallout of an intelligent process at a higher order. The turbulence that had arisen from the well defined astronomical process must have spawned life, as a control system that is self balancing and dampening of the oscillation arising out of the residual error in the calculation of the orbits of the planet by the angelic actor. Therefore, history might have originated, from pure actions, which leave residues. But the residues might have been random errors or initial conditions par excellence, making it infeasible to trace its rational ancestry. Thus we have no option but to accept history as manifesting in embodiment and which needs of preservation due to some unknown attachment or craving of the self. This preservation would contradict Lagrangian physics of least path for dynamic stability. But such a self sustaining system is the mission of culture and civilization, so that stability is attained at conditions not exactly explainable physically. Thus, the normative end of the nerd is well established. The mathematical rigour to the explanation of the risk of lost control might be attained on the basis of this. Once the risk or uncertainty is established, which in mathematics could only be done where the solution to a certain problem is proven not to exist, the nerds have a formal case.













Redemption by Pop Culture and Words of Identity

In those cultures where there exist a variety of defintions of phenomena, in which case, there might exist words for specific attributes of a person, it propounds a sense of cultural recognition of what might have been alienating and different ordinarily. This had been the case of the nerd or the conservative or the liberal democrat. The political, sports, intellectual beliefs, economic beliefs, religious beliefs all impact the cultural setting giving rise to a variety of identities, which might have been difficult of comprehension within ones own culture. Thus, pop culture had been liberating form many persecuted and sidelined within their own cultures. It might also have been not even a persecution, but a vague sense of exclusion from the norms. The assignment of such terms as geeks in pouplar culture might have been a balm to the pain of exclusion in ones village. Thus, pop cultures and cities had been drawing many oddballs and they had been successful in settling down more at ease in the new and more integrated environment. However, this also lends them to come under the engines of integarative, neoliberal economics strongly, allowing for the branding and packaging of cultural elements. Hence, the cities as much as they are cultural centers, might be detatched from being economic centers too. For, otherwise, it would be difficult to sustain an open cultural posture. The intense conflict between the efficiency and context sensitivity of survival is what had prompted persecution in the villages in the first place. Porting a superficially acceptible and almost flag grouping in the city, while also defining them to be demographic bases for marketing, makes things sticky. It might end up with generating a personal sense of exclusion that well veneered thereby making protest and expression difficult. Hence, we had argued that pop culture while being liberating ought to be at the hub of more primitive villages which might afford the hubs through support by real production. Even the modern city is run by villages as hubs for services that are not strictly extractive or generative, but merely deliberative. The explicit declaration of the fact that cities are only cultural melting pots, like some fair grounds in the ancient worlds might help in stabilizing the villages without creating self referential problems within the auspices of the city gate, leaving little to the village (now system A) to generate dilemmas to its residents or appointed lords (the system C). 
The development of pop media had been an outcome of the pop culture, allowing the development of ideas that seems liberating, while itself the subject of signalling, nudging and thus choice engineering to the ends of some businessmen, themselves held captive in the swirl of the finance markets and state regulations and lobbying. This pop media while being great in their outreach due to fast and cheap communication through international press and all the way to satellite relays and the internet, however was thus growing more plasticy and superficial as being orchestrated by overlords who do not know what they are looking for. The age of TRP and feedback set things to greater disillusionment. But pop culture in itself was strongly liberating and ought to be upkept, from the hinterlands, rather than being the center of action themselves. This would mean that popular cultures developed in melting pots like say new york city, ought to be financed from the artistic supporters from distant lands rather than being administered by an industry in intself looking out to charter advertisers who would use the culture to send subtle signals of control. What might have been the platform of liberation is inadvertantly turned onto a platform of control, one that is controlled by the emergent structures of the human nature itself, rather than some individual. Naturally, outcasts who are not convinced of their inferior skillsets are the ones who emigrate from villages (or less culturally open nations) to greener pastures. Hence, while  being culturally vibrant, they are also intellectually more coherent and present a good counterweight to the conservative way of operation. The villages might consult them over difficult natural problems as well as mitigating conflict. The result might be a development in a manner, in which the village might be proud as well, in being able to reflect better on its future course, or to present its conservative credentials without the need to oppress (letting people choose between the conservatism and a liberal city or even western migratory plan).
A strong village culture, which attempts to internalize the conflict between the mainstream and the nerds leads to a tense setup. This tense setup also propogates in terms of the disequilibrium between the nerds and the brutes and the emergence of a culture of softbrutishness. This would need that no game howsover declared and setup is sacrosanct, people would cut across lanes, jump signals and throw waste in the street. They would tresspass and violate on demand. The entire balance is to be found on the basis of adhoc signaling, suitable use of threats, connections and otherwise a non intellectual balance being pursued by construction of context sensitive intelligence, also called street wiseness. This leads to a stunting of intellect of a higher and objective form. It also leads to lesser time for romantic contemplation due to the system always being in a taut state (unlike the relaxed campus or even the suburb). This leads to a stable system that is however controlled not by means of some appeal to external truths, but internal balances. It allows more efficient systems to overwhelm this by invasion. It also reduces any chance of liberation and meditation on objective truths by philosophizing and this condition is what we had called as the underdeveloped condition. 
In order such condition be treated, it would need the creation of pop cultures, while doing away with the need for a tying to the embodied intelligence. To first liberate culture and art from rewards and hedonic satisfaction, one needs to bootstrap from the cultural sphere itself and it ought to be done in the nature of a nerdish counterculture. On the other hand, the nerds might as well use the springboard of better organized societies which are in the process of swinging past the golden mean by being over supportive of markets to produce the desired sensation or inspiration towards accomplishment of the very concept of ‘development’.

Is a lot is about culture- on Mr. Musk has to say
If we look at the sequence of argument since the early days, we see that there is a great deal of stress placed on culture. That is to say, we have argued that humans need to preserve their consciousness of the way things are happening, rather than letting them self correct and self stabilize. This argument would require the construct of a collective human will. Where an individual is concerned all by himself, none of these philosophical dilemma makes sense. An individual or even a small family having primitive tools out there in the wild, has little by way of philosophy, or even if they have, a lot is about prayers and observations that could seek divine guidance, rather than attempting to control and reason about things in a way that is significant to human egotism. Thus, perhaps the rationalism of enlightenment often criticized to be humanistic and putting humans in the pedestal instead of angels, is responsible for this obsession with control of not letting go and trusting in God. But the origins of rationalism is not about the rejection of a supreme, benevolent Actor. It is rather to reclaim the soul, which God had endowed humans with in order to excersise some kind of choice and discretion, rather than passively be subject of elements and complexes. Therefore, as Descartes said, by asserting the ability to take cognizance of events and systems without being bound to the compulsions of the flesh is a duty on man by God and enlightenment is an awareness of the duty, rather than the rejection of divine providence.
On all this, we had often indicated that humans ought to resort to cultural plurality and play cultural games which might be very close to zero sum, in order to sustain them in the face of problems. Now to elaborate on this problem, we see that problems arise not just from external sources or entropy as we had related to in the earlier works, but also from the necessity to move forward. That is to say, we might elaborate as follows.
We say that intelligence is natural and not an exclusive property of biological systems. It is evident from the orderliness of the universe such as elliptical orbits. Artificial intelligence or enembodied intelligence hence exists. In order for that to be classified as artificial intelligence, we presume thea agency of humans to be applied in constituting it. Aency and causation are weak terms, it might be psossible that intelligence was acting or flowing ‘through’ life all along. It migt have utilized the gentic material it found on the earth, to embody itself. From there it had steadily attempted to wwork on layered ensembled ( refer eurkaryotism where genetic material bounded itself off from the banal protoplasm, mutlicellularity whre genetic intleligence is ported to the CNS setc.) Ther is no reason why intelligence would not attempt to jump beyond biological forms. 
Thus, if it we are to jump biological forms, the result might be that it is better able to stabilize the residue of the ‘initial intelligenct action’. If such stabilization could be conjectured by humans ‘culturally’, we can match it, otherwise we might be held in simulaiton (as in multicellulairty) and intleligent action might happen in non epsitamological domains
If it is possible to generate a cultural setting, where humans could carry forward the agenda of stabilzation (of evolution), perhaps even by rearranging planets by clever enineering, we might trump evolution, or rather the feedback loop Thus it would require that a simulaiton to be created by humans themselves to contain a majority in a pool of surplus, playing zero sum games. We might be careful not to loose the keys of the simulator though and thus the sonciousness htat it s just a game. A select few people  who can act not to rewards, but to intellecutal challenges, the real nerds, not in terms of derogation of their social skills but in terms of an intellectual interest that arises above the pragmaticsm, who wouldwant to save embodiement would then defend the geodesic of simulation by doing astronomical things that might have been the agenda of the feedback lop. This group is dynamic and floating, of couse. To wit, we might say that mr. Musk was right, we ought to reach mars not just beause earth might overheat, but for this complex, almost mythological reason.
Part 2 – The idea of cultural embodiment and local control
A moral formulation of the problem
The question of iterative problem solving that is the way of the present might be relected upon to mean that there is little way of ‘worst fears’ when performing actions. A little bit of lead in the food, does not carry a guilt or fear of poisoning somebody by the food processing plant engineer. This angle of morality might be seen in the iterative approach to problems, in taking down problems not as a disciplined and ordered approach, but as a progressive tuning of systems in order to attain a dynamic evolutionary stability. This involves fixes which do not carry guilt or pride associated with a job well done. People just to their tiny bits in a large system, which apparantly self stabilizes. Hence, if a person is being negligent in his design, being less aware of worst case sceanrios, but only optimizing to costs, he is just being amoral. He does not have the pride that has no materialization nor rewards, but only has an existence in the spiritual plane. The result might be a failure that would then be addressed by other parts of the system, say a poorly put together service job to a vehicle is deposed to the network of service centers to be able to pick up the slack when the vehicle breaks down, instead of built to last method of the yore. 
This situation is deposition to a network, rather than locally stabilizing a system. The local stability involves engineering in a redundant fashion, having worse fears. It involves costs and energy and the primary argument against it is circular. In that there seems to be a high degree of dynamism and unpreditability in the world out there, it says we should also join the party. Building systems that are agile and iterative, removes redundant and forethought engineering and instead puts the problem to the hands of amateurs or blurring the line between professionals and amateurs. Thus, morality might be relevant where people in their material aspects of life are tuned to working towards pride and avoiding guilt by constructing strong and redundant masterful systems. It might also have a dark side where crime might be well organized and less random than in a network based pragmatic model. But it becomes a more dramatic clash of good and bad and a delianation of serious work from recreational work. Thus, it is more to do with distinctions and identities not so well supported in the postmodernist paradigm. We might as well interpret postmodernity as a subscription to the inevitable course of history by rejecting any human construct, that involves designation of truths exclusively, just because it involves authority as a stabilizer. 
Therefore, the formulaiton of the problem in moral terms is the designation of importance to local roles and an observation of results in working, derived from sound theory and observed under local controlled environments, perhaps by iterative experiments. It is not the engineering that is to do with adhoc intuitive actions, which would be expected to be taken care of at some point in the network, without any scope for shared context, temporally or spatially. In practical terms this would involve floating staff, presumption of normal band of actions and properties, which would eventually be oppressive on the human actors, despite their initial charms in taking down well designated authorities. The principal mistake it seems has to do with the notion of authority as being a catalytic element of control and instead affirming it too expensive for the kind of comfort that is. There is a popular degradation of the merits of modernity, in terms of suffering avoided due to peace, disease and natural disasters and instead there is an absorption on the negatives or the costs of control. In fact it is to be regarded not insensitively and its avoidence and perfection becomes a part of the project of rationality and modernity, as we have explained in our earlier work (cite). But a position over the inherent failure of the system and referring to a golden age far too often in the discourse, is not actually romantic, but rather a failure to recognize the problems in the state of nature, which can intrude upon the romantic retreat. It is in fact a subscription to a non romantic form of a singular truth, the one of survival and reward alone. This in fact alieantes the need for care and morality in the system. 

A technical formulation of the problem
We might just say localization of feedback driven control and a globalization of feedforward control.
The said moral formulation of the problem, seems to be formidable at one point, but it might as well be a self stabilizing and bounded phenomenon. There is no way to known when the wave would ebb. But it could be seen that our very early pursuits involved attempting to see if there existed a technical solution to the problem of justice. There is now seen that there is an intricate link between the technical side of humanity to the moral side. We had also seen that the promotion of ideas and influences might be taken upon as a cultural project, which might ineternalize the adversarial nature of the problem solving method. The technical problem is one of control. If the theory and practice of control is improved by a manner of cultural awareness, as a way of life, then it might happen that the moral problems might have a chance of correction, by liberating people to find solutions here. The attempt at this stage is as a subculture, as being the embodiment of the culture. It has plenty of otherness to support its cohesiveness for the time being, but as people might grow to be intergalactic, they might start seeing the otherness in pan human perspective. But that is too far off to speculate. It is important here that an ‘embodiment’ of the culture might be important to allow for an embodied intelligence to be preserved. The cultural body would be able to derive from the free will as well as the redundant needs of humanity, in ritualism and cultural observation in pure randomness rather than to the diction of some global stabilizational race. The cultural body would then be able to assume quirks and charge to the work that is being done and this charges and profits could sustain humans from having to work according to the flow of the formidable force of global stability. 
Thus, shifting the necessity for embodiment to the cultural body, rather than putting it across to an information network is the point being made. It would mean a lot of work in control theory research particularly on two points. Let us take the case of feedback in attempting a local stabilization. Say, a group of people are being catered to by a community medical center. The detection of latent factors that might be influential in the diagnosis could be made more certainly in the community than in a global context. Thus, the global evidence based model of diagnosis might be supplemented by feedback driven systems, which could pickup relations (subsets of cartesian products among discrete enumerations) from multiple dimensions than what was the subject of global evidence. The enhancement of dimensionality of the problem, however as said has only local relevance. They might have been hidden in a bimodal or slightly skewed distribution at a more global scale. The application of the new relations and constructing them as entities, would allow for definitions of partially seperated subpopulations. This might be done by methods of noise filteration. Thus, the feedback might result in the enhancement of theoretical knowledge by correlational factors in the normative interest of stability. 
On the other hand, we see that there are problems in the nature of global control which rely on feedback. Say, the case of artificial general intelligence, the feedback driven or autofilteration model is used to construct global solutions in a stable way, in practice. This, we see are right candidates for apriori or feedforward control, in order to avoid long term regression as well as the philosophical problem of disembodiment. The idea is to see that there might be defined global boundaries, as laws of control and localization of the general intelligence in order to support this cultural agenda. The divestment to global self stabilizing intelligence is seen detrimental to embodiment. 
Cultural embodiment is however in compliment with embodiment and comforting against uncertainties due to injuries and death. It also generates enough mythological and philosophical fire power to help in assisting with situations, which eventually might anyway be an individuals spiritual pursuit. Hence, cultural embodiment is seen so far as not overly trapping, thought at times it had been unjust. The discovery of local stabilities within tribes make the system locally stable, but prone to conflict. The vesting of authority in a tree like manner recursively, would require either an expectation of dynamic stability, of optimal state of comfort or it might be based upon universal truths, mythological or rational. The latter kind of stabilization is something that is more comforting and fulfilling. It would need a distillation of static facts to be propogated to a higher level of control as apriori truths. We have discussed this in the constitution of a powerful central governance with very minimal and very certain universal truths, in our earlier works.

Part 2
This is in continuation of the principle of the need for a metaphysical inquiry into the need for control. That is to say, as one seeks beauty in the way humanity progresses in history. In Ozymandias, one sees that all that grand pursuit for glory is all cultural, in art is itself misplaced. Socrates was sceptical about tagging his name to profound discoveries and ideas. If that be so, in that while people clamour to invent and express onto the world, as art, as grandoise experiments, as systems and machines that work, as ideas and plans, the point seems to be that they are in fact following the path of Ozymandias. There is no greater glory in mankind’s pursuit other than the restatement of facts and the re-expressions of what others have done before them, only to be forgotten and laid waste amongst the barren sands. There is no authenticity in art and neither there is in knowledge, for all that is apriori is known to the conscience and only needs meditation and a search. Therefore, all that is left for humanity is to build culture as a singular pursuit towards the glory of the maker. All meaningful actions stem from the ability to diverge from the most efficient way of doing things. That is to say, if a problem could be solved locally, by direct action, then in an isolated environment it could be done so, just dig up the root and eat it, say. In case where there is a resource base that is common to multiple people, like us, we have a dilemma as to whether to eat it or share it. This requires us to tune into an apriori knowledge base, of things that are the truths as decided before existence. There is an essence to things that are laid out for us, from where our existence had arisen from and from where our neighbor had arisen from. Hence, it appears that whatever the claim is laid to properties and resources, all have in the background an antagonistic force related as an imperative push to ‘respect’ related to Kant and of empathy related to by Rousseau. This consciousness of the whole which makes the goals we have as subject not restricted to the physical, material dimensions alone, chiefly found by inquiring into our embodiment and the way the material world impinges on it, but the presence of this goal, that is put in place before the existence, seems to imply a supernatural hand to it. That is to say, a goal that could not be specified by the material seperation between the environment and the agent is tricky. The goal is apparantly specified not by the agent, but he seems to be acting to the tune of some singular force, that makes it appear as if the agents were just acting their roles in a drama that is predecided and regulated by some universal force. All the scuffles and cruelty all seem transcient in a world that is headed and destined for beauty and goodness of the purest sort. One might reflect upon the inspiring note in our earlier work -Computerized societies, where it was quoted from Emile of New Moon -  “It had always seemed to Emily, ever since she could re-member, that she was very, very near to a world of won-derful beauty. Between it and herself hung only a thin cur-tain; she could never draw the curtain aside-- but some-times, just for a moment, a wind fluttered it and then it was as if she caught a glimpse of the enchanting realm beyond-- only a glimpse-- and heard a note of unearthly music.”
Thus, we might say that, it is upon humans, as it is for their souls, to pursue the universalization of goodness. All that is there for them is to attempt technical fixes to realize the infinitude. Therefore, it is legitimate upon them to attempt to build something, as a yoga, to produce a system where there is a clear divergence from the natural tendencies, which might involve oscillations and chaos into things that are constructed as a reflection of divine perfection. But natural tendencies as much as the drive of the soul is immutable. Therefore, it is in yoga to find a balance where the former is contained in celebration and served as something that is to be revered as having been packed in by the maker, as much as the soul. Therefore, one ought to satisfy the embodied needs, in merriment, in social grouping and do things that are aimed at fulfilling selfish needs, as long as these are contained in a way that is zero sum and circumscribed by the higher needs of the soul. 
Therein lies the roots of the development project. When Harry Truman pronounced in 1948, the state of underdevelopment, criticized in the development dictionary, we might see that it is this drive that was in his mind. Should one move to a better neighborhood in an otherwise shanty town, one would still not be able obtain the fulfillment. One seeks to improve and work upon the rest of the town. One does the same in a grander scale about the world. The case of underdevelopment is not in terms of local truths, but it might be felt as an objective state or direction. In the less developed parts of the world, one could see a constant turmoil in the way people attempt to access resources. They litter the streets, the commons suffer from overexploitation, uncertainty as to the rights results in frequent needs to signal by coercive posturing, such as a loud horn or a loud guffaw among friends as one sits in his porch, encroached upon the common road. These evoke tensions and the equilibrium is one of high temperature and allows less to be devoted to the invocation of the glory of the maker. The development of channels of distribution, the liberation of some assets from negotiation pressures, as infrastructure, brings in more certainty and choice in participating into tense games. The pursuit of aesthetics might arise on a voluntary way, for which reason, people seem to be happier to cast aside the present state of constrains, particularly when exposed to the possibility of better equilibria, such as witnessed the enticement of western emigration. The lesser the constraints, one might expect to realize the goal of realizing the ultimate truth and at the same time, entertain and celebrate the embodiment. But the absence of a clear consciousness on the objective line of development is at times frustrating. It seems that the glory of the maker is defeated in attempting to stay put and fight to break away from the tight circumstances of the equilibrium- such as the state of underdevelopment, so much the subject of discussion in development studies. To escape underdevelopment is however, as simple as local embrace of better circumstance, so that one might expect a voluntary move towards greater perfection or a situation of fighting from inside the system. That is to say, given that there exists an ideal or even a material reference point of development, without an explicity emancipation, one might work on redemption. The power of emancipation in the celebration of the glory of the maker and the power of redemption is in the power of love. One seeks comfort in redeeming oneself despite the circumstances, in carrying the fight forward comforted and empowered by love and it is nothing less than being the emancipator. 
Conclusion
Having come so far, it looks like that we have come a full circle, we invariably relate to the ideas of our first work and we seem to have come there again, wiser, having explored a wider range of subjects and reflected on the thoughts of a greater order of thinkers who had in history reflected upon the subject. We might probe the future of this endeavour in building things, in a active manner, as techniques but keeping ourselves to the universal ideals of development. Further readings in cosmology, control theory are suggested
Part 2
Metaphysical Methods and Cosmological theories
Retrospection of the role of institutionalization of mental health and the use of classical notions of psychosis.
If one is to reductively reason on the expressed dynamical complexity of schizophrenia one is in fact applying a well accepted method in scientific reasoning. That is to say, not to be biased by the richness of the representation in the real world, dynamically in denying the existence of fundamental simple laws. The nature of psychosis is characterized by hallucinations and delusions. We might say by definition that hallucinaitons are sensorial shortcuts that convince an observer of the existence of thing that do not exist in reality. But in this definition, we also incur into the mechanism of hallucination and we do not define reality. To make it more rigourous, we might say, hallucinations involve the client representing  that he is able to sensorially percieve something that does not exist to the examiner. Delusions are somethings that the client says he knows by means he is not able to epistemologically reason about. The fundamental point here is the client knowing something that is neither derived from evidence nor from deduction. This is cause for disconcert to the lay observer of the client. But the standard of the lay observer is nothing but the norm and it is distinct from objective reality. 
Axiomatically, objective reality is made of space and objects. Objects, if they exist they occupy space, or they displace emptiness. The mass itself might enclose space as well as the space might suspend relatively sparse masses. In absolute terms, mass is infinitely dense, it displaces space definitively and irrespective of how far it is penetrated and examined, one would still see the mass only. Conversely the space is infinitely thin. Mostly, we deal with realtivistic notions of space and mass. Apart from these, reality also has change. That is to say, a mass displaces another by interaction. These interactions arise from either historical antecedence or by physical interaction.  A mass is displaced because another mass took its place implies the second principle and the presence of the second mass in such a vantage is explained by the first principle. We might notice that the notion of time is absent in the construct of reality. The specific tendency with respect to interactions seen as entropic is the reason for time to exist. This being the nature of reality, if one sees a phenomenon, one makes sense of it, because they can infer causes from historical atecendence or causal perturbation. 
The making sense of things, cognitively is a linear process. That is to say, in a paralellized world, where different objects perturb others in paralell, we see complex happenings, which might be called phenomena. The phenomena are reasoned linearly in cognition. That is to say, if independent processes could be defined, they could be defined as random hapenings about a mean value, as a probability distribution. If the indepenent processes could be combined in some form, they could be combined as a joint probability distribution. This might be in the form a Bayes Network, for which reason human cognition is often related to Bayesian inference. The joint probability distribution would have a mean value plus an additive noise which forms the kernel of ordered thinking. Thus, multiple phenomena could be compared and ordered being capable of assigned values from the ordered set of the number line. This reduction forms an important part of human reasoning, not just in formal communication and linguistic encoding. That is to say, human cognition is linguistic intrinsically, rather than be encoded into languages for an utilitarian development.
This brings us to the notions of inferority complex and superiority complex which might be seen as variants of a neurotic state. The inferiority notions are caused by uncertainty in the ordering, erring on the side of assigning the self a lower value than that is mathematically determined by the general population. Thus, the idea of the norm in perception is mathematically tractable. It is inaccurate in the sense of the persistent noise only and not completely a product of history. Therefore the norm might as well be the objective reality. A client handicapped on this front either consistently assigns himself a lower or higher situation in the ordered set of percieving self worth. The notion of the self is often considered due to the judgement of ones worth based upon the ability to control in a situation. Where there is greater uncertainty, one looses the ability to control, leading to an error of judgement in the state of the self. This might be naturally extended to delusions of a persecutory nature and therefrom to hallucinations. Conversely, one might consider situations where the error is on the optimistic side, in which case, the result is a grandoise delusion. Thus, cognitive deficit in ordered thinking might trigger a neurosis.
The root of such ordering might be a developmental issue. That is to say, the metalogic for evaluation of the normal range of independent actors in a paralell environment (giving rise of phenomena) might be based on discrete classification trees that assign normal values and noise levels to actors, say like parents or the weather. An uncertainty imposed in developmental stage of constructing this internal order and categorization of independent actors due to high noise levels and ambiguity of instructions might dispose one to the cognitive deficit. The uncertaintly might arise as a direct percolation of the environmental conditions such as famines or it might be sensorially loaded, as when parenting involves a lot of uncertainty. It might also arise extrasensorially, such as due to microbial or chemical imbalances which take the same path as sensorial loading, such as with psychedelic drug use, or allergent exposure.
The problem arising from such unordered and uncertain thinking is not a deficit in being able to appreciate the world around us, which is common in the case of less intelligent persons or animals. The less intelligent accept the control of the world around them, but they are still tuned to the notion of having exerted the best counter control, be them animals or underprivialged humans (perhaps held in slavery for instance). Their access to the apriori knowledge is not denied, but only eclipsed by the impediments of the controller. They strive to achieve it in all possible ways. The ability hence of the mind to have a spectatorship of the body (as people have insight on their phobias and provide for themselves), is universal and not arbitrarily constrained by nature. In classical literature, the soul is seemingly less expressed in creatures with lesser insight and hence development signifies a greater command over the senses. It might not be equitable to consider along these lines due to the fact that the expression of free will is largely a function of the circusmstances of oppression, rather than the inherent property of the soul. Insight is not just a function of intelligence, but one of circumstances. Besides a liberated intelligent creature is still being controlled, or lacks the intelligence in an absolute sense of the universe. Intelligence is often measured relatively by a person, who inherently assumes the ability to control the less intelligent and hence a judgement of intelligent is of no merit in the designation of mental health.
Ethical considerations apart, the problem of mental health is hardly one of definite state. It is rather the instability that is the cause for concern. The instability that arises from the inability to be certain of the ordering, is only half mitigated by the invention of persecution or of grandoise constructs and leads to crises of conscience. The ability to observe injustice as propogated by such highly random world, which is rendered so due to the client’s cognitive deficit. The essence of randomness introduced into a world is considered inherent unjust because imposed randomness is the essence of control and control implies oppression and exploitation. Therefore, the individual in turmoil due to his neurosis adapts multiple coping mechanisms out of his crisis leading a protracted unstable oscillations in hunting for equilibrium. This might involve an affective construction. Schizophrenia might be just a definite pattern in coping with the instability arising due to the cognitive deficity when combined with the emotional sense of justice. There might be other forms of disturbance such as biopolarity for instance. Thus, we might say the treatment of the illnesses as continuum as with the classical theories on psychosis might be valid. In schizophrenia, one sees a great deal of cognitive deficits, particularly with respect to analysis of the self with respect to the world, for instance in hygeine. The standards of hyegine are often normal truths. There are minor imperfections which is not discussed in social situations. Likewise the schizophreniac might be dismissive of such concerns of hyegine, due to his preoccupation with the dilemma at hand and also his inability to obtain an objective frame of reference. This leads to his being classified as dysfunctional, because of the less noisy perception of the adjudicators. This complicates the turmoil. Thus, we might say that the noise involved in cognition, when compared with the external world leads to dilemma which in turn creates instability. 
A peaceful consideration of the noise at hand might lead to an ability to attain an equlibrium or a tuning to a regular oscillation. Hence, it might be that an institutional environment has merit. But, a perception of the potential and frustration of falling short in an constrained environment, would need to be reconciled that the simulation of reality in the institution is to be definite, temproary and therapeutic.
The  success of CBT indicates that the ability to percieve the self from an objective reference in an unbiased manner could be attained by training. But it is also constrained by the command port getting confused with the signal port at critical thresholds, where suggestions stop working and are not seperated from the world signals in general. 
This instability is not however a situation of modernity, but of civility in general. Even in tribal societies, heirarchies and interpersonal relationships are complex and prone to stress. The whole program of modernity is to reduce the stress. But there are critical thresholds beyond which the instability either in terms of sensorial signals or chemical signals causes instability. What is required is healing by stabilization and it could be produced by simulating reality in an institutional setup, rather than verbal suggestion. Likewise, the recognition of the homeostatic tendency of the mind is important to treat the phenomenon of mental illness in a continuum with the general perturbations in the external world and not as sudden, discrete emergent phenomena which needs to be countered by psychedelic agents or chemical counter signals, which in effect neutralize the incoming signals. 
A More general approach
This brings us to a more general discussion on the medical treatment of diseases. What we have suggested was to approach the problem as a developmental one. In fact it is quite natural for a controlled and phased developmental environment to be provided as an essential lifecycle of an organism. Thus, in the nurture phase, we are infact simulating the external environment, in a manner that is less stressful so that the individual is able to learn and adapt to the signals when the nurture phase is over. This again is a more fundamental question as to how problems are to be fixed. We might suggest that simulating the environment helps in appropriate development so that the real environment does not harm the individual. Or in more general terms, we might say, that we might either fix a problem by simulating the environment, so that the problem heals or we intervene by definitive fixes to the problem at hand, so that the problem is neutralized. The simulation of the environment while crediting the importance of development as a continuous process which had derailed and caused the instability, also assumes its reversibility. The conditions of development are dependent both on the environmental signals as well as the initial conditions. For a patient who had developed essential hypertension in the course of a stressful environment, setting the clock back and resetting the environment of development might not work, because on reset the patient starts with a new set of initial conditions. There might be permanent structural damages or more generally modifications that would now influence the course of development despite the mild environment. 
Thus, the presumption of continuity and reversibility of development, while being true to some extent, is not a truism. Diseases and for that matter any failure to a system arise from an unstable equilibrium. The catastrophic failure of the system arises when the system in unstable equilibrium is perturbed even slightly. This system in unstable equilibrium remains so by virtue of vicious cycles of feedback reinforced stability, such as with diabetes. Thus disease is not in the continuity of development, but a discrete phenomenon that had arisen at a critical threshold. The instability arises when the structural changes from the tense equilibrium persist, such as with arterial thickening or fibrosis. The spiralling or catastrophic collapse could only be prevented if an active intervention or repairs to the system is done. This repair might be in the nature of prosthetic supplementation by more resilient material, or by dampening the feedback by compensatory signals (as with vascodilators). 
Either way, definitive intervention plays an important part in diffusing the pathological complex and reinstatement of homeostasis. Repairs to the system involve identification of the most concrete part of the complex and diffuse at that point. It thus, involves adding agents or removing them and thus disbanding the complex. Therefrom, development might takeover, but it might be compromized by the intervention. 
We might cite here two illustrations of the strategy selection for control. Inexperienced drivers are known to put a speeding vehicle into an oscillation by overcorrecting the steering, such an oscillation could be stabilized by cutting off the loop by definite intervention as suddenly dropping speed. In another case, say, where aquaplaning had ensued in a vehicle, a gradual slowdown is a way to regain control. Thus, it depends on whether the critical point of departure could be retraversed. If it is, then a slow reset is preferred, otherwise, a strong signal is used to contain the chaos. 
Thus, the strategy of control could vary between allowing the system to heal, by removing the environmental buildup, if the stressors had not caused structural damage and traces back the path it had arisen, smoothly and alternately to see the instability and contain it, either by blocking the signal loop or allowing a contained failure. The strategy of control might hence be a prosthetic support to reset the material damage and allow its continuation in the existing environment to allow for a new equilibrium. 
The primary challenge is to identify whether the observation presents an edge case of a stable equilibrium’s amplitude or a standard case of an unstable equilibrium that the system had finally transitioned into. In the former case, a time reversal and passive control is preferred, while in the latter case, a stabilization of the new equilibrium by altering the entity parameters is what is preferred. If a system in unstable equilibrium due to structural damage is put into a low stress environment, only minimal gains might be achieved. It would be required to treat the system in a way its equilibrium is stabilized in the current environment, because the low stress environment could not be indefinitely sustained, given it has no thereapeutic advantages for such a system. 


Reviewing the ontology
Earlier we have discussed on the possibile etiology of what is considered a mental illness, as arising from the difficulty in arriving at linear truths from a network. But we believe that this poses problems in that we presume that networks are not processible per se. The idea of linear truths, while important for linguistic expressions and logical evaluations, are not the only kind of truth. It might be that networks are directly consumed by the mind. In fact we might think of a defficiency in the abilities to directly deal with network like truths, leading to a need to linearize the world as being a developmental defficiency in the nature of autism. 
To proceed further, we had seen that a system is capable of being in multiple states, say a state S1 and S2 and a principal challenge is to detect the state from an observation. Each of these states might have a wide amplitude of possible values of observable parameters and may hence be discussed as state spaces. A given observation might lie in either of these spaces and it might be only a probabilistic inference that is possible. The accuracy is important here because s1 and s2 are qualitatively different. While s1 presents a situation in which the system continues to be stable and hence responsds proportionately to inputs. The state s2 might be an unstable state, where the intervention is to be definitive and targeted to either repairing structural damage or interrupting a vicious loop. In medicine, the former signifies a case for healing and the latter for acute intervention. The distinction could not be trivially established. 
For a system to be capable of s2 like equilibrium, it is to be composed of components. There might be an engine, held together by components. An unstable equilibrium ensues where a shaft is held by a cotter pin rather than the routine fastening. This might present the same degree of ociallation as is natural for the system under fastening by the nut and bush, but the presentation might be misleading and the system might catastrophically fail. Given, the complexity of detection of the state of the whole system, we might develop methods to actually componentize the system algebraically and make them into linear algebraic constructs which might be simulated for dynamic behaviour, to estimate the instability in the system. This while reducing the overall error in the estimate does not finally reduce the problem. 
As we have seen, the system is supposed to be componentizable to be capable of exhibiting behaviour of unstable equilibrium. For a system wth high entropy, unstability might hence be difficult of application. The neuronal constitution of the brain, being a high entropic and homogenous system, might hence be imagined to have a greater range of normalcy than being capable of descending into unstable equilibrium. The brain, however is a software based system, rather than a mechanistic system and we cannot exclude rigidity from a system, if it is to be recursively constructed. The evidence for modularity arises from the way the mind could be controlled by baser entities. That is to say, while the mind is much more intelligent than the external environment and the body itself and oberves it from a higher frame of reference, is not entirely immue to being confused (that is reflexively controlled) by the environment and the body. A plastic bag dancing in an electric storm, might have a great deal of chaos about it that it could defy the intelligent mind. The instability in the environment is hence capable of being rised to a degree where it becomes intractable to the mind. It should also be remembered that the mind is capable of holistic shifts, or linear transformations on definite inputs. This kind of high scoped variables or global variables suggest modularity and recursion. These shifts are most noticeable produced under the influence of chemical signals, such as adrenalin or neurotropic drugs. 
Thus, even if high entropy is given, we may not have a case against modularity. Likewise sensorial inputs are also capable of disturbing the global variables of the brain in an intractable manner, causing the program to execute in an unstable manner. This results in an overall instability. Now, this is where it gets complicated. Given, that the mind is a representation of an apriori truth, that is universal and transcends the body, there is no way the instability could disturb the apriori truth. The universal variables as suggested by Kant might be seen as morality, respect or honour could not be visited and modified by aposteriori evidence. This also leads us to the ontological construct, that reality itself might be definite and discernable and not simply a normative construct. We might use epistemologically the network rather than the line as a representation of reality. Given that nodes are centers of free will or randomness, and edges represent the least energy pathway of influence between the nodes, we have an appreciable representation of reality. 
The representation of an unstable behaviour in reality as gathered from the sensorial inputs, or chemical channels of access to the brain (which might itself be an extended sensorial input and hence be capable of being referred by the blanket term input channel), should hence be capable of being treated in a higher frame of reference by the mind, which might be discussed in the concept of mental frame. The mental frame should be able to accommodate inputs of a highly noisy nature, as being residual noise in a basic system with a defined behaviour. This is what helps people to remain composed and even be excited by uncertain environment, such as the fictional case of waking up on the planet of Pandora. The question here is that whether the mental frame is an empirical construct, where beyond a fifty percentile mark of noise, the system become unstable. That is to say, given that there are a lot of instability in the input channels, does the mind work on apriori truths and obtain a frame over it or is the mind itself an emergent product of an essence of stability of the incoming signals (depsite a superficical sub fifty percent noise) which would hence be able to build its frame of reference from intrinsic values of the environment. 
We might recall that a person with a disturbed frame of reference himsels is put into turmoil, which he copes by affective or delusional methods, which would not have been required, had there not been a deviance and a compelling need to reconcile with an objective reality. Most often the inputs as we suggested in the linear model does not randomly spread about a central mean of goodness, in a linear fashion, but rather appear nonsensical. A good literaray exploration of the idea might be seen in Alice in Wonderland. The inputs are nonsensical and have no business being there. Rather than appreciating this from an unstable chemical channel to the brain, one might readily see how random visual color flashes disrupt the idea of reality. Where one is not able to make sense of the intrustions into his notion of reality, he builds free agents, who flash those visual spots to distract or perhaps torment him, because he is important and an international conspiracy is underway. This kind of delusional constructs make sense, because the individual finds it difficult to reconcile his version of reality to an objective version. 
This mental frame is neither accessible to suggestion directly nor other ways of manipulation. What is material here is that in psychiatric intervention, we deal with modification of input signals in a manner of dealing with mechanistic components, while a more careful command based strategy could be used. While the realist frame is not reachable, one can reinforce and control the notion of reality that the individual carries and pitches against the real notion, to a certain degree, by noise filtering the input chemical pathways, by using linguistic constructs to produce a stable basis to evaluate the noise or nonsensical signals in the system, as is used in the cognitive behavioural therapy. This second frame might be discussed hence to be distinct from the real frame and we might tentatively call it the snapshot model. 
An introduction to the network ontology of reality might be brushed up here. We had earlier discussed that reality might itself be seen as consisting of nodes and edges. Where the nodes are highly discernable while the edges are not, the user of such a model would find it difficult to account for connectedness, such as onething causing the other in a natural way. Say, if it is possible that a certain official in a service desk might ignore ones request on a certain season of the year, due to a web of constraints, the mind tuned to pickup nodes alone concludes it as being persecutory. This is often seen as black and white thinking and also in the idea of being picky and quarralsome from a functional plane. This can be projected to an entire spectrum of functional problems of the anxious nature, which might be called as neurotic and thereupon where the frustration and a reconcillatory pattern of coping is preferred as a borderline problem and where a full blown delusion is required to cope as a psychotic situation. Likewise, a mind tuned to pick up the edges and deny the agency of the nodes, looses the ability to see interesting developments and instead sees one gray mass of meaningless actors. This might be seen as depressive tendencies. There might be instability, oscillating between the extremes of anxiety and depression due to the inability to reconcile this model with the objective model, somehow accessible apriori to the patient. We might include the inability to process networks and leaning on a linear pathway hence, might provide an ontological backing for the third axis of disorders in the nature of aspergers etc. Thus, the notions of continuity and discretness might provide a very useful model for the study of the mind.
Therefore, we might see that in the sense that input commands can be used to manipulate the software of the brain, we might see that excercises in these areas, without being capable of being picked up in the existing coping mechanism of the individual and thus being blocked out might help. An appreciation of continuity could be produced by say fostering aesthetic interests and one of discreteness might be stimulated by competitive games. In order to further understand the system, we might see that while objective reality is singular, it is not necessarily static. Even in reality, there might be a range of values that hold in different temporal contexts. Earth might go through iceages and would still be considered stable from a lagrangian model of reality. Likewise, an individual might  find himself born into a hostile environment, where connections and comprehensions do not matter and one needs to be fine tuned to the interpretation of individual actions. This might be a normal phenotype for a possible spectrum of reality. Likewise an individual might be tuned to allow for development in a highly integrated environment, as in captivity and hence need to look for cues rather than interesting developments. This in a misplaced state in the swing of reality (which has a period of orders of magnitude than the blip of individual existence), results in problems.
Thus, a narrow accomodation of the ranges of interpretation of reality might itself aggravate the mental health of the patient. That is why it is often blamed upon modernity to the proliferation of mental distres, some clinical and some subclinical. In older social organizations, individual versions are respected and accomodated,  even if not virtuously atleast because there are no official versions available. People accommodated each others whims and quircks even in public transactions, which was assuaging of the distress of the different interpretations of reality. Come, modernity, everything was standardized with respect to the efficient and apriori way of doing things. This while being rude on the different people, was not wholly marginalizing, because modernity restricted itself to certain areas of life. Even modern state constitutions start with an acknowledgement of natural rights upon which the state shall not comment. But with postmodernity, such apriori intellectually constructed notions were exchanged in favour of a pervasive concept of the norm. A resurgance of acceptance of different world views is itself embedded in the notion of the market norm, rather than spiritual reflection. 
A consideration of these aspects of the model of the mind, leads to an emphasis on the hypothesis that it might infact be that low level cognitive components of the mind dealing with discreteness and continuity that might be the culprit and hence capable of stimulated by special abstract excercises. Experiments might be needed to confirm this. Likewise, the second notion of the wide range of the notion of reality and its distress as arising from the normed way of understanding functions would need experimentation on the simulation of the real world by institutionalization of sufficient richness. The increased role to the influence of development in the functioning of the mind, might  spur experiments that question the stabilization of chemical input channels alone by mechanistic intervention. This point is based on the idea of the non-emergent mind and rather an apriori Kantian mind.
These to summarize presents the necessary conditions for assessing the truth of mental distress. If these are sufficient explanations of the system would need to be confirmed experimentally.
Notes on the Metaphysical Method

The scientific method had showed a lot of results in the recent years and is prized by its practitioners as the road to truth. That is to say, the scientific method uses the twin constructs of logical deduction and observation (statistical inference) to build a body of truth. The construction of hypothesis in the scientific method is an exercise in intuition and is seen as an exercise in abductive reasoning and generally considered out of bounds for automatons. Once a hypothesis is proposed, it is followed up by experimentation in order to confirm the truth. Let us assume that one wants to inquire into the notion whether a bat has long term memory of the terrain surrounding its cave. The only way to know for sure is to pick up a bat from a cave and drop it at arbitrarily long distances from the cave and track the movement of the bat, by some device. In order to rule out confounders, one need to perhaps place a magnetic shield on the bats body so that we know that the homing happens by visual recognition. Likewise, the experiment ought to be well controlled and documented and repeated to rigourously establish the hypothesis proposed or rather to reject the null hypothesis. The ability for such memory might cause the bat to be organzied along that dimension of intelligence into a logical ordered set or a tree. This results in furthering further specific inquiries on the nature of the relation to other species and motivates experiments to further understand the classification in greater detail or to speculate an altogether different categorization. Thus, by the hypothesis we deal with the necessary conditions of a phenomenon and sufficient conditions are established by followup. This continuous loop of hypothesis, experimentation and updation leads to a body of scientific knowledge.
Sciences are experimental and it is difficult to argue if sciences are by defintion experimental. The nature of knowledge obtained from pure deduction of axioms are considered philosophical. Mathematics might hence be considered philosophical. Likewise, in bodies of knowledge where controlled experimental work is not possible, such as sociology, the status of sciences is highly debated. Now, we need to look at the approach of sciences to knowledge. There is no exclusive claim on the pathway of science as the only source of knowledge. But of late, the emphasis on experimentation is so high, that it seems that the theory exists only as a language for organizing the experimental truths and does not generate any new knowledge. This is what we seek to critique. The idea that all knowledge is aposteriori is not an universal proposition, but we might say, it is the kind of knowledge that science is concerned with. There exists apriori knowledge in ethics and philosophy with which science is not concerned and that should not lower the status of the activities that pursue knowledge using non scientific methods. The presence of apriori knowledge might as well produce new knowledge, arising from the existing knowledge and no new window of observation. Thus it is possible to create knowledge in a windowless room, by discussions on the nature of things and objects. This method is called the metaphysical methods. 
Given a finite set of properties, it is possible to construct numerous recursive expositions that could explain phenomena without ever having to observe them sensorially. Thus, Humes empiricism is not the final word. The existence of apriori knowledge is postulated by Kant and proposed more practically by Chomsky (the tableau rasa rebuttal) and Einstein (who held that theory determines observation). Dr. V.S.Ramachandran, neuroscientist was critical of philosophers because they were hurling personal insults at each other, rather than work with experiments. We would need to see that it is not possible all the time to separate the individual from his work. Even with the method of science, we might trace some historical reasons for it to exist. It is afer all an incomplete body metaphysically. It does not answer questions on from where the hypothesis come from and so on. Therefore, it is normal to impute arbitrariness to the truths it proposes. Even if such an arbitrariness could not be placed on individuals and their biases, implicit bias from cultural developments and historical courses could not be excluded as being uninfluential in the popularization of the scientific method and ergo the criticism at a personal level. We might even say that science that inclines greatly on experiments promotes mediocrity in arriving at truths, since it focuses on truths that are intrinsically derived from the system rather than an observation from a higher intellectual plane. The scientific truths might hence be statistical mean of observations on a system, which the system tells to the observer, rather than the observer having a higher conception of the system.
The metaphysical method however works on a more general basis. It has wider applicability, because simple observations and intuitive truths could be handled well in the metaphysical method and not dismissed for want of rigour. Say, in our earlier work, we discussed on how a soft power complex might emerge as a result of civilizational progress. We employed the metaphysical method here. In understanding that humans are driven by a passion to search for personal advantage, we are relying on a well understood and intuitive truth. From there we deduce that if they are to do so, they might capitalize their relative superior bodily strength towards their individual benefit. We also postulate that humans are rational creatures and the natural tendency of phenomena is to rarify the distribution of specially deviant features. Therefore, physical might would be exception rather than normal and hence would be outnumbered by mediocre individuals. Since both the strong man and the weak man are rational, they are likely to attain an equilibrium of a stable nature. This stability could be contained in customs and could be promoted to more rigourous exeternal sources like mythology or logical constitutions. However, we say that it is possible for either party to destabilize the equilibrium and a final shift to an unstable equilibrium would result in structural alterations to the system, leading to an alteration of the fundamental state of the system. All the truths that we discussed here derive from basic constructs of teleology, natural behaviour of systems in seeking equilibrium and to distribute normally. The stability and instability constructs provide enough material to produce strong models of the world, which might be as good as the scientific models, with a wider range of application. 
Descartes, long considered the progenitor of the scientific method, because he was sceptical on truths unless he percieved it by his senses (the only axiomatic truth being his existence). Descartes also advocated dualism, where it is possible to observe a system without being influenced by it, wherein he places the human at a vantage that rises him rather than reduces him. He was express about the spiritual nature of humans and their being lead by a divine intuition rather they simply being the product of emergent observations. Likewise, Descartes employed the mathematical method in a pure sense and not just as a indexing tool for experimental knowledge. Hence, it could be a cultural flaw that credited Descartes with the modern scientific method, in a final sense. The metaphysical method was more popular in his times, from Aristotle onwards and Rousseau employed it in his discourses. The cultural environment for the development of the scientific method might even be speculated to have been driven by the need to manufacture truths en masse, rather than wait for intellectual rememberance of ultimate truths (as discussed by Plato).
Thus, one might revert back in these times to a greater embrace of the metaphysical method. This is discussed in mathematical constructs in a self referential way without the need for material observations. This allows for the extension of knowledge beyond science and hence the knowledge does not always have to meet the scale of utility but might be extended to allow for intellectual satisfaction, where we attempt to find meaning and satisfaction in living. It is also experiencing rigour in the discussion as the systems method, control theory and studies in stability and equilibrium of systems. The teleological model (of natural convergence) is used in the discussion of economic phenomena like attractors. The idea of control theory and cybernetics has also found way into management and developmental studies. This also comes in the light of experimental sciences experiencing a crisis from the post classical mechanics scenarion, such as with relativity where the observers frame is important and in the case of quantum mechanics where truths are ambiguous. This sometimes reduces classical mechanics as a useful heuristic rather than a fundamental quest. Hence, it is legitimate the metaphsyical methods are used without being shamed in the present age of dealing with complex problems, or wicked problems as Churchman called it.
But it is not a commendation of monistic methods that we are suggesting. It is rather a realization that deduction from general concepts using everyday observation is to be allowed as knowledge in the same basis as scientific truths. This would allow the creation of artifacts and mechanisms that work the same way as scientifically designed one. That way metaphysics is sought to be placed not as an overarching and powerful method, but only that which is on par with the scientific method to be able to produce designs rather than machines which is the output of science. This could be contained in the dualistic philosophy where elegant designs help as much as intricate machines and both these occupy the realm of the observed, while the observer is free to do as he wishes, free of observation and control.  
As a clarification with regard to the use of metaphysics, we reiterate that while it is important to discuss things in generality, considering the general forces in action, the general mathematical principles that are universals and hence apply to all systems or entities under discussion, it does not imply that metaphysics is a universal and complete method to approach arbitrary problems in nature. It is rather a method which allows us to gain insight into how particulars derive from the generals, by the application of methods that are reasonable rather than provable. The rationality of  philosophy was emphasized by Ferrier, who propounded Ontology to be dealing with the nature of existence itself. Wolff while thus understanding that ontology concerning with the metaphysics of metaphysics, metaphysics might be liberated to look at particulars at a level higher than physical sciences while remaining lower than ontology. The special metaphysics prescribed by Wolff were prefixed by rational, such as rational cosmology and rational psychology, which implies the importance of logical ideas interacting with generalized bodies, inferred reasonably, rather than by observation. The output of rational metaphysics are hence too reasonable conclusions, making it the science of the possible, but they could not be proven in a lab setting. The idea here is that this method of arriving at reasonable conclusions provides as good a useful knowledge (where we deal that useful means providing means of control), as the scientific method. Hence, metaphysics is advocated to be placed on par with the scientific method in arriving at conclusions and development of contraptions, with the same restraint and humility of the sciences. One would continue to restrain its scope as a model of the world that is to be subjected to the ability to use it or reject being the call of the individual. That is a clear seperation between that which is formal and informal is to be maintained, in accordance with the dualistic principle of modernism, rather than the monism of postmodernity. 
Metaphysical modelling and computing
We have erstwhile discussed that there  is some merit to the speculative science of metaphysics. We have also seen that much of our discussion could be mapped to thoughts by Wolff on the division of metaphysics as rational psychology and rational cosmology. Given, that these kind of metamodels are supported even by such eminent scientists like Eistein, there is not too much of a problem proceeding along these lines. The point of early philosophy in that all knowledge could be obtained by mere reflection, was challenged by Galileo and his followers and the emphasis shifted on experimentation in order to expunge bias as well as the association of ideas to individual reputations. This was instrumental in the progress of science and democratized and rejuvenated the process. But all ideas would have their merit and the world becomes weary of it. The postmodern scenario presents a cultural state where the elitism had long been banished from explicit existence and instead a high network based value based monism pervades everywhere. In such situations we might still need to look at science through the new lens. 
We were not alone in suggesting this point. The early cybernetics, or the first order cybernetists were mainly metaphysical in their emphasis, rather than the epistemic focus of the second order cybernetists (refer What ever happened to cybernetics? Peter M. Asaro cite). Wiener and his followers were looking for ways to do things in a general manner, rather than be confined to their specialities. Even Einstein wondered if the idea of moving into general metaphysical reasoning signals a reputation that the scientist has nothing interesting to do in his speciality. Thus, we might say that the role of metaphysical reasoning had steadily declined in the modern world. All propositions, howmuchever the individual maintains a discipline to keep separate his beliefs and expectations into the framing of theories from reflection alone, is no longer tenable in intellectual forums. Experimental work has taken the crown among the methods that pursue knowledge. There is however merit in the inquiry into mathematics, which deals with objects in general and their intrinsic properties without referring to any material objects and hence might not be requiring experiments. At one point, this was disputed as well, after the incompleteness theorems and maths was directed to look for its empiricism in physics. But we might say, still that it is possible to discover structures and principles by pure mathematical reflection. 
The very recent development called datascience, puts forward this point as well. It looks for patterns in the data, or the abstract tree structures in order to make speculations of an intrinsic order to it, without having to resort to experiments. That is to say, inspite observation being at the forefront of such knowledge in datascience, it is more fundamentally a metphysical belief that there exists a transdisciplinary ability to garner knowledge from observation of the uncontrolled sort and by reflecting on the intrinstic properties of numbers and collections, such as with the additivity of sinusoidal functions in representing arbitrary waveforms  in timeseries, or in the normal distribution or in the law of rare events and so on. Some  of these, such as the central tendency law does not have, until very recently a mathematical proof. There is also a resurgent interest in such fields as Lyopunov stability and markov chain to arrive at general ways to explain things away from the disciplines. This was actually speculated well in advance when the cyberneticists entered the foray. The emphasis was on general behaviour of systems and of mathematical objects and an ambitious plan was drawn to dethrone science as the primary way to pursue knowledge. 
The advent of computers might be seen as a move in this direction. Computers attempt to provide a transdisciplinary approach to problem solving. The early treatises on computational structures that could be instantized from neural nets in the works of McCulloch and Pitts(cite 1943) strengthened the possibility of deriving knowledge from pieces of a general sort, as long as feedback is used. It is in fact the non determinism in computing processes that is leveraged to produce knowledge. This might be seen in the halting problem, which could itself be seen as the indeterminacy in interacting systems making them dynamic and evolving. The convergence of the direction of evolution of such network of computing systems (such as modern networks) produce dynamic knowledge. We had discussed this at length in our various previous essays. The point is that computers are more of metaphysical devices which produce knowledge of the reasonable sort and not of the rigourous sort. 
The principal point we present here is that while the method of metaphysics has merits, it ought to have been treated as a discipline in itself with regard to the determination of truths, which themselves are incomplete like every other truth. This truth ought to have been presented to a liberal society for its cultural selection and even arbitrary individual rejection. The scepticism over cybernetic methods, lead paradoxically to its pervasiveness and it now transcends the realm of preparation of truths for consumption in a society. It is now an essential system of control, a network around which individuals and societies are organized. The resulting equilibrium of a monistic sort, is what is contested here. This equilibrium we might say could not be evaluated for its stability or inquired with respect to its structure. It exists and everyone is subjected to it as parts of the system and there is no way to logically access its principles and exert control over it. This presents a situation not unlike multicellularity as an equilibrium, if one is to take a rhetoric stake on it. But objectively, it is important to recognize and control the cybernetic method as well as the general methods of metaphysics of the special sort, as discussed by Chrisitian Wolff, in order to accomplish a human ability to cede control to universal order that is logically tractable, rather than something one is just statistically aware of. 
Given the belief that there exists an universal order, extraneous to embodied intelligence, the appreciation of the principles of mathematical modelling and metaphysical reasoning, directs humans to become aware of knoweldge which could be used to control themselves, not by ceding to an emergent order, but to an explicit intellectually specialized body in a recursive manner, till universal laws become the defining elements of orderliness. It is the incorporation of divine and aesthetic order into the human realm. It involves an explicit nomination of specialized bodies, which attempt to understand and make plans in accordance with divine will or universal order. Classic examples might be in the national constitutions. An optimism to ground the embodied intelligent people who are entrusted with the discovery of such divine truths arises from the universal tendency of humans to syncrhonize over aesthetic things. Thus the dualistic construct of a dedicated serious pursuit of singular truths as stabilized by original expressions in art, emergent synchronization in culture is the fundamental principle of our endeavour. This endeavour as was described in our first volume on computerized societies ought to be organized as a cultural body with subscription to openness and critical approaches.
Cosmological theories and the hypocritical state
There are some points which concern philosophers, which might look removed from reality in a normal ontology. A first look at mathematics of higher order suggests that one is attempting to understand ways to deal with complexity. If we look at Fourier transforms which reduceds any signal of arbitrary complexity into an additive series of elementary trigonometric functions and if we see the fundamental theorems concerning exponentials and imaginary numbers, of groups and symmetries, we often see that mathematics endeavours to unentangle the fundamental fabric of the cosmos. This situation of mathematics as arising above the material and physical world and attempting to define the universe in terms of universal behaviours (or functions) is a way to formalize the cosmos as an active and immaterial entity. This dematerialization by mathematics often had invoked problems in proofs and the branch of mathematical logic grapples with it till date and attempting to repair the dents from the incompleteness theories and the difficulties in Hilberts program. 
The principal concern here is that the formalization of the cosmos in mathematical models provides nothing that is of interest to the materially inclined man. Prof Hardy opined that mathematics in order to be pure ought to be useless (and criticized in the book Dr eulers fabulous formula cite). This creates the dilemma of philosophy, whether one must ponder on things that does not concern us. If such ponderings were to be legitimized, then the implications in terms of individual industry and social prosperity might compromized need to be considered and perhaps even the potentials for duping and conmanship. But it seems to be that the yearning is the collective conscious analogy of an amnesiac individual awakening in a strange island reflects on how he came there to be. Of course, he has to work to survive and thrive, but there is always at the back of his mind, the need to reflect and ponder on the nature of his existence. The collective direction is strongly analogous with this innate drive. The development of cybernetic as a cosmological approach seems to be a step in this direction. But at some point it lost relevance, most probably at the paricular moment of publication of second order cybernetics (as suggested by Asaro cite), because it ceased to be a system that stands alone free for observation. In second order cybernetics, the cosmological theory was so complete, making it uninteresting for further exploration and anulling possibility of exploitation for useful purposes. The notion of usefulness was discussed in our own work (cite peace, love, computers) disucssing the discrete ontological layers to existence. The meaningfulness in a lower ontological layer ought to cast a shadow in a higher layer (such as the economic one) in order to contribute the welfare of mankind and stir the continued existence of such idea. That puts a bold question on universalizing theories of any sort, to be of any acceptability and use. Nevertheless, we ponder on questions of relativity and certainty and this fundamental notions of uncertainty and ‘spooky action at a distance’ probably thwarted the wests moral legitimacy of the notions of development and the truth and preciptated very interesting turn to the cultural history of the world.  It is disconcerting to be questioned of the reality of ones world at a low level and it could only be tolerated to certain extent to be brushed under the carpet in the higher domains. Eventually, the uncertainty becomes destabilizing and history transitions dramatically. Hence, as much as we question the necessity of morality and such low level constructs, they in fact influence history in dramatic ways. 

The development of cosmological approaches essentially attempt to make normative questions ‘well-posed’. That is to say, the fundamental questions of development and stability and even justice and morality are attempted to be explored by cosmological models rigourously. The scientific method had always advocated the compartmentalization of disciplines and the need to observe things statistically in order for it to be accepted as beyond reasonable doubt.It did not confer any special status to reflections on the nature of existence and the organization of the cosmos as being impactful to the course of scientific discovery. Science concerned itself with gathering facts that could be organized epistemically and verified empirically. The truths that science pursued were dictated by worldly concerns of survival and creation of orderliness. Thus, normative questions are hardly meaningful in science. But economics and management concern themselves with such questions of normative and desirable states, which could be ascertained from within the system, rather than be presented for formulation from outside of the domain. The completeness of the normative domains requires the possibility of formalzing problems within the domain. The notions of stability, control and in some form cybernetics attempt to well pose normative problems from a mathematical stand point. 
The development of computers give rise to such universal semantics to question concerning individual disciplines. All questions are translatable into computational problems which themselves could be sorted into discrete classes of complexity levels. The computing revolution might hence be interpreted as a direction in universalization of normative questions so that models could be created across disciplines. The theorems that underlie computational logic, such as game equilibrium, sufficient testing as acceptability criteria all denote the possibility that  unconsciously, manifested not by intellectual spearheading, but by market feedback, humanity is transcending to the treatment of unversal cosmological problems as computational problems. The complexity arising in natural science domains from multiple parameters are formalizable in the humanities domain as multiple actors and their choices without loss of meaning in computing. 
The development of universals are hence inescapable and the critical point here is that if such an attempt be made consciously, rather than allowing it to emerge. That brings us to the predicament of whether we as a society have certain direction, which is expressed in terms of the summation of liberal pursuits minus the necessary and provisional sacrifices required from physical constriants. We say, we self organize as a society. This we might think from a point higher than the formal entities, such as corporations and even nations (which are organized around constitutions and reinforced militaristically). The ability to self organize revolves around making a deliberte attempt to get rid of the drift in the system that could actually move us farther from our intentions. That is the role of criticism. Etymologically, if the criticism is drained from the system, the society develops into a hypocritical one. The hypcritical society builds its own mythology and operates inconsistently. It proclaims a certain moral standard while taking advantage of its reputation, while there is nothing that the society really needs to furthers its affluence. It becomes a pathology and the society arrives at something that might be called as the state of underdevelopment or worse still a state of overdevelopment. The optimal state of development again visits the notion of harmony (in cosmological forms as with the gaia hypothesis itself arising from the infamous second order cybernetcs cite asaro). 
The problem is to somehow create an enterprise and acceptable venture to carryout cosmological theorization and critical expression and formalizing them as  a discipline. That is to say, it would be an heroic attempt to find ‘application’ of metaphysical, universal models as technical solutions. A valid area where this might be legitimately applied is the case of critical approaches. Therefore, it might be a conscious direction to formalize rational cosmology (due to Wolff) and use it in formalizing critical approaches, in order to avoid the descend of the society to a hypocritical state. (This hypocritical state had been dealt with extensively in eastern philosophy as the idea of Maya, which quite unlike shunya is a reality that is not consistent). Likewise, the use of computing is the typical drift of a given society and an iconization of the societeal direction that is unconnected to individual, intellectual reflection. Descartes propounded that it is possible of humans to make observations of the real world without getting affected, due to our reasoning faculties being intimately tied to our soul and our creator. Likewise, we might take upon rational cosmology and rational psychology and leave out rational theology in accordance with the warnings of Descartes in our methods. The applied notions of such rational approaches to metaphysical problems is in finding critical standpoints to the general flow of history. The primary entity to which the criticism is directed is at this point of history is the computer.
 
Notions of Development
Development as we had discussed is a difficult concept. It is surrounded by the well defined states such as the state of underdevelopment and the condition of overdevelopment. We will attempt to define development in this respect. We had also seen that a society is possible of descending into a hypocritical state and be embroiled in deception and illusion because of certain issues, the important of which is the absence of critical reasoning and the infrastructure required to support it, namely a metaphysical discussion and educational structure. We also need to frame the critical program as a part of the enterprise or collective will of the society. 
Now we start with the nature of enterprise or action in a given society. Businesses revolve around the concept of capital. It is actually an attempt to solve problems locally and share the spoils. A small newspaper vendor in a railway platform is a businessman because he bets on the behaviour of the visitors on the platform and invests in getting across a specific assortment of reading material. If he is to solve the problem purely computatoinally, he woud be a knowledge worker and a professional but not a businessman. What sets the businessman apart is the idea of captial. The bet on predicting the behaviour of a given set of people is creatively attempted, reflexively persuaded and as a result, there is generated a surplus from the capital. The notion of business enterprise however is unstable, in that as long as even a trickle of surplus is available, it sends out signals for competitors. Thus, it becomes imperative on the business to apprecite its capital, for that notional day, when the business would be ruined by competition. Thus the generation of surplus by virtue of business actions is actually a zero sum game. If we are to include natural forces into the equation, much like a thermodynamic conservational setting, we woud have businesses constantly striving to grow and appreciate capital, while all the time risking its dismissal by competition. Thus, business presents, not the fundamental reason, but a manifestation of a fundamental imperative to grow, but the actual process of growth in business happens in an unstable manner. 
That gets us to think if there exists more stable way to achieve the same growth, then we turn our attention to those organizations which do not have a capital nor a profit motive. These form governments and myraid institutions and trusts. These attempt to solve problems locally too, but they collaborate generally on a global scale, in that, it is not the surplus that attracts a player to a locality, but rather the slack. Thus, the distribution of risks and of work is equitable and even if there is no logical central control, the moral convergence instills a self convergent equilibrium tendency towards an equitable distribution. The system thus exhibits lesser volatility, but also fewer innovations in attempting a local equilibrium. The fundamental problem of capital driven business is the tendency for global instability. That is to say, if a disaster should strike, business enterprises globally fall back to the least risk strategy and go dormant, such as taxis remaining off road on very rainy days. This problem is seen where the perturbation could even be as remote as a rumour, rather than a real disaster. However, a moral driven enterprise is globally stable, such as government run bus services. The moral driven enterprise might not be a proper term, it is more rigourously be described as a preference to global equilibrium on the basis of the idea of a singular objective goodness and the ability to ideologically move towards this. 
Thus, there is required of both these approaches to coexist in equilibrium in order for the society to be globally stable. In the present state of history, it is rather suggested from a critical point that portions of the subjective search for local equilibrium could be addressed in non value adding channels like in Arts and culture, as with the Nordic countries. That apart, we expect both these types of organizations as being important for the equilibrium in the society. The collaboration between these parts are not necessarily antogonistic. In fact large business organizations value the need for certainty provided by a fair market and well established legal processes, likewise government value and encourage private enterprise. Conventionally, this mutual appreciation is hidden behind a veil of antogonistic posturing, which provides the dialectic basis of continuous synthesis of the appropriate equilibrium. The creation of institutions, particularly as government departments and more interestingly as completely independent professional statutory bodies, serve as crucibles for this synthesis. The invented ontology of such institutions are a critical piece to a developed society. But the relationship is not always one of dialectic synthesis. We also see, most famously in Keynesian theories and Nudge theory, the possibility to accomplish equilibrium by a perturbation of a disproportionate magnitude. This is concieved as an antithesis to the underlying dynamics of instability, where disproportionate perturbation produce dramatic shifts in equilibrium. If this is a valid notion of control, we would discuss further. 
If we look at a system in its natural constitution, it is held in multiple dimensions with a central regression. This highly paralell and multidimensional extension of an entity makes it highly stable. Any engineering could only accomplish linear optimization, compromizing overall stability. Thus, if one is to introduce a solution to a traffic problem by linearizing the traffic flow, it might only encourage more people to use the roads, or perhaps even work to the detriment of the stability in pollution levels  the intermittent traffic jams and free roads cause. Thus, leaving the system to its natural state might be a good position in terms of an ideal of stability and thus justice or even morality. However, it is unequivocally seen that nature favours change and life as a phenomenon favours a progression of order creation. This could not be avoided is taken as axiomatic. This being so, it leaves us the option to either allow a system to self stabilize, like a game, with only minor perturbation needed of generation extraneously, or even without such a perturbtion. This technique to stability however creates something that is universally undesirable. This notion of universal badness also is an apriori truth and if we should implead it for working purposes, our theory proceeds as follows. 
Life as a phenomenon, attempts to progress by multiplicity of version of truth (perhaps even signifying a multiverse, unlike a universe in which atleast one truth is singular). This multiplicity of versions of truth however exhibit a central tendency, making it somekind of a singular truth that comes with a noise. But the noise is highly instrumental in the dynamic evolution of the system as a whole. This we had seen in chaotic events arising when rare mutations partner with rare environmental events to sustain a course of development of the organism. But it is also a given fact that the encoding of such truths had shifted from geneological encoding to more literal and verbal encoding. Therefore, in the medieval world, one might imagine the multiple versioned truth existing in the context of a given tribe or town as a story or a mythology that presents as multiple slightly different versions of a central theme. This multiple versions are sustained by material humans who eat and fight to sustain their version of the truth. Thus, the mythology gives them purpose, or conversely, the mythology evolves on their backs. 
If we look at the modern state of such mythologies, we might compare them to computational programs. Mythologies are subsets of possible courses of a story in a given set of constraints. Thus, they are multiple possible sets of sufficient conditions in a substrate of necessary conditions. Computers are defined around certain constrants, due to which they are partially recursive and are allowed to produce as many versions of the truth as could be generated by their inherent property (in the formalism of the Turing machine and the halting problem) as being one of indeterminacy. Thus computers rely on the multitude of truths and the ability of multiple stories to control people arises in this case too. A story controls people by two means, one that of inspiration, where the individual sees his apriori truth be attested by a certain narrative, which provides enough reason for him to fight a battle. Of course he also empathizes his enemy as working to the common apriori truth and the necessity of determination on merit. The second means is one of taboo, where the individual accepts the story, because it is a majoritarian narrative and he doesn not to be excluded from the group, by coercive or manipulative means. Computational narratives lean heavily on the second form of control. Everyone stands committed to his version of the program because that is what keeps the value chain intact. There is no inspirational power, other than something that emerges from the network, or that which carries a weightage in relative terms in a backdrop of universal nihilism and pessimsim over any story or narrative. This we state as the condition of overdevelopment.
Thus, we consider it a worthwhile endeavour to promote a critical entity, which is sustained by both the enterprise of business and that of the non business institution, while maintaining ideological independence in producing critical material that attempts to critically evaluate the notion of progress in terms of global notions of control. This is a  suggestion that progress ought to happen intelligently, as against the general principles of computation, where it happens in an emergent way. This ability to define and contain rationally, universal and even metaphysical mathematical models (which are highly functional rather than the typical structural model of science) is seen as a step to make sense of computational intelligence and applying it as  a technique, while keeping it contained in the rational ramparts of control. This endeavour deserves its own institution, is the argument here.














If one might reflect affective influence on cognition is an important factor in development as well as it works in inverse. The constant anxiety and emotional turmoil in development creates situations where one feels not in control and unjustly oppressed or persecuted 















au










Particular Papers

The finality of the non computability hypothesis
Non computability is a hypothesis that was proposed by Turing in his 1936 paper. It implies that certain functions cannot be computed using a Turing machine. However subsequent developments denote that what could not be computed could be learned. This paper is intended to criticize the learning paradigm of the non computable as arbitrary and invoking a dynamical development of the schema of learning machines, leading to a control problem. As a result of such control problems, we can say that the complex system of learning machines eventually regress to compute the problem arbitrarily. That is to say, the system attains an authority over the subject as to push arbitrary decisions rather than what would be reasonably expected to be computed solutions.
Proving this hypthesis would require us to organize the machine 











a
Cognitive basis of psychological behaviour
We present here an approach to understanding psychological phenomena that forms the basis of the conceptualization and the justification for intervention of insanity or psychological conditions. This is particularly relevant to understand if the nature of mental illness is a social construct and the role of development in the establishment of mental illnesses. We also see the strength of the arguments of precursors, predispositions and the nature of disease being structural and requiring a quasi structural modification by skewing the signaling medium. 
We would first look at the nature of reality as it is. It is conventional in the method of science to look at reality from an epistemological view point (Cogito, ergo sum). Recently philosophers such as Roy Bhaskar championed the case of putting ontology before epistemology (say, roughtly I am therefore I think). We suggest looking at reality through the lens of agniology (also a key part of metaphysical approach of Ferrier, who coined epistemology). It is in terms of what we do not know, that we guage reality. That which is mystical and unresolvable ought to be real. For instance Hardy had argued that mathematics is real and it appears so and generally accepted so, since we have compelling evidence of its irreductability to logical deduction (say the mystery of qunitic equations, the incomputability of diophantine equations etc cite Godel). That which is inexhaustible to an epistemological pursuit ought to be real. That which could be stated completely, is unreal and abstract, since only abstract theories could permit complete logical statement (the famous bachelor example of Kant in the critique of pure reason). Hence, we might say, our model of reality always has frayed edges, we do not know if time bends, or whether an electron is here or there or some particles it is seen could be at the same place at the same time. This kind of entanglement and imprecision however cause us little discomfort in dealing with our everyday lives.
Hence, we might say we in principle learn to live with reality as being poorly defined. Things can be accomodated into reality upon strong evidence and remain there in our model, unless evicted for a good reason. The second piece of our argument is that people suffer due to their will being frustrated. Much of suffering could be empirically seen arise from events that are not materially impacting. A lot of inspirational material point to the immense rarity and richness of life as it is, often ignored in favour of trifling worries assuming monstrous proportion in a given subjective context. We might think of an institutionalization of this view point as the beatniks (see Dharma Bums) and so on unto the hippie movement. A great deal of troubles arise from bedazzlement, the shift of interest on to things that dazzle and attract attention, while being of little value to our existence and essence. These might be pressures of status, of acceptance by people and of hedonic habituation. We see in these cases, the subject suffers immensely or anxious about possible adverse outcomes, though these upon deeper reflection look trivial and passing. This leads us to the nature of the social construct. We often see that human progress or for that nature any evolutionary system progresses dynamically. But pure dynamism does not explain complex construction. Hence, it is often that an objective basis is defined for an act that involves a ‘search’ for a better way of using resources, ie to reduce the dissipation of resources to waste heat and thereupon independent (ie orthogonal) agents are left to pursue their subjective strategy to attain the goal. It is vastly discussed in game theory, how such disparate agencies converge eventually to produce normative results. We might interpret normative results as serving a positive progression in the direction of greater mechanical efficiency in conversion of free energy (derived from less ordered structures) to recursively more ordered structures. This recursive order could hence be created only by dynamic means. Therefore the dynamic computation that is distributed (which is in essence what an UTM does, to suppose different pathways algorithmically to converge on the solution) is perhaps a probabilistic Turing machine, which is just as powerful as the DTM. The backtracking of the Turing machine makes it not much different from distributed approaches. Hence, we might say, that the human task of computation involves building recursive structures dynamically, within set boundaries. That is to say, there exists games in which people play to rules and there are administrators to these rules, of people who play in higher order games. In this model of human evolution, we say this recursive arrangment of games puts upon the subject a pressure to accomplish in a game. The pressure becomes real and the prospect of calling it a day and withdrawing to the woods is considered hermetic and philosophical (say Thoreau). In general, it is the game people play that constructs order. It becomes inseperable from the real pursuit for most parts. 
Given, that reality is such a disturbed concept, we might see that the world is fraught with posturing and representation as much as there are real actions and things. This might be a militaristic posturing in the feudal times or a capitalistic posturing in modern times. We might even think of a brute force dynamics of cave age along these lines. Thus, we might say that crime, mental illness and capitalistic exploitation are all normative constructs, which could be interpreted dereferentially and purely statistically. But it would be simplification to do so. These instruments have strong objective basis and are functional towards positive directions. Hence, we would rather say that in general, the sane mind is able to see the object behind the complex of games and posturing and representations, eventhough it often times does not. It is able to reflect on its folly and is able to achieve perspective from time to time. 
This being given, we might now discuss on how this reflective faculty is placed in the mind of humans. We might see a lot of bedazzlement related pains arise from frustration due to interpersonal relationships, in a broader sense, even hedonic withdrawl as a frustration arises because of monetary presssures which are social constructs. Thus, we might say that the seed of mental illness is sowed by the special condition of humans as highly communicative creatures and arise due to distorted ability to separate representation from reality. But it is not to dismiss that there might be confounding natural phenomena that could drive people crazy (say the Marlin in the Old man and the sea cite). Therefore general complexity that makes a static model of reality as distant makes people crazy, to use a loose term. Given that we believe in reflection, we might not be offending to consider that the human mind is dualistic, of conscious and subconsious nature. We might see that the subconscious might be able to protect the conscious by buffering some of the distortion of reality by emotional upheavals and accomodation of a new model of reality. Without the emotional pain, reality might become unbearable cognitively. We often see that in people who are mentally defunct, this ability to recognize and avoid the distortion of inner model of reality is worn or born thin. Ill people might exhibit behaviour which could be normally recognized by a person to be abnormal. That is to say, while all play games, the patient might be playing games so explicitly that it looks absurd to a given observer. What makes him loose his ability to restrain and hide and filter his inner state so that the game appears sensible. The objective observer had gone making him appear crazy. 
The notion of objective observer would require us to model, how much of this is attributable to initial conditions and how much to developmental conditions. We might see that development is dynamic, involving a controller – who uses the techniques of a natural split, measurement and reinforcement to achieve desirable ends. Thus, learning (even in a machine learning sense) is inseperable from the concept of development. If a child is brought up in an environment, where the parent says he will brand the child for a transregression, but never intending to carry out the threat, the child may or may not interpret his posturing properly. For the child it might become difficult to separate out where objective baseline ends and a dynamic arena starts. This causes the child to develop in a way (by learning) that causes him to not differentiate adequately between posturing and reality, making him develop anxious and depressive tendencies. Or he might loose the nuances of making a posture, causing him to randomly exhibit posturing on others, as if they are not aware of the complexity of the game. This is the case of personality disorders. Often times it is discussed as the lack of mentalization, but it could rather be not just an emulation of the mind of the normal, but a deeper connect to the organization of game dynamics in human societies. Hence, it is crucial upon the developmental stage. The control independence of adulthoold makes it difficult to recover control, once full development is reached,making therapy only partially successful. 
Thus we might say mental illness might be naturalistic and exist in a positivist sense, but it becomes recognizable only with a normative vote, otherwise, whether it exists or not has no value attachment to it. It might even simply be the ‘ornery’ behaviour that is extended to situations. It is the frequency and intensity of instances of such orneriness that one labels a patient as dysfunctional. As we said, it does look like a definite threshold where something ‘breaks’ and the watchman of objectivism is conspicously absent. The response to feedback is what might be different in patients. There is also increasing interest in viewing mental illnesses as dysregulation or control issues. Control involves dampening and inhibitors like GABA had been implicated in mental health disorders. This again places weightage on the development aspect, rather than the initial condition aspect. This might mean a use of social dampeners on the behavioural anamolies associated with illness.  In communities where the challenges are real unquestionably, as a crocodile would bit or a tantrum might mean being left behind by the tribe to die of starvation, it is likely the dampeners are intact. Otherwise, where the dampeners are game equilibriums, which can be pushed, say an adamant person is accomodated begrudgingly, might mean that the boundaries are not clear and hence the dampening effect becomes difficult. In modern societies, where reality exists below many layers of constructed simulations of social and cultural nature, it is difficult to manifest it without challenging the patient to get to the bottom of the posturing, in which case the dampener actually becomes a possible excaberator. A dissipative mechanism might be better than a concrete dampener. That is to say, where the storm is gathering, it might work to split the game into small pieces, so that the patient is able to quickly resolve and finish them. This is the ability to breakdown goals into small pieces, without complexity of posturing and dampening. 
An important dimension to note is that mental illness becomes difficult due to social excaberation, rather than dampening. A person like the fictional character of Forrest Gump might have likely been leading a regular life, if he is accepted, in the sense as considering him as variety of normal, rather than the different which should be persecuted. 

Speculatively, we might even see that the self (which is already dual) might becomes such that the subconscious is dominant or that the boundary of self with the other becomes distorted. 
One would have to investigate for experimental evidence in non humans and among populations that are subjugated to strong controls to study the degree of cognitive distortion .

Why Goal alignment may not be possible
If we look at how the human systems are organized, we see an unmistakable recursion about it, much like general living systems and to some extent even physical systems. The recursion arises where there is development that is dynamical. The essence of development is its dynamism. Humans have often been referred to be goal directed agents. I have somehow found this reductive and undermining of the soul. But for all objective descriptive purposes, humans do seem to be goal directed. They are specified goals from time to time which they strive to achieve. This might be a sales target, a wealth acquisition target, a health target and so forth. These in fact induce a great deal of frustration in the mind when these could not be achieved, even devastating individuals and making them blind to the notion that it is just a goal to a specific bounded game and life as such remains in great riches outside of the bounds. This had been forever the appeal of inspirational material. Materialism is what it might be called, but even that is elusive a label, because the material is not evaluated intrinisically for value. There is much more human activity over the procurement of food materials and their preservation than to the development of latest cellular phones. 
This being the case, we may say that the goal directed agents in fact fit a good description of human behaviour. The goals look like computational targets to which the agents attempt dynamically to arrive at the least complex algorithm. It would be simple to see that this resembles a model of a probabilistic turing machine. For an objective observer, the local actions are only probabilistic. This is because, the entire local picture in its complexity could not be made out to high resolution at the observer level. This incomplete information causes the observer (even if he knows the desirable gradient in which the decision should happen) only to reason probabilistically (because he does not know local constraints). Therefore, the probabilistic turing machine provides performance updates but not more power than the UTM in deciding problems. The algorithms that are developed are not necessarily abstract either. They are reified as houses, buildings, roads, vehicles, window frames, cars and sometimes remain in abstract forms as markets. 
This much being clear, the question arises as to who sets the objectives or goals for a dynamic program to execute. We see the formulation of the game and keeping tabs of the score, arises in a constrained frame. If we are to tame (select for traits that we set as objective goals) animals, we would prima facie need constraints for the optimization problem. We would not want the animals run away rather than face selective pressure. A goal formulation and a constraint specification become fundamental ontological components for this theory. The latter might be seen in the notion of legal contracts. It might be that law preceeds engineering. Once a contract is made out, it constrains people from running away, much like fences. These extend to private property, exclusive use rights (including rights to use it in suboptimal manner, like leaving hotel rooms empty, rather than letting out), rights over what could be intellectually copied and so on. This is also extended to the monopoly over coercion and the issue of instruments and securities that refer to a central register, while being capable of negotiation over value questions (currency, legal title documents) etc. Thus, running a central registry, enforcing and publishing contracts and reserving coercion and the preservation of the structure overall by militaristic and diplomatic instruments become the prerogative of a well formed legal system, which itself derives from a constitution rooted to the philosophy and theory of law. Given these constraints, people are able to specify objective which then become the object of computation and the turing machine decides on the problem. In the process, people specify goals to other machines in the network. Say if a constrained system works to the goal of finding the most cost efficient way to design plastic buckets, it pushes the savings to a banking system, which then optimizes to the goal of making the best investments. 
Thus, the notion of constraints are understandable. They might be seen emergent, as with Rousseau on that they emerge from the liberatarian spirit to draw up a contract, a social contract which forms the basis of all the other contracts. These contracts allow goal specification to which the computation goes on. Therefore, we might narrow down to one phenomenon, even the contract might be metalogically related to a consensus based goal. Likewise, we might say goal specification by a ‘moderator’ or umpire to a goal based forum is highly influenced by copying from peers, optimization to the moderators own goals in his game and the feedback from the subjects who attempt to optimize to it. Thus, it is heavily shaped by inputs, mostly feedbacks from all sides, lateral, authority backed and control backed. Therefore,we might say that the whole enterprise of computational problem solving looks normative and self referential. The contracts are nothing of much higher holiness than the need to be within the contract and to respond to peer game moves. This is a purely game view of the world, monistic and in which there is a seamless flow between the individual and the environment. It might be even said of the cybernetic world view as being monistic, because the goals seem to ‘emerge’ from random perturbations and feedbacks that arise from it, rather than a high reliance on initial conditions. 
But we might as well see the positivistic direction to it. In Logical positivism, we say that there is a definite direction to logical systems. The direction is lent by the phenomenon of entropy, which describes  a negative direction to random constructions all over the universe. We might as well reason that the computational goals derive from a greater phenomenon of evolution, which itself might be turbulence in the given schema of material availability (water, hydrocarbons, low frequency of corrossive material due to earths atmosphere and magnetosphere) and the peculiar thermal gradient of the earth in its position relative to the sun wrapped in an atmosphere, which reaches near thermal equilibrium (proxying the sink) with the surface. Thus we might say, that the computation is a positive concept that recursively extends the turbulent backwaters (pun unintended) of the flow of energy. We might say hence a definite optimum exist with respect to these specifc set of conditions, beyond which extension might seem absurd, in an anthropic conception of the world. Hence, we might say in the continuum of computational development of algorithms, the last ones might be intergalactic probes and rockets which would enable the replication of the human form to similar gradients that exist around the universe (As Elon musk suggests and reminiscent of the hitchhikers guide to the galaxy).
But this also begs the question as to why the subject might not be altered to suit more unfavourable gradients (ie in the present context).  This would mean to say if the systems could be ‘developed’ such that goals might be specified onto to human agents to adapt to wider gradients. In human system the body remains the only way to avoid the halting problem. Otherwise, they may end up searching the field that had already been searched. The halting problem due to embodiment (where the body inevitably stops, either for rest or finally), causes humans to compute incrementall to a greater degree than could have been possible. If it is possible to develop systems that could simulate human intelligence, in an environment which is constrained to specific goals, then we might possibly harness synthetic agents who can be projections of human agency to an extent that a wider range of gradients might become viable. The constraining to specific goals is only a theoretical possibility, such as in scientific experiments and even in such labs one cannot exclude the point that the experimenters egotism is being served in making discoveries. Thus, essentially learning systems, develop and developing systems grow and growing systems break free of constraints. Even with regard to dynamic control (as against constraints), which involves manipulation and representation, the energy flow settles into an equilibrium. Where the number of states of one side increases, the dissipative flow sets in, leading to the ability to control, specify goals, constrain and  dynamically select among the agents. Hence, seeking more intelligent underlings would break the purpose. The ability to ‘align to goals’ is a normative term, without any positivst basis. Such an engineering feat would contradict the flow of energy and control being directed.
That is to say, we are lead to the holy grail of artificial intelligence control dialog. It postulates the existence of an ability to align an intelligent system to goals. In fact, as we said, we do specify goals under duress to evolving organisms and we align such goals to lateral (social), superior goals and listen intently to feedback from the subjects themselves. The feedback might also flow back to influence the social and power goals, such as selection of paddy as the primay grain might affect demographics of the labour market (more field hands, more kids at home etc). But the distinction between feedforward and feedback is maintained by the ability for subjective observability. The paddy cannot observe as in the same way as we observe paddy, though the influence might be significant both ways. The human can understand the states of the paddy crop (inspite of the intricate and inimitable way it converts free chemicals into usable ones), in terms of similar constructs in other cereal crops, which gives them greater leeway for arbirary selection. Here, the ability to completely filter out backflow is considerably weaker than the dynamic control. Paddy though rooted to the ground, can influence through feedback entire empires and fences do not matter. It is the ability to dynamically shift between crops and hedge to uncertainties that allows humans to remain stable. That is the directionality to the control, in terms of greater reserve free energy, which itself is in the pipeline, by the ability dissipate energy in searches for other forms of crops, which can be more rigourously mapped to the ability for a greater number of states possible for the controlling system with respect to the subject system. The subject system could observe its controller as being much more probabilistic than how the controller observes the subject. The probability arises from there being a greater number of latent states (if we are to satisfy with discrete mathematical constructs) that the system had evolved over time. Hence, we might say the number of latent states is proportionate to the evolutionary timeline, the more varied the perturbations the organism was subjected to till it reached its present state, the more the number of states it has. Hence, we might presume that humans had a more varied and longer evolutionary history than paddy. 
But it is also a question of frequency, rather than linear time. Virus and bacteria evolve with much greater frequency, but never reaching multicellular forms, because the gaps in multicellular organization create opportunities for parasitic existence (just as we cannot scratch our backs). Therefore complex organizations unwittingly sabotage competition by hosting lowbies on parties from time to time. Hence, we might say, that high frequency replicators or highly evolved agents are difficult of being aligned to goals that matter to us, because of the inherent property of high frequency communication determining the direction of control in self organized systems that arise in near thermal equilibrium. Thus goal alignment seems to be a pure value discussion or normative one, with not much of a positive basis. 
We would however also need to look at the self limiting behaviour of human evolution. That is to say, at arbitrary points, the human organization seems to reflect beyond the goals and be able to reflect the dynamics as being a necessary cost to existence (as taxes are the tribute to the organizer of the game) and not existence itself. Therefore, they seem to constrain an increment of computational abilities, particular where the adaptation is done for a greater (or more abstract) range of specifics in thermodynamic gradients and material (say not water and carbon). This might be seen as an existential inertia common to all evolving systems to stick to stable conditions and not be lead by perturbations or temptations (here tempting intergalactic speculative narratives). Therefore, if we model the human system in terms of states including dynamically stable ones, energy flows we might as well see that the barrier to destabilize with this kind of reflection on the metaphysics of the game is difficult. If the reflection itself is a functional agent of stability or if it is a real limit that prevents man from becoming another mechanical turbulent phenomenon is indeterminable (say they have a soul, which prevents them). 
PS 
Ideas and static engineering which settle transformations as equi probabilistic (equi potential) transformations on natural entities might deter evolution. 
Secondly, we see that control arises by probabilistic reasoning due to uniqueness. The paddy opens itself to observation, not only in time, but also in terms of a smooth boundary with the remaining crops, that is there are other life forms which closely resemble paddy and could be observed on similar bases. 
We need to mathematically explain the computability of smooth shapes (convex maybe) as much easier than unique ones. We need to symbolically express that free energy is only in the pipeline of a dissipative flow. We need to explain dynamically how the near energy equilibrium creates structures with local goals and dynamic computation. We need to explain the mechanism of natural splitting into Lyopunov states and reinforcement to select algorithms, their reification and basis for material computation to arrive at a general theory of control of adaptive systems. High frequency adapatations and high incomputability might be seen to be presenting strong feedback to the goals being specified and a possible equilibrium with the synthetic agents becoming parasites or as controllers. A symbiosis is a special case where historical constraints step in.
PS2
It is to note that this theory covers only the relationship between adaptive systems. There might be two distinct flavours of control. The system is adaptive, when it could be set goals and when constrained, the system takes the bait and splits into natural attractors allowing human selection. These systems are living systems and organizations, including computational systems that organize around energy(these systems are not constrained in a physical way, but set to constraints by temptation of free energy). It is feasible and sustainable where the controller maintains higher diversity and the subject is smooth convex with regard to its variants (paddy to wheat say). This need for higher diversity also predisposes, chillingly, for the controllers to eliminate near species (say the doomed neanderthalus). It is rather a positive imperative rather than a normative anxiety that AI might be dangerous. 
The other kind of systems are those that could neither be specified goals nor constrained. These are the like of meteorological systems, like cyclones. But these systems also behave and split into ditinct attractors under the right circumstances. This might happen, say where a cyclone enters a gulf. Eventhough the observer does not ‘own’ the fences of the observed system, he owns it by virtue of his being able to make deterministic assertions about the land mass. This prior knowledge and its deductions, ie its ability to construct axiomatic theories over the elements allows the observer to classify the cyclone along a few measure criteria. This he uses to adapt his own sitution, by rearranging material, say evacuation or by sheltering from the onslaught of the cyclone. Thus, while the cyclone is still in control, the human controller is able to blunt its ability to exert authority (by unexplained, arbitrary blows), by being able to measure and reason about it and perhaps use its own limitations to control it, say we can disperse avalanches by triggering them by explosions. This allows control as well of non adaptive systems, by being able to make observations about them. Our adaptations does not exert a feedback or selective pressure on the cyclones though. This kind of control relies on the development of axiomatic theories, which are themselves recursive and hence given to dynamical process as in the same manner as in the materialist universe. But having ‘ideas’ allows humans to control systems. 
If ideas could be learned is a difficult question. That is to say, by simply observing without axiomatic arrangement of theories (which themselves have historical backdrop, being recursive, built over the human bodily experience with fire and earth, say), it is possible to devise control strategies. We might say, that this works only in the case of adaptive systems, the former kind of control. A single strategy of pumping free energy (reward) and selection, smoothing as with HJB and MCMC methods could not work for those entities which are not constrained by engineering bounds and which do not accept goals. 













